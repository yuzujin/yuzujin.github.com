---
layout: post
title: "线性模型"
description: ""
category: [机器学习] 
tags: [模型]
---
{% include JB/setup %}
## 线性模型
线性模型（linear model）试图学得一个通过属性的线性组合来进行预测的函数，即  ![线性组合](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性组合.png?raw=true "Title")

一般用向量形式写成
        ![线性组合(向量形式)](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性组合(向量形式).png?raw=true "Title") ，其中***w***=(*w1*;*w2*;...;*wd*)，解释一下这里向量中的分号：逗号分割的向量通常为行向量，分号分割通常为列向量。

*w*和*b*学得之后，模型就得以确定。*w*和*b*一般使用训练数据拟合、迭代或直接解出，这个求解过程被称作“学习”。

## 线性回归
线性回归是一个回归问题，即用一条线去拟合训练数据。”线性回归“试图学得一个线性模型以尽可能准确地预测实值输出标记。

**目标函数**：![线性回归1](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归1.png?raw=true "Title")，使得![线性回归2](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归2.png?raw=true "Title")

**损失函数**：MSE-均方误差（注意与感知机的区别，此处误分类点与坐标轴垂直）

![线性回归3](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归3.png?raw=true "Title")

**学习策略**：最小化损失函数，求解参数w和b

![线性回归4](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归4.png?raw=true "Title")

**求解方法**： 最小二乘法，梯度下降法

## 逻辑回归
逻辑回归是一个二分类问题。将线性回归的输出作为sigmoid函数的输入，最终的输出便是分类的结果即是输入的条件概率。

**目标函数**：![逻辑回归1](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归1.png?raw=true "Title")

**原始损失函数**：求所有训练样本的条件概率之积的最大值。

![逻辑回归2](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归2.png?raw=true "Title")

**对数损失函数**：目标是求得损失函数的最大值，即：最大似然估计。通过转换为对数形式，将最大值优化问题转换成最小值优化问题

![逻辑回归3](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归3.png?raw=true "Title")

**学习策略**：最大似然估计

**求解方法**：梯度下降、牛顿法

## 感知机
感知机是一个二分类问题。线性回归的输出作为阶跃函数的输入，最终的输出便是分类的结果。（感知器算法存在跳跃，在0点不可导，且在0附近模型容易受到干扰）

**目标函数**：![感知机1](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/感知机1.png?raw=true "Title")

**损失函数**：误分类点到超平面（wx+b=0）的总距离（1/\|w\|不影响求最优值，故省掉）

![感知机2](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/感知机2.png?raw=true "Title")

**学习策略**：最小化损失函数。（注意与线性回归的区别，此处误分类点与超平面垂直）

**求解方法**：梯度下降法

## SVM
在感知器分类选分类超平面时，我们可以选择很多个平面作为超平面，而选择哪个超平面最好呢，我们可以选择距离正样本和负样本最远的超平面(间隔最大)作为分类超平面，基于这种想法人们提出了SVM算法。

支持向量机（support vector machines，SVM）是一种二类分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机。

支持向量机学习方法包含构建由简至繁的模型：
当训练数据线性可分时，通过硬间隔最大化（hard margin maximization），学习一个线性分类器，即
**线性可分支持向量机（linear support vector machine in linearly separable case）**；
当训练数据近似线性可分时，通过软间隔最大化（soft margin maximization），也学习一个线性的分类器，即 **线性支持向量机（linear support vector machine）**；
当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习 **非线性支持向量机（no-linear support vector machine）**。

**损失函数**: 为合页函数，当分类错误时，函数间隔越大，则损失函数值越大。当分类正确且样本点距离超平面一定距离以上，则损失函数值为0。误分类的点和与分类超平面距离较近的点会影响损失函数的值。

**学习策略**：间隔最大化，可形式化为一个求解凸二次规划（convex quadratic progrmming）的问题，也等价于正则化的合页损失函数的最小化问题。


## 总结

线性回归用于解决回归问题；其他三类解决分类问题，且都是对线性回归的输出做了一些处理，logistic和svm是由感知器发展改善而来的。

区别在于三者的损失函数不同。后两者的损失函数的目的都是增加对分类影响较大的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。考虑局部最优化，如何让靠近中间线的点尽可能的远离中间线会占用更高的权值，远离中间线的值，权重为零；逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重，在所有样本上最优。
