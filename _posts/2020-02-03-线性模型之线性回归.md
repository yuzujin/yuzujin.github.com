---
layout: post
title: "线性模型之线性回归"
description: ""
category: [机器学习]
tags: [线性模型，回归]
---
{% include JB/setup %}

线性回归（linear regression）试图学得一个线性模型以尽可能准确地预测实值输出标记。

## 输入一维，输出一维
我们先考虑一种最简单的情形：输入属性的数目只有一个。  
对于点对 ![lir1](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/lir1.png?raw=true "Title") , 其中xi ∈ R。对离散属性，若属性值间存在”序“关系，可通过连续化将其转化为连续值，例如二值属性”身高“的取值”高“ 和 ”矮“ 可转化为{1.0, 0.0}

有以下线性模型：  
![线性回归1](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归1.png?raw=true "Title")  
使得  
![线性回归2](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归2.png?raw=true "Title")  
当输入是很多个xi ，每个xi都有一个标签yi ，那么该线性模型，就是找一条直线拟合这些点，也就是上式中的f(x) ，对于每一个输出的f(xi) 与 yi ，可以设置不同的损失函数，我们以MSE（均方误差）为例：  
![线性回归3](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归3.png?raw=true "Title")  
均方误差有非常好的几何意义，它对应了常用的欧几里得距离或简称欧式距离。基于均方误差最小化来进行模型求解的方法称为”最小二乘法“（least square method）。在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。
那么使得 Loss 最小的 w 与 b ，就是最优模型参数。既：  
![线性回归4](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归4.png?raw=true "Title")   
求解方法，我们可以先将 Loss 对 w 与 b 求导：  
![线性回归5](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归5.png?raw=true "Title") 

直接令上式为0，求解 w 与 b ，就是“最小二乘法”；在一维输入，一维输出的情形下，通常直接使用最小二乘求出最优解。当输入输出维度变多后，可以使用梯度下降法迭代求解。

## 输入多维，输出一维
更一般的情形是样本有d个属性描述，对于点对![6](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归6.png?raw=true "Title")，其中![7](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归7.png?raw=true "Title")，有以下线性模型：  
![8](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归8.png?raw=true "Title")  
使得  
![9](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归9.png?raw=true "Title")  
为了便于讨论，我们把w和b吸收入向量形式w=(w;b),即  
![10](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归10.png?raw=true "Title")  
相应的，把数据集D表示为一个m x (d+1)大小的矩阵X，其中每行对应于一个示例，该行前d个元素对应于示例的d个属性值，最后一个元素恒置为1，即  
![11](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归11.png?raw=true "Title")  
上式可改写为：  
![12](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归12.png?raw=true "Title")  
由于 ***X*** 的维度是m x (n + 1) ， ***W*** 的维度是 (n + 1) x 1， 方程组的个数和求解未知数个数（***W***）不相等，上式无法使用传统求解方程组的算法来求解。这里同样使用最小二乘法，在等式两边同乘![16](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归16.png?raw=true "Title") ：  
![13](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归13.png?raw=true "Title")  
现在左边 ***W*** 的系数矩阵![17](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归17.png?raw=true "Title")的维度是(n+1) x (n+1) ，右边的矩阵维度也为 (n+1) x (n+1) ，并且此方程 ***W*** 的解正是最小二乘解，相应证明可参照数值代数中的详解。即该模型的参数最优解为：  
![14](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归14.png?raw=true "Title")  
该线性模型为：  
![15](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归15.png?raw=true "Title")  

## 输入多维，输出多维
对于点对![18](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归18.png?raw=true "Title") ，其中 ![19](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归19.png?raw=true "Title")，![20](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归20.png?raw=true "Title")，有以下线性模型：

![21](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归21.png?raw=true "Title")

这就是神经网络中的全连接层，用矩阵的形式可写为下式：

![22](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归22.png?raw=true "Title")

可简写为下式：

![23](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归23.png?raw=true "Title")

其中，***X*** 的维度为 m x (n + 1) ，***W*** 的维度为 (n + 1) x k ，***Y*** 的维度为 m x k 。多维输入，多维输出的线性模型，也就是全连接层，一般不使用最小二乘法求解，在神经网络寻优中，通常使用梯度下降法，求解参数。该算法较为深入，详细过程可参考神经网络参数训练求解方法。

## 参考：
1. https://zhuanlan.zhihu.com/p/90917024
2. 周志华《机器学习》