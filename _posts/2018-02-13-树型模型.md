---
layout: post
title: "树型模型"
description: ""
category: 
tags: []
---
{% include JB/setup %}

## 树型模型

树型模型有助于探索数据集，并可视化预测的决策规则。当你听到关于树型模型的东西时，你可以将其想成是决策树或分支操作序列。树型模型高度精确、稳定且更易于解释。与线性模型相反，它们可以映射非线性关系以求解问题。

![树型模型](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型.jpeg?raw=true "Title"){:height="200px" width="650px"}

## 决策树
决策树(decision tress)是一类常见的机器学习方法。以二分类任务为例，我们希望从给定训练数据集学得一个模型用以对新示例进行分类，这个把样本分类的任务，可看做对“当前样本是正常分类吗？”这个问题的“决策”或“判定”过程。  
顾名思义，决策树是基于树结构来进行决策的，这恰是人类面临决策问题时一种很自然的处理机制。例如下面判断西瓜是“好瓜”还是“坏瓜”的过程：  
![西瓜决策树](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/西瓜决策树.jpg?raw=true "Title"){:height="300px" width="300px"}  
在上图的决策树中，决策过程的每一次判定都是对某一属性的“测试”，决策最终结论则对应最终的判定结果。一般一颗决策树包含：一个根节点、若干个内部节点和若干个叶子节点，易知：  
* 每个非叶节点表示一个特征属性测试。
* 每个分支代表这个特征属性在某个值域上的输出。
* 每个叶子节点存放一个类别。
* 每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。  

### 决策树的构造过程
决策树的构造是一个递归的过程，有三种情形会导致递归返回：
* (1) 当前结点包含的样本全属于同一类别，这时直接将该节点标记为叶节点，并设为相应的类别；
* (2) 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分，这时将该节点标记为叶节点，并将其类别设为该节点所含样本最多的类别；
* (3) 当前结点包含的样本集合为空，不能划分，这时也将该节点标记为叶节点，并将其类别设为父节点中所含样本最多的类别。算法的基本流程如下图所示：  
![1](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型1.png?raw=true "Title"){:height="550px" width="650px"}  
可以看出：决策树学习的关键在于如何选择划分属性，不同的划分属性得出不同的分支结构，从而影响整颗决策树的性能。属性划分的目标是让各个划分出来的子节点尽可能地“纯”，即属于同一类别。因此下面便是介绍量化纯度的具体方法，决策树最常用的算法有三种：ID3，C4.5和CART。  

### ID3算法
ID3算法使用信息增益为准则来选择划分属性，“信息熵”(information entropy)是度量样本结合纯度的常用指标，假定当前样本集合D中第k类样本所占比例为pk，则样本集合D的信息熵定义为：  
![2](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型2.png?raw=true "Title"){:height="80px" width="400px"}  
“信息熵“值越大表示越混乱，只有一个类别时，信息熵为0。
假定通过属性划分样本集D，产生了V个分支节点，v表示其中第v个分支节点，易知：分支节点包含的样本数越多，表示该分支节点的影响力越大。故可以计算出划分后相比原始数据集D获得的“信息增益”（information gain）。  
![3](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型3.png?raw=true "Title"){:height="80px" width="400px"}  
信息增益越大，表示使用该属性划分样本集D的效果越好，因此ID3算法在递归过程中，每次选择最大信息增益的属性作为当前的划分属性。

### C4.5算法
ID3算法存在一个问题，就是偏向于取值数目较多的属性，例如：如果存在一个唯一标识，这样样本集D将会被划分为|D|个分支，每个分支只有一个样本，这样划分后的信息熵为零，十分纯净，但是对分类毫无用处。因此C4.5算法使用了“增益率”（gain ratio）来选择划分属性，来避免这个问题带来的困扰。首先使用ID3算法计算出信息增益高于平均水平的候选属性，接着C4.5计算这些候选属性的增益率，增益率定义为：  
![4](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型4.png?raw=true "Title"){:height="120px" width="400px"}

### CART算法
CART决策树使用“基尼指数”（Gini index）来选择划分属性，基尼指数反映的是从样本集D中随机抽取两个样本，其类别标记不一致的概率，因此Gini(D)越小越好（表示集合越纯），基尼指数定义如下：  
![5](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型5.png?raw=true "Title"){:height="80px" width="400px"}  
进而，使用属性α划分后的基尼指数为：  
![6](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型6.png?raw=true "Title"){:height="80px" width="400px"}

## 剪枝处理
从决策树的构造流程中我们可以直观地看出：不管怎么样的训练集，决策树总是能很好地将各个类别分离开来，这时就会遇到之前提到过的问题：过拟合（overfitting），即太依赖于训练样本。剪枝（pruning）则是决策树算法对付过拟合的主要手段，剪枝的策略有两种如下：  
* 预剪枝（prepruning）：在构造的过程中先评估，再考虑是否分支。
* 后剪枝（post-pruning）：在构造好一颗完整的决策树后，自底向上，评估分支的必要性。
评估指的是性能度量，即决策树的泛化性能。之前提到：可以使用测试集作为学习器泛化性能的近似，因此可以将数据集划分为训练集和测试集。预剪枝表示在构造数的过程中，对一个节点考虑是否分支时，首先计算决策树不分支时在测试集上的性能，再计算分支之后的性能，若分支对性能没有提升，则选择不分支（即剪枝）。后剪枝则表示在构造好一颗完整的决策树后，从最下面的节点开始，考虑该节点分支对模型的性能是否有提升，若无则剪枝，即将该节点标记为叶子节点，类别标记为其包含样本最多的类别。

未剪枝决策树：  
![7](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型7.jpeg?raw=true "Title"){:height="400px" width="600px"}  
预剪枝决策树：  
![8](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型8.png?raw=true "Title"){:height="350px" width="600px"}  
后剪枝决策树：  
![9](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型9.jpg?raw=true "Title"){:height="400px" width="600px"}  

预剪枝与后剪枝优缺点比较
（1）时间开销　　　

　　•   预剪枝：训练时间开销降低，测试时间开销降低

　　•  后剪枝：训练时间开销增加，测试时间开销降低

（2）过/欠拟合风险

　　•  预剪枝：过拟合风险降低，欠拟合风险增加

　　•  后剪枝：过拟合风险降低，欠拟合风险基本不变

（3）泛化性能：后剪枝通常优于预剪枝

## 连续值和缺失值处理
对于连续值的属性，若每个取值作为一个分支则显得不可行，因此需要进行离散化处理，常用的方法为二分法，基本思想为：给定样本集D与连续属性α，二分法试图找到一个划分点t将样本集D在属性α上分为≤t与＞t。  
* 首先将α的所有取值按升序排列，所有相邻的属性取值的均值作为候选划分点（n-1个，n为α所有的取值数目）。
* 计算每一个划分点划分集合D（即划分为两个分支）后的信息增益。
* 选择最大信息增益的划分点作为最优划分点。
![10](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型10.png?raw=true "Title"){:height="100px" width="400px"}  
现实中常会遇到不完整的样本，即某些属性值缺失。有时若简单采取剔除，则会造成大量的信息浪费，因此在属性值缺失的情况下需要解决两个问题：（1）如何选择划分属性。（2）给定划分属性，若某样本在该属性上缺失值，如何划分到具体的分支上。假定为样本集中的每一个样本都赋予一个权重，根节点中的权重初始化为1，则定义：  
![11](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型11.png?raw=true "Title"){:height="200px" width="500px"}  
对于（1）：通过在样本集D中选取在属性α上没有缺失值的样本子集，计算在该样本子集上的信息增益，最终的信息增益等于该样本子集划分后信息增益乘以样本子集占样本集的比重。即：  
![12](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型12.png?raw=true "Title"){:height="100px" width="400px"}  
对于（2）：若该样本子集在属性α上的值缺失，则将该样本以不同的权重（即每个分支所含样本比例）划入到所有分支节点中。该样本在分支节点中的权重变为：  
![13](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型13.png?raw=true "Title"){:height="100px" width="400px"}  

## 参考
1.周志华《机器学习》  
2.https://blog.csdn.net/u011826404/article/details/53606485



