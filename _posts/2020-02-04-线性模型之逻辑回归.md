---
layout: post
title: "线性模型之逻辑回归"
description: ""
category: 
tags: []
---
{% include JB/setup %}
## 定位
逻辑回归又名sigmoid回归、对数几率回归，是一种用于解决二分类问题的机器学习方法，用于估计某种事物的可能性。

## 由来
如何使用线性模型做分类任务呢？答案：只需找一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来。
逻辑回归与线性回归都是一种广义线性模型。逻辑回归假设因变量y服从伯努利分布，而线性回归假设因变量y服从高斯分布。逻辑回归是以线性回归为理论支持的，去掉sigmoid映射函数的话，逻辑回归就是一个线性回归，但是逻辑回归通过引入sigmoid函数引入了非线性因素，因此可以轻松处理0/1分类问题。

## 目标函数
考虑二分类任务，其输出标记y ∈ {0,1}，而线性回归模型产生的预测值z = wTx + b 是实值，于是，我们需将实值z转换为0/1值。最理想的是“单位阶跃函数(unit-step function)”  
![4](http://127.0.0.1:4000/images/逻辑回归4.png?raw=true "Title"){:height="100px" width="200px"}  
即若预测值z大于0就判断为正例，小于零就判断为反例，预测值为临界值零则可任意判别，如下图所示：  
![5](http://127.0.0.1:4000/images/逻辑回归5.jpg?raw=true "Title"){:height="300px" width="550px"}  
但是阶跃函数不连续，于是我们希望找到能在一定程度上近似单位阶跃函数的“替代函数”，并希望它单调可微。对数几率函数(logistic function)正是这样一个常用的替代函数：  
![6](http://127.0.0.1:4000/images/逻辑回归6.png?raw=true "Title"){:height="80px" width="150px"}  
对数几率函数是一种“Sigmoid 函数（即形似S的函数）”，它将z值转化为接近0或1的y值，并且其输出值在z = 0附近变化很陡，将z值带入上式得：  
![7](http://127.0.0.1:4000/images/逻辑回归7.png?raw=true "Title"){:height="80px" width="200px"} 

上式可变化为：  
![8](http://127.0.0.1:4000/images/逻辑回归8.png?raw=true "Title"){:height="80px" width="250px"} 

若将y视为样本x作为正例的可能性，则1-y是其反例可能性，两者的比值    
![9](http://127.0.0.1:4000/images/逻辑回归9.png?raw=true "Title"){:height="80px" width="80px"}   
称为“几率”，反映了x作为正例的相对可能性。对几率取对数则得到“对数几率（logit）”  
![10](http://127.0.0.1:4000/images/逻辑回归10.png?raw=true "Title"){:height="80px" width="80px"}   

所以对数几率函数实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率，因此，其对应的模型称为“对数几率回归（logistic regression）”

## 损失函数
若将hθ(x)视为类后验概率估计p(y=1|x;θ)，则有：  
p(y=1|x;θ) = hθ(x), y=1的概率  
p(y=0|x;θ) = 1 - hθ(x), y=0的概率  
所以有：  
![12](http://127.0.0.1:4000/images/逻辑回归12.png?raw=true "Title"){:height="40px" width="300px"}  

上面为样本的条件概率之积，即交叉熵，交叉熵是用来衡量两个概率分布之间的差异。交叉熵越大，两个分布之间的差异越大，越对实验结果感到意外，反之，交叉熵越小，两个分布越相似，越符合预期。

于是，我们可通过“极大似然法(maximum likelihood method)”来估计参数θ，损失函数原始形式如下：    
![2](http://127.0.0.1:4000/images/逻辑回归2.png?raw=true "Title"){:height="50px" width="400px"}   
即令**每个样本属于其真实标记的概率越大越好**。L表示所有训练样本的条件概率之积。

转化为对数形式（可以将连乘变成累加），即为对数损失函数：  
![3](http://127.0.0.1:4000/images/逻辑回归3.png?raw=true "Title"){:height="50px" width="400px"}

令![13](http://127.0.0.1:4000/images/逻辑回归13.png?raw=true "Title"){:height="50px" width="60px"} ，将最大值优化问题转换成最小值优化问题：  
![14](http://127.0.0.1:4000/images/逻辑回归14.png?raw=true "Title"){:height="60px" width="150px"} 

对数损失函数是关于θ的高阶可导连续凸函数，根据凸优化理论，经典的数值优化算法如梯度下降法（gradient descent method）、牛顿法（Newton method）等都可求得其最优解。

梯度：  
![15](http://127.0.0.1:4000/images/逻辑回归15.png?raw=true "Title"){:height="60px" width="350px"}  
更新：  
![16](http://127.0.0.1:4000/images/逻辑回归16.png?raw=true "Title"){:height="60px" width="400px"}

## 参考：  
1.https://blog.csdn.net/Zfq740695564/article/details/81783420  
2.周志华《机器学习》

