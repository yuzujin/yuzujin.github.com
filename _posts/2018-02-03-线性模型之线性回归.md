---
layout: post
title: "线性模型之线性回归"
description: ""
category: [机器学习]
tags: [线性模型]
---
{% include JB/setup %}

线性回归（linear regression）试图学得一个线性模型以尽可能准确地预测实值输出标记。  

我们使用从UCI Housing Data Set获得的波士顿房价数据集进行模型的训练和预测。下面的散点图展示了使用模型对部分房屋价格进行的预测。其中，每个点的横坐标表示同一类房屋真实价格的中位数，纵坐标表示线性回归模型根据特征预测的结果，当二者值完全相等的时候就会落在虚线上。所以模型预测得越准确，则点离虚线越近。  

![0](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归0.png?raw=true "Title"){:height="300px" width="450px"}{:height="200px" width="500px"}


## 输入一维，输出一维
我们先考虑一种最简单的情形：输入属性的数目只有一个。  
对于点对 ![lir1](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/lir1.png?raw=true "Title") , 其中xi ∈ R。对离散属性，若属性值间存在”序“关系，可通过连续化将其转化为连续值，例如二值属性”身高“的取值”高“ 和 ”矮“ 可转化为{1.0, 0.0}

有以下线性模型：  
![线性回归1](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归1.png?raw=true "Title")  
使得  
![线性回归2](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归2.png?raw=true "Title")  
当输入是很多个xi ，每个xi都有一个标签yi ，那么该线性模型，就是找一条直线拟合这些点，也就是上式中的f(x) ，对于每一个输出的f(xi) 与 yi ，可以设置不同的损失函数，我们以MSE（均方误差）为例：  
![线性回归3](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归3.png?raw=true "Title")  
均方误差有非常好的几何意义，它对应了常用的欧几里得距离或简称欧式距离。基于均方误差最小化来进行模型求解的方法称为”最小二乘法“（least square method）。在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。
那么使得 Loss 最小的 w 与 b ，就是最优模型参数。既：  
![线性回归4](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归4.png?raw=true "Title")   
求解方法，我们可以先将 Loss 对 w 与 b 求导：  
![线性回归5](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归5.png?raw=true "Title") 

直接令上式为0，求解 w 与 b ，就是“最小二乘法”；在一维输入，一维输出的情形下，通常直接使用最小二乘求出最优解。当输入输出维度变多后，可以使用梯度下降法迭代求解。

## 输入多维，输出一维
更一般的情形是样本有d个属性描述，对于点对![6](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归6.png?raw=true "Title")，其中![7](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归7.png?raw=true "Title")，有以下线性模型：  
![8](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归8.png?raw=true "Title")  
使得  
![9](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归9.png?raw=true "Title")  
为了便于讨论，我们把w和b吸收入向量形式w=(w;b),即  
![10](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归10.png?raw=true "Title")  
相应的，把数据集D表示为一个m x (d+1)大小的矩阵X，其中每行对应于一个示例，该行前d个元素对应于示例的d个属性值，最后一个元素恒置为1，即  
![11](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归11.png?raw=true "Title")  
上式可改写为：  
![12](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归12.png?raw=true "Title")  
由于 ***X*** 的维度是m x (n + 1) ， ***W*** 的维度是 (n + 1) x 1， 方程组的个数和求解未知数个数（***W***）不相等，上式无法使用传统求解方程组的算法来求解。这里同样使用最小二乘法，在等式两边同乘![16](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归16.png?raw=true "Title") ：  
![13](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归13.png?raw=true "Title")  
现在左边 ***W*** 的系数矩阵![17](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归17.png?raw=true "Title")的维度是(n+1) x (n+1) ，右边的矩阵维度也为 (n+1) x (n+1) ，并且此方程 ***W*** 的解正是最小二乘解，相应证明可参照数值代数中的详解。即该模型的参数最优解为：  
![14](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归14.png?raw=true "Title")  
该线性模型为：  
![15](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归15.png?raw=true "Title")  

## 输入多维，输出多维
对于点对![18](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归18.png?raw=true "Title") ，其中 ![19](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归19.png?raw=true "Title")，![20](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归20.png?raw=true "Title")，有以下线性模型：

![21](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归21.png?raw=true "Title")

这就是神经网络中的全连接层，用矩阵的形式可写为下式：

![22](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归22.png?raw=true "Title")

可简写为下式：

![23](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归23.png?raw=true "Title")

其中，***X*** 的维度为 m x (n + 1) ，***W*** 的维度为 (n + 1) x k ，***Y*** 的维度为 m x k 。多维输入，多维输出的线性模型，也就是全连接层，一般不使用最小二乘法求解，在神经网络寻优中，通常使用梯度下降法，求解参数。

类似于一维输入最小化均方误差公式，有：  
![24](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归24.png?raw=true "Title"){:height="50px" width="350px"}  
令![25](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归25.png?raw=true "Title"){:height="20px" width="200px"}，对w^求导得到：  
![26](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归26.png?raw=true "Title"){:height="50px" width="250px"}  
令上式为零可得w^最优解的闭式解，但由于涉及矩阵逆的计算，比单变量情形要复杂一些。  
当![29](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归29.png?raw=true "Title"){:height="20px" width="30px"}为满秩矩阵或正定矩阵时，令上式为零可得：  
![27](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归27.png?raw=true "Title"){:height="30px" width="150px"}  
其中![28](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归28.png?raw=true "Title"){:height="20px" width="30px"}是矩阵![29](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归29.png?raw=true "Title"){:height="20px" width="30px"}的逆矩阵。令xi^ = (xi:1)，则最终学得的多元线性回归模型为：  
![30](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归30.png?raw=true "Title"){:height="30px" width="200px"}

然而，现实任务中![29](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归29.png?raw=true "Title"){:height="20px" width="30px"}往往不是满秩矩阵。例如在许多任务中我们会遇到大量的变量，其数目甚至超过样例数，导致***X***的列数多于行数，![29](https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归29.png?raw=true "Title"){:height="20px" width="30px"}显然不满秩。此时可解出多个w^，它们都能使均方误差最小化。选择哪一个解作为输出，将由学习算法的归纳偏好决定，常见的做法是引入正则化项。

## 参考：
1. https://zhuanlan.zhihu.com/p/90917024
2. 周志华《机器学习》