

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <title>树型模型之GBDT &larr; </title>
   <meta name="author" content="Zujin" />

   <link rel="start" href="/" />

   <link rel="shortcut icon" href="/images/favicon.ico">

	
	
	
  	<link rel="alternate" type="application/atom+xml" href="http://feeds.feedburner.com/feedname" title="RSS feed" />
	
	

   <!-- syntax highlighting CSS -->
   <link rel="stylesheet" href="/assets/themes/mark-reid/css/syntax.css" type="text/css" />

   <!-- Homepage CSS -->
   <link rel="stylesheet" href="/assets/themes/mark-reid/css/screen.css" type="text/css" />

</head>
<body id="">
<div id="site">
  
  <div id="header">
    <h1>
    	<a href="/" title="Tumbler">Tumbler</a>
    	<span class="byline">&larr; <a href="/">Zujin</a></span>
    </h1>
    <ul class="nav">
      <li><a class="home" href="/">Home</a></li>
      <li><a  href="/archive.html">Archive</a></li>
      <li><a  href="/pages.html">Pages</a></li>
      <li><a  href="/categories.html">Categories</a></li>
      <li><a  href="/tags.html">Tags</a></li>
    </ul>
  </div>

  

<div id="page">
	
  <h1 class="emphnext">树型模型之GBDT</h1>
  <ul class="tag_box inline">
  
  


  
    
  



  </ul>

  
<h2 id="boosting">Boosting</h2>
<p>Boosting算法是一族可将弱学习器提升为强学习器的算法。Boosting族算法的工作机制如下：<br />
先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。</p>

<h2 id="adaboost">AdaBoost</h2>
<p>Boosting族算法最著名、使用最为广泛的就是AdaBoost(英文全称：Adaptive Boosting)，因此下面主要是对AdaBoost算法进行介绍。AdaBoost使用的是指数损失函数，因此AdaBoost的权值与样本分布的更新都是围绕着最小化指数损失函数进行的。其实现思路如下(类比Boosting的工作机制)：</p>

<pre><code>利用“重赋权法”（re-weighting),即在训练过程的每一轮中，根据样本分布为每个训练样本重新赋予一个权重。
首先，初始化训练集权重分布。即先对原始训练集中的每个样本赋予相同的权重，假如训练集有N个样本，那么每个样本的初始权重均为1/N, 
其次，循环训练弱分类器。将加有权重的训练集用于训练弱学习器，在训练过程中，如果某个样本能够准确地分类和预测，那么在构造下一个训练集的过程中，就要降低该样本的权重，同时增加无法准确分类或预测的样本的权重。重新确定样本权重后，然后进行下一次弱学习器的训练。如此重复进行，直至基学习器数目达到事先指定的值T或者准确率。
最后，组合生成的多个弱学习器得到强学习器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。
</code></pre>

<p>AdaBoost算法编程流程图:<br />
<img src="https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT1.jpeg?raw=true" alt="GBDT" title="Title" height="400px" width="650px" /><br />
假定给出了一个二分类训练数据集：<br />
<img src="https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT2.png?raw=true" alt="GBDT" title="Title" height="50px" width="400px" /><br />
其中yi属于二分类的标记组合，即yi∈{-1,+1}。<br />
<strong>1. Step1 初始化训练数据的权值分布</strong>  <br />
   首先将训练集中每一个样本赋予相同的权重，即1/N，即每个训练样本在基本分类器的学习中作用相同，用数学化的语言表示为：  <br />
    <img src="https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT3.png?raw=true" alt="GBDT" title="Title" height="50px" width="400px" /></p>

<p><strong>2. Step2 迭代训练基学习器</strong><br />
   用m表示迭代的轮数，其中m = 1,2,3,…,M<br />
   2.1. 使用具有权值分布Dm的训练数据集学习，得到基本分类器   <br />
   <img src="https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT4.png?raw=true" alt="GBDT" title="Title" height="50px" width="400px" /><br />
   2.2. 计算得到的基分类器Gm(x)在训练集上的分类误差率em<br />
   <img src="https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT5.png?raw=true" alt="GBDT" title="Title" height="200px" width="400px" /> <br />
   以上公式表明，Gm(x)在加权的训练数据集上的分类误差率是被Gm(x)误分类样本的权值之和。<br />
   2.3.计算Gm前面的权重系数am，该系数表示Gm在最终分类器中的重要程度，目的在于使我们得到基分类器在最终分类其中所占的权值，系数计算公式如下：<br />
   <img src="https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT6.png?raw=true" alt="GBDT" title="Title" height="50px" width="400px" /><br />
   由表达式可知，当em≤1/2时，am≥0，并且am随着em的减小而增大，意味着分类误差越小的基本分类器在最终分类器的作用越大，而e_m≥1/2则刚好相反，这正好验证了集成学习中每个个体分类器的分类精度必须大于0.5的前提条件。<br />
   2.4.更新训练数据集的权值分布（目的：得到样本的新的权值分布），用于下一轮迭代。
   <img src="https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT7.png?raw=true" alt="GBDT" title="Title" height="200px" width="400px" /><br />
   Zm是我们引入的一个规范化因子，它的作用在于使Dm+1 成为一个概率分布,公式如下：<br />
    <img src="https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT8.png?raw=true" alt="GBDT" title="Title" height="60px" width="400px" /><br />
   由于是二分类，所以对上式可进一步简化为：<br />
    <img src="https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT9.png?raw=true" alt="GBDT" title="Title" height="80px" width="400px" /><br />
    由此可知，被基本分类器G_m(x)误分类样本的权值得以扩大，而被正确分类样本的权值得以缩小。两两比较，误分类样本的权值为：<br />
    <img src="https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT10.png?raw=true" alt="GBDT" title="Title" height="60px" width="100px" /> <br />
    因此，误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作业，这是AdaBoost的一个特点。<br />
   2.5.重复步骤二中的1至4步骤，得到一系列的权重参数am和基分类器Gm。</p>

<p><strong>3.Step3 根据权重参数线性组合各个基学习器</strong><br />
   <img src="https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT11.png?raw=true" alt="GBDT" title="Title" height="300px" width="600px" /><br />
   线性组合f(x)实现了M个基本分类器的加权表决。系数am表示了基本分类器Gm(x)的重要性，这里，所有的am之和并不为1。f(x)的符号决定实例x的类，f(x)的绝对值表示分类的确信度，利用基本分类器的线性组合构建最终分类器是AdaBoost的另一特点。</p>

<h2 id="提升树">提升树</h2>
<p>提升树是以分类树或者回归树为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一。<br />
   <strong>提升树模型</strong><br />
   提升方法实际采用加法模型（即基函数的线性组合）与前向分步算法。以决策树为基函数的提升方法称为提升树（boosting tree）.对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。提升树模型可以表示为决策树的加法模型：<br />
   <img src="https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT12.png?raw=true" alt="GBDT" title="Title" height="60px" width="200px" /><br />
   其中T(x;Θm)表示决策树；Θm为决策树的参数；M为树的个数。<br />
   <strong>提升树算法</strong><br />
   提升树采用前向分布算法。首先确定初始提升树f0(x)=0，第m步的模型是<br />
   <img src="https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT13.png?raw=true" alt="GBDT" title="Title" height="40px" width="300px" /> <br />
   其中，fm-1(x)为当前模型，通过经验风险极小化确定下一棵决策树的参数Θm，<br />
   <img src="https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT14.png?raw=true" alt="GBDT" title="Title" height="70px" width="400px" /><br />
   由于树的线性组合可以很好地拟合训练数据，即使数据中的输入和输出之间的关系很复杂也是如此，所以提升树是一个高功能的学习算法。<br />
   下面讨论针对不同问题的提升树学习方法，其主要区别在于使用的损失函数不同。包括用平方误差损失函数的回归问题，用指数损失函数的分类问题，以及用一般损失函数的一般决策问题。<br />
   回归问题的提升树算法：<br />
   <img src="https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT15.jpeg?raw=true" alt="GBDT" title="Title" height="350px" width="600px" /><br />
   对回归问题的提升树算法来说，只需简单低拟合当前模型残差。</p>
<h2 id="梯度提升gradient-boosting">梯度提升（Gradient Boosting）</h2>
<p>提升树利用加法模型与前向分布算法实现学习的优化过程。当损失函数是平方损失和指数损失函数时，每一步优化是很简单的。但对一般损失函数而言，往往每一步优化并不那么容易。针对这一问题，Freidman提出了梯度提升(gradient boosting)算法。这是利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前魔性的值：<br />
   <img src="https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT15.png?raw=true" alt="GBDT" title="Title" height="100px" width="400px" /><br />
   作为回归问题提升树算法中的残差的近似值，拟合一个回归树。<br />
   梯度提升算法：<br />
   <img src="https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT16.png?raw=true" alt="GBDT" title="Title" height="500px" width="600px" /><br />
   算法第1步初始化，估计使损失函数极小化的常数值，它是只有一个根结点的树，即 x&gt;c 和 x&lt;c；
   第2 (a)步计算损失函数的负梯度在当前模型的值，将它作为残差的估计。对于平方损失函数，它就是通常所说的残差；对于一般损失函数，它就是残差的近似值。
   第2 (b)步估计回归树叶结点区域，以拟合残差的近似值。
   第2 (c)步利用线性搜索估计叶结点区域的值，使损失函数极小化。
   第2 (d)步更新回归树。
   第3步得到输出的最终模型。</p>

<h2 id="参考">参考</h2>
<p>1.李航 《统计学习方法》
2.https://blog.csdn.net/u011826404/article/details/70172971
3.http://www.imooc.com/article/258253</p>


  <address class="signature">
    <a class="author" href="/">Zujin</a> 
    <span class="date">15 February 2020</span>
    <span class="location"></span>
  </address>
  
  <div class="prev-next">
  
    <a href="/2020/02/16/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" class="next" title="神经网络">Next Post &rarr;</a>
  
  
    <a href="/2020/02/14/%E6%A0%91%E5%9E%8B%E6%A8%A1%E5%9E%8B%E4%B9%8B%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97" class="prev" title="树型模型之随机森林">&larr; Earlier Post</a>
  
  </div>
  
</div><!-- End Page -->




  <!--
  <div id="footer">
  	<address>
  		<span class="copyright">
  			Content by <a href="/info/site.html">Zujin</a>. Design by 
  			<a href="http://mark.reid.name/">Mark Reid</a>
  			<br/>
  			(<a rel="licence" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">Some rights reserved</a>)			
  		</span>
  		<span class="engine">
  			Powered by <a href="http://github.com/mojombo/jekyll/" title="A static, minimalist CMS">Jekyll</a>
  		</span>
  	</address>
  </div>
  -->
  
</div>

<!--[if IE 6]>
<script type="text/javascript"> 
	/*Load jQuery if not already loaded*/ if(typeof jQuery == 'undefined'){ document.write("<script type=\"text/javascript\"   src=\"http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js\"></"+"script>"); var __noconflict = true; } 
	var IE6UPDATE_OPTIONS = {
		icons_path: "http://static.ie6update.com/hosted/ie6update/images/"
	}
</script>
<script type="text/javascript" src="http://static.ie6update.com/hosted/ie6update/ie6update.js"></script>
<![endif]-->

  



</body>
</html>

