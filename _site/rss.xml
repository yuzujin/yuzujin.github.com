<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
        <title>Tumbler</title>
        <description>Tumbler - Zujin</description>
        <link>http://username.github.io</link>
        <atom:link href="http://username.github.io/rss.xml" rel="self" type="application/rss+xml" />
        <lastBuildDate>Sun, 16 Feb 2020 17:09:14 +0800</lastBuildDate>
        <pubDate>Sun, 16 Feb 2020 17:09:14 +0800</pubDate>
        <ttl>60</ttl>


        <item>
                <title>神经网络</title>
                <description>
&lt;h2 id=&quot;神经网络&quot;&gt;神经网络&lt;/h2&gt;
&lt;p&gt;生物学中的神经网络是互相交换信息的相互连接的神经元。这个想法现在已经适用于机器学习的世界，并被称为人工神经网络（ANN）。深度学习（deep learning）是一个经常出现的词，是指几层连续放置的人工神经网络。&lt;/p&gt;

&lt;p&gt;人工神经网络（ANN）包含了许多可以学习类似人脑的认知能力的模型。其它算法不能处理的极其复杂的任务（如图像识别），神经网络就可以办到。然而，就像人类的大脑，它需要很长时间来训练模型，且需要很多的能量（想一想我们为了保持大脑的工作，我们吃了多少东西）。 &lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/神经网络.jpeg?raw=true&quot; alt=&quot;神经网络&quot; title=&quot;Title&quot; height=&quot;200px&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;神经元模型&quot;&gt;神经元模型&lt;/h2&gt;
&lt;p&gt;神经网络中最基本的单元是神经元模型（neuron）。在生物神经网络的原始机制中，每个神经元通常都有多个树突（dendrite），一个轴突（axon）和一个细胞体（cell body），树突短而多分支，轴突长而只有一个；在功能上，树突用于传入其它神经元传递的神经冲动，而轴突用于将神经冲动传出到其它神经元，当树突或细胞体传入的神经冲动使得神经元兴奋时，该神经元就会通过轴突向其它神经元传递兴奋。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/神经网络1.png?raw=true&quot; alt=&quot;神经网络&quot; title=&quot;Title&quot; height=&quot;400px&quot; width=&quot;650px&quot; /&gt;&lt;br /&gt;
一直沿用至今的“M-P神经元模型”正是对这一结构进行了抽象，也称“阈值逻辑单元“，其中树突对应于输入部分，每个神经元收到n个其他神经元传递过来的输入信号，这些信号通过带权重的连接传递给细胞体，这些权重又称为连接权（connection weight）。细胞体分为两部分，前一部分计算总输入值（即输入信号的加权和，或者说累积电平），后一部分先计算总输入值与该神经元阈值的差值，然后通过激活函数（activation function）的处理，产生输出从轴突传送给其它神经元。M-P神经元模型如下图所示：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/神经网络2.jpg?raw=true&quot; alt=&quot;神经网络&quot; title=&quot;Title&quot; height=&quot;300px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
与线性分类十分相似，神经元模型最理想的激活函数也是阶跃函数，即将神经元输入值与阈值的差值映射为输出值1或0，若差值大于零输出1，对应兴奋；若差值小于零则输出0，对应抑制。但阶跃函数不连续，不光滑，故在M-P神经元模型中，也采用Sigmoid函数来近似， Sigmoid函数将较大范围内变化的输入值挤压到 (0,1) 输出值范围内，所以也称为挤压函数（squashing function）。&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/神经网络3.jpg?raw=true&quot; alt=&quot;神经网络&quot; title=&quot;Title&quot; height=&quot;300px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
将多个神经元按一定的层次结构连接起来，就得到了神经网络。它是一种包含多个参数的模型，比方说10个神经元两两连接，则有100个参数需要学习（每个神经元有9个连接权以及1个阈值），若将每个神经元都看作一个函数，则整个神经网络就是由这些函数相互嵌套而成。&lt;/p&gt;

&lt;h2 id=&quot;感知机与多层网络&quot;&gt;感知机与多层网络&lt;/h2&gt;
&lt;p&gt;感知机（Perceptron）是由两层神经元组成的一个简单模型，但只有输出层是M-P神经元，即只有输出层神经元进行激活函数处理，也称为功能神经元（functional neuron）；输入层只是接受外界信号（样本属性）并传递给输出层（输入层的神经元个数等于样本的属性数目），而没有激活函数。这样一来，感知机与之前线性模型中的对数几率回归的思想基本是一样的，都是通过对属性加权与另一个常数求和，再使用sigmoid函数将这个输出值压缩到0-1之间，从而解决分类问题。不同的是感知机的输出层应该可以有多个神经元，从而可以实现多分类问题，同时两个模型所用的参数估计方法十分不同。&lt;/p&gt;

&lt;p&gt;给定训练集，则感知机的n+1个参数（n个权重+1个阈值）都可以通过学习得到。阈值Θ可以看作一个输入值固定为-1的哑结点的权重ωn+1，即假设有一个固定输入xn+1=-1的输入层神经元，其对应的权重为ωn+1，这样就把权重和阈值统一为权重的学习了。简单感知机的结构如下图所示：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/神经网络4.png?raw=true&quot; alt=&quot;神经网络&quot; title=&quot;Title&quot; height=&quot;300px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
感知机权重的学习规则如下：对于训练样本（x，y），当该样本进入感知机学习后，会产生一个输出值，若该输出值与样本的真实标记不一致，则感知机会对权重进行调整，若激活函数为阶跃函数，则调整的方法为（基于梯度下降法）：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/神经网络5.png?raw=true&quot; alt=&quot;神经网络&quot; title=&quot;Title&quot; height=&quot;500px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
其中 η∈（0，1）称为学习率，可以看出感知机是通过逐个样本输入来更新权重，首先设定好初始权重（一般为随机），逐个地输入样本数据，若输出值与真实标记相同则继续输入下一个样本，若不一致则更新权重，然后再重新逐个检验，直到每个样本数据的输出值都与真实标记相同。容易看出：感知机模型总是能将训练数据的每一个样本都预测正确，和决策树模型总是能将所有训练数据都分开一样，感知机模型很容易产生过拟合问题。&lt;/p&gt;

&lt;p&gt;由于感知机模型只有一层功能神经元，因此其功能十分有限，只能处理线性可分的问题，对于这类问题，感知机的学习过程一定会收敛（converge），因此总是可以求出适当的权值。但是对于像书上提到的异或问题，只通过一层功能神经元往往不能解决，因此要解决非线性可分问题，需要考虑使用多层功能神经元，即神经网络。多层神经网络的拓扑结构如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/神经网络6.png?raw=true&quot; alt=&quot;神经网络&quot; title=&quot;Title&quot; height=&quot;400px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
在神经网络中，输入层与输出层之间的层称为隐含层或隐层（hidden layer），隐层和输出层的神经元都是具有激活函数的功能神经元。只需包含一个隐层便可以称为多层神经网络，常用的神经网络称为“多层前馈神经网络”（multi-layer feedforward neural network），该结构满足以下几个特点：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;每层神经元与下一层神经元之间完全互连&lt;/li&gt;
  &lt;li&gt;神经元之间不存在同层连接&lt;/li&gt;
  &lt;li&gt;神经元之间不存在跨层连接&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/神经网络7.png?raw=true&quot; alt=&quot;神经网络&quot; title=&quot;Title&quot; height=&quot;400px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
根据上面的特点可以得知：这里的“前馈”指的是网络拓扑结构中不存在环或回路，而不是指该网络只能向前传播而不能向后传播（下节中的BP神经网络正是基于前馈神经网络而增加了反馈调节机制）。神经网络的学习过程就是根据训练数据来调整神经元之间的“连接权”以及每个神经元的阈值，换句话说：神经网络所学习到的东西都蕴含在网络的连接权与阈值中。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bp神经网络算法&quot;&gt;BP神经网络算法&lt;/h2&gt;
&lt;p&gt;由上面可以得知：神经网络的学习主要蕴含在权重和阈值中，多层网络使用上面简单感知机的权重调整规则显然不够用了，BP神经网络算法即误差逆传播算法（error BackPropagation）正是为学习多层前馈神经网络而设计，BP神经网络算法是迄今为止最成功的的神经网络学习算法。&lt;/p&gt;

&lt;p&gt;一般而言，只需包含一个足够多神经元的隐层，就能以任意精度逼近任意复杂度的连续函数[Hornik et al.,1989]，故下面以训练单隐层的前馈神经网络为例，介绍BP神经网络的算法思想。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/神经网络8.png?raw=true&quot; alt=&quot;神经网络&quot; title=&quot;Title&quot; height=&quot;400px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
上图为一个单隐层前馈神经网络的拓扑结构，BP神经网络算法也使用梯度下降法（gradient descent），以单个样本的均方误差的负梯度方向对权重进行调节。可以看出：BP算法首先将误差反向传播给隐层神经元，调节隐层到输出层的连接权重与输出层神经元的阈值；接着根据隐含层神经元的均方误差，来调节输入层到隐含层的连接权值与隐含层神经元的阈值。BP算法基本的推导过程与感知机的推导过程原理是相同的，下面给出调整隐含层到输出层的权重调整规则的推导过程：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/神经网络9.png?raw=true&quot; alt=&quot;神经网络&quot; title=&quot;Title&quot; height=&quot;400px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
学习率η∈（0，1）控制着沿反梯度方向下降的步长，若步长太大则下降太快容易产生震荡，若步长太小则收敛速度太慢，一般地常把η设置为0.1，有时更新权重时会将输出层与隐含层设置为不同的学习率。BP算法的基本流程如下所示：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/神经网络10.png?raw=true&quot; alt=&quot;神经网络&quot; title=&quot;Title&quot; height=&quot;500px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
BP算法的更新规则是基于每个样本的预测值与真实类标的均方误差来进行权值调节，即BP算法每次更新只针对于单个样例。需要注意的是：BP算法的最终目标是要最小化整个训练集D上的累积误差，即：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/神经网络11.png?raw=true&quot; alt=&quot;神经网络&quot; title=&quot;Title&quot; height=&quot;60px&quot; width=&quot;200px&quot; /&gt;&lt;br /&gt;
如果基于累积误差最小化的更新规则，则得到了累积误差逆传播算法（accumulated error backpropagation），即每次读取全部的数据集一遍，进行一轮学习，从而基于当前的累积误差进行权值调整，因此参数更新的频率相比标准BP算法低了很多，但在很多任务中，尤其是在数据量很大的时候，往往标准BP算法会获得较好的结果。另外对于如何设置隐层神经元个数的问题，至今仍然没有好的解决方案，常使用“试错法”进行调整。
前面提到，BP神经网络强大的学习能力常常容易造成过拟合问题，有以下两种策略来缓解BP网络的过拟合问题：&lt;/p&gt;

&lt;p&gt;早停：将数据分为训练集与测试集，训练集用于学习，测试集用于评估性能，若在训练过程中，训练集的累积误差降低，而测试集的累积误差升高，则停止训练。&lt;/p&gt;

&lt;p&gt;引入正则化（regularization）：基本思想是在累积误差函数中增加一个用于描述网络复杂度的部分，例如所有权值与阈值的平方和，其中λ∈（0,1）用于对累积经验误差与网络复杂度这两项进行折中，常通过交叉验证法来估计。
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/神经网络12.png?raw=true&quot; alt=&quot;神经网络&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;600px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;全局最小和局部最小&quot;&gt;全局最小和局部最小&lt;/h2&gt;
&lt;p&gt;模型学习的过程实质上就是一个寻找最优参数的过程，例如BP算法试图通过最速下降来寻找使得累积经验误差最小的权值与阈值，在谈到最优时，一般会提到局部极小（local minimum）和全局最小（global minimum）。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;局部极小解：参数空间中的某个点，其邻域点的误差函数值均不小于该点的误差函数值。&lt;/li&gt;
  &lt;li&gt;全局最小解：参数空间中的某个点，所有其他点的误差函数值均不小于该点的误差函数值。
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/神经网络13.png?raw=true&quot; alt=&quot;神经网络&quot; title=&quot;Title&quot; height=&quot;400px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
要成为局部极小点，只要满足该点在参数空间中的梯度为零。局部极小可以有多个，而全局最小只有一个。全局最小一定是局部极小，但局部最小却不一定是全局最小。显然在很多机器学习算法中，都试图找到目标函数的全局最小。梯度下降法的主要思想就是沿着负梯度方向去搜索最优解，负梯度方向是函数值下降最快的方向，若迭代到某处的梯度为0，则表示达到一个局部最小，参数更新停止。因此在现实任务中，通常使用以下策略尽可能地去接近全局最小。&lt;/li&gt;
  &lt;li&gt;以多组不同参数值初始化多个神经网络，按标准方法训练，迭代停止后，取其中误差最小的解作为最终参数。&lt;/li&gt;
  &lt;li&gt;使用“模拟退火”技术，这里不做具体介绍。&lt;/li&gt;
  &lt;li&gt;使用随机梯度下降，即在计算梯度时加入了随机因素，使得在局部最小时，计算的梯度仍可能不为0，从而迭代可以继续进行。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;深度学习&quot;&gt;深度学习&lt;/h2&gt;
&lt;p&gt;理论上，参数越多，模型复杂度就越高，容量（capability）就越大，从而能完成更复杂的学习任务。深度学习（deep learning）正是一种极其复杂而强大的模型。&lt;/p&gt;

&lt;p&gt;怎么增大模型复杂度呢？两个办法，一是增加隐层的数目，二是增加隐层神经元的数目。前者更有效一些，因为它不仅增加了功能神经元的数量，还增加了激活函数嵌套的层数。但是对于多隐层神经网络，经典算法如标准BP算法往往会在误差逆传播时发散（diverge），无法收敛达到稳定状态。&lt;/p&gt;

&lt;p&gt;那要怎么有效地训练多隐层神经网络呢？一般来说有以下两种方法：&lt;/p&gt;

&lt;p&gt;无监督逐层训练（unsupervised layer-wise training）：每次训练一层隐节点，把上一层隐节点的输出当作输入来训练，本层隐结点训练好后，输出再作为下一层的输入来训练，这称为预训练（pre-training）。全部预训练完成后，再对整个网络进行微调（fine-tuning）训练。一个典型例子就是深度信念网络（deep belief network，简称DBN）。这种做法其实可以视为把大量的参数进行分组，先找出每组较好的设置，再基于这些局部最优的结果来训练全局最优。&lt;/p&gt;

&lt;p&gt;权共享（weight sharing）：令同一层神经元使用完全相同的连接权，典型的例子是卷积神经网络（Convolutional Neural Network，简称CNN）。这样做可以大大减少需要训练的参数数目。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/神经网络14.png?raw=true&quot; alt=&quot;神经网络&quot; title=&quot;Title&quot; height=&quot;200px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
深度学习可以理解为一种特征学习（feature learning）或者表示学习（representation learning），无论是DBN还是CNN，都是通过多个隐层来把与输出目标联系不大的初始输入转化为与输出目标更加密切的表示，使原来只通过单层映射难以完成的任务变为可能。即通过多层处理，逐渐将初始的“低层”特征表示转化为“高层”特征表示，从而使得最后可以用简单的模型来完成复杂的学习任务。&lt;/p&gt;

&lt;p&gt;传统任务中，样本的特征需要人类专家来设计，这称为特征工程（feature engineering）。特征好坏对泛化性能有至关重要的影响。而深度学习为全自动数据分析带来了可能，可以自动产生更好的特征。&lt;/p&gt;

&lt;h2 id=&quot;其他常见神经网络&quot;&gt;其他常见神经网络&lt;/h2&gt;
&lt;p&gt;1.RBF网络   &lt;br /&gt;
RBF(径向基函数)网络是一种单隐层前馈神经网络，使用径向基函数作为隐层神经元激活函数，而输出层是对隐层神经元输出的线性组合。假设输入为d维向量x，输出为实值，则RBF网络定义为&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/神经网络15.png?raw=true&quot; alt=&quot;神经网络&quot; title=&quot;Title&quot; height=&quot;400px&quot; width=&quot;600px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;2.ART网络&lt;br /&gt;
竞争性学习是神经网络中一种常用的无监督学习策略，网络的输出神经元相互竞争，每一时刻仅有一个竞争胜利的神经元被激活，其他神经元被抑制，称为“胜者通吃”原则。&lt;/p&gt;

&lt;p&gt;ART(自适应谐振理论)网络是竞争性学习的重要代表，该网络由比较层、识别层、识别阈值和重置模块构成。比较层负责接收输入样本，传递给识别层神经元。识别层每个神经元对应一个模式类，识别层神经元之间相互竞争产生获胜神经元。&lt;/p&gt;

&lt;p&gt;3.SOM网络&lt;br /&gt;
SOM(自组织映射)网络将高维输入数据映射到低维空间(通常为二维)，同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元。&lt;/p&gt;

&lt;p&gt;训练目标:为每个输出层神经元找到合适的权向量，达到保持拓扑结构的目的&lt;/p&gt;

&lt;p&gt;训练过程：每个输出层神经元计算接收的训练样本与自身携带的权向量的距离。距离最近的神经元竞争获胜，称最佳匹配单元。最佳匹配单元与邻近神经元的权向量会被调整使得权向量与当前输入样本距离减小，过程不断迭代直至收敛&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/神经网络16.png?raw=true&quot; alt=&quot;神经网络&quot; title=&quot;Title&quot; height=&quot;350px&quot; width=&quot;600px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;4.级联相关网络&lt;br /&gt;
级联相关网络是结构自适应网络的代表&lt;/p&gt;

&lt;p&gt;级联是指建立层次连接的层级结构，刚开始训练时网络只有输入层和输出层，是最小拓扑结构，随着训练的进行，加入新的隐层神经元，其输入端连接权值是固定的。相关是指通过最大化新神经元的输出与网络误差之间的相关性来训练相关的参数&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/神经网络17.png?raw=true&quot; alt=&quot;神经网络&quot; title=&quot;Title&quot; height=&quot;350px&quot; width=&quot;600px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;5.Elman网络&lt;br /&gt;
递归神经网络允许网络中出现环形结构，从而使神经元的输出反馈可作为输入信号。Elman网络是常用的递归神经网络。&lt;/p&gt;

&lt;p&gt;如图5.14，隐层神经元的输出被反馈回来，与下一时刻输入层神经元提供的信号一起作为隐层神经元在下一刻的输入。隐层神经元常使用Sigmoid激活函数&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/神经网络18.png?raw=true&quot; alt=&quot;神经网络&quot; title=&quot;Title&quot; height=&quot;350px&quot; width=&quot;600px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;6.Boltzmann机&lt;br /&gt;
如图5.14(a)所示，Boltzmann机的神经元分为显层和隐层，显层用于数据的输入与输出，隐层是数据的内在表达。其神经元是布尔型的，状态1表示激活，0表示抑制。受限Boltzmann机仅保留显层和隐层的连接，将Boltzmann机结构由完全图简化为二部图&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/神经网络19.png?raw=true&quot; alt=&quot;神经网络&quot; title=&quot;Title&quot; height=&quot;350px&quot; width=&quot;600px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;周志华 《机器学习》&lt;/li&gt;
  &lt;li&gt;http://www.imooc.com/article/258253&lt;/li&gt;
  &lt;li&gt;https://blog.csdn.net/u011826404/article/details/53767428&lt;/li&gt;
&lt;/ol&gt;

</description>
                <link>http://username.github.io/2020/02/16/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C</link>
                <guid>http://username.github.io/2020/02/16/神经网络</guid>
                <pubDate>Sun, 16 Feb 2020 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>树型模型之GBDT</title>
                <description>
&lt;h2 id=&quot;boosting&quot;&gt;Boosting&lt;/h2&gt;
&lt;p&gt;Boosting算法是一族可将弱学习器提升为强学习器的算法。Boosting族算法的工作机制如下：&lt;br /&gt;
先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。&lt;/p&gt;

&lt;h2 id=&quot;adaboost&quot;&gt;AdaBoost&lt;/h2&gt;
&lt;p&gt;Boosting族算法最著名、使用最为广泛的就是AdaBoost(英文全称：Adaptive Boosting)，因此下面主要是对AdaBoost算法进行介绍。AdaBoost使用的是指数损失函数，因此AdaBoost的权值与样本分布的更新都是围绕着最小化指数损失函数进行的。其实现思路如下(类比Boosting的工作机制)：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;利用“重赋权法”（re-weighting),即在训练过程的每一轮中，根据样本分布为每个训练样本重新赋予一个权重。
首先，初始化训练集权重分布。即先对原始训练集中的每个样本赋予相同的权重，假如训练集有N个样本，那么每个样本的初始权重均为1/N, 
其次，循环训练弱分类器。将加有权重的训练集用于训练弱学习器，在训练过程中，如果某个样本能够准确地分类和预测，那么在构造下一个训练集的过程中，就要降低该样本的权重，同时增加无法准确分类或预测的样本的权重。重新确定样本权重后，然后进行下一次弱学习器的训练。如此重复进行，直至基学习器数目达到事先指定的值T或者准确率。
最后，组合生成的多个弱学习器得到强学习器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;AdaBoost算法编程流程图:&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT1.jpeg?raw=true&quot; alt=&quot;GBDT&quot; title=&quot;Title&quot; height=&quot;400px&quot; width=&quot;650px&quot; /&gt;&lt;br /&gt;
假定给出了一个二分类训练数据集：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT2.png?raw=true&quot; alt=&quot;GBDT&quot; title=&quot;Title&quot; height=&quot;50px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
其中yi属于二分类的标记组合，即yi∈{-1,+1}。&lt;br /&gt;
&lt;strong&gt;1. Step1 初始化训练数据的权值分布&lt;/strong&gt;  &lt;br /&gt;
   首先将训练集中每一个样本赋予相同的权重，即1/N，即每个训练样本在基本分类器的学习中作用相同，用数学化的语言表示为：  &lt;br /&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT3.png?raw=true&quot; alt=&quot;GBDT&quot; title=&quot;Title&quot; height=&quot;50px&quot; width=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Step2 迭代训练基学习器&lt;/strong&gt;&lt;br /&gt;
   用m表示迭代的轮数，其中m = 1,2,3,…,M&lt;br /&gt;
   2.1. 使用具有权值分布Dm的训练数据集学习，得到基本分类器   &lt;br /&gt;
   &lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT4.png?raw=true&quot; alt=&quot;GBDT&quot; title=&quot;Title&quot; height=&quot;50px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
   2.2. 计算得到的基分类器Gm(x)在训练集上的分类误差率em&lt;br /&gt;
   &lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT5.png?raw=true&quot; alt=&quot;GBDT&quot; title=&quot;Title&quot; height=&quot;200px&quot; width=&quot;400px&quot; /&gt; &lt;br /&gt;
   以上公式表明，Gm(x)在加权的训练数据集上的分类误差率是被Gm(x)误分类样本的权值之和。&lt;br /&gt;
   2.3.计算Gm前面的权重系数am，该系数表示Gm在最终分类器中的重要程度，目的在于使我们得到基分类器在最终分类其中所占的权值，系数计算公式如下：&lt;br /&gt;
   &lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT6.png?raw=true&quot; alt=&quot;GBDT&quot; title=&quot;Title&quot; height=&quot;50px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
   由表达式可知，当em≤1/2时，am≥0，并且am随着em的减小而增大，意味着分类误差越小的基本分类器在最终分类器的作用越大，而e_m≥1/2则刚好相反，这正好验证了集成学习中每个个体分类器的分类精度必须大于0.5的前提条件。&lt;br /&gt;
   2.4.更新训练数据集的权值分布（目的：得到样本的新的权值分布），用于下一轮迭代。
   &lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT7.png?raw=true&quot; alt=&quot;GBDT&quot; title=&quot;Title&quot; height=&quot;200px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
   Zm是我们引入的一个规范化因子，它的作用在于使Dm+1 成为一个概率分布,公式如下：&lt;br /&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT8.png?raw=true&quot; alt=&quot;GBDT&quot; title=&quot;Title&quot; height=&quot;60px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
   由于是二分类，所以对上式可进一步简化为：&lt;br /&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT9.png?raw=true&quot; alt=&quot;GBDT&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
    由此可知，被基本分类器G_m(x)误分类样本的权值得以扩大，而被正确分类样本的权值得以缩小。两两比较，误分类样本的权值为：&lt;br /&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT10.png?raw=true&quot; alt=&quot;GBDT&quot; title=&quot;Title&quot; height=&quot;60px&quot; width=&quot;100px&quot; /&gt; &lt;br /&gt;
    因此，误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作业，这是AdaBoost的一个特点。&lt;br /&gt;
   2.5.重复步骤二中的1至4步骤，得到一系列的权重参数am和基分类器Gm。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3.Step3 根据权重参数线性组合各个基学习器&lt;/strong&gt;&lt;br /&gt;
   &lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT11.png?raw=true&quot; alt=&quot;GBDT&quot; title=&quot;Title&quot; height=&quot;300px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
   线性组合f(x)实现了M个基本分类器的加权表决。系数am表示了基本分类器Gm(x)的重要性，这里，所有的am之和并不为1。f(x)的符号决定实例x的类，f(x)的绝对值表示分类的确信度，利用基本分类器的线性组合构建最终分类器是AdaBoost的另一特点。&lt;/p&gt;

&lt;h2 id=&quot;提升树&quot;&gt;提升树&lt;/h2&gt;
&lt;p&gt;提升树是以分类树或者回归树为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一。&lt;br /&gt;
   &lt;strong&gt;提升树模型&lt;/strong&gt;&lt;br /&gt;
   提升方法实际采用加法模型（即基函数的线性组合）与前向分步算法。以决策树为基函数的提升方法称为提升树（boosting tree）.对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。提升树模型可以表示为决策树的加法模型：&lt;br /&gt;
   &lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT12.png?raw=true&quot; alt=&quot;GBDT&quot; title=&quot;Title&quot; height=&quot;60px&quot; width=&quot;200px&quot; /&gt;&lt;br /&gt;
   其中T(x;Θm)表示决策树；Θm为决策树的参数；M为树的个数。&lt;br /&gt;
   &lt;strong&gt;提升树算法&lt;/strong&gt;&lt;br /&gt;
   提升树采用前向分布算法。首先确定初始提升树f0(x)=0，第m步的模型是&lt;br /&gt;
   &lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT13.png?raw=true&quot; alt=&quot;GBDT&quot; title=&quot;Title&quot; height=&quot;40px&quot; width=&quot;300px&quot; /&gt; &lt;br /&gt;
   其中，fm-1(x)为当前模型，通过经验风险极小化确定下一棵决策树的参数Θm，&lt;br /&gt;
   &lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT14.png?raw=true&quot; alt=&quot;GBDT&quot; title=&quot;Title&quot; height=&quot;70px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
   由于树的线性组合可以很好地拟合训练数据，即使数据中的输入和输出之间的关系很复杂也是如此，所以提升树是一个高功能的学习算法。&lt;br /&gt;
   下面讨论针对不同问题的提升树学习方法，其主要区别在于使用的损失函数不同。包括用平方误差损失函数的回归问题，用指数损失函数的分类问题，以及用一般损失函数的一般决策问题。&lt;br /&gt;
   回归问题的提升树算法：&lt;br /&gt;
   &lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT15.jpeg?raw=true&quot; alt=&quot;GBDT&quot; title=&quot;Title&quot; height=&quot;350px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
   对回归问题的提升树算法来说，只需简单低拟合当前模型残差。&lt;/p&gt;
&lt;h2 id=&quot;梯度提升gradient-boosting&quot;&gt;梯度提升（Gradient Boosting）&lt;/h2&gt;
&lt;p&gt;提升树利用加法模型与前向分布算法实现学习的优化过程。当损失函数是平方损失和指数损失函数时，每一步优化是很简单的。但对一般损失函数而言，往往每一步优化并不那么容易。针对这一问题，Freidman提出了梯度提升(gradient boosting)算法。这是利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前魔性的值：&lt;br /&gt;
   &lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT15.png?raw=true&quot; alt=&quot;GBDT&quot; title=&quot;Title&quot; height=&quot;100px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
   作为回归问题提升树算法中的残差的近似值，拟合一个回归树。&lt;br /&gt;
   梯度提升算法：&lt;br /&gt;
   &lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/GBDT16.png?raw=true&quot; alt=&quot;GBDT&quot; title=&quot;Title&quot; height=&quot;500px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
   算法第1步初始化，估计使损失函数极小化的常数值，它是只有一个根结点的树，即 x&amp;gt;c 和 x&amp;lt;c；
   第2 (a)步计算损失函数的负梯度在当前模型的值，将它作为残差的估计。对于平方损失函数，它就是通常所说的残差；对于一般损失函数，它就是残差的近似值。
   第2 (b)步估计回归树叶结点区域，以拟合残差的近似值。
   第2 (c)步利用线性搜索估计叶结点区域的值，使损失函数极小化。
   第2 (d)步更新回归树。
   第3步得到输出的最终模型。&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;
&lt;p&gt;1.李航 《统计学习方法》
2.https://blog.csdn.net/u011826404/article/details/70172971
3.http://www.imooc.com/article/258253&lt;/p&gt;
</description>
                <link>http://username.github.io/2020/02/15/gbdt</link>
                <guid>http://username.github.io/2020/02/15/gbdt</guid>
                <pubDate>Sat, 15 Feb 2020 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>树型模型之随机森林</title>
                <description>
&lt;h2 id=&quot;集成学习&quot;&gt;集成学习&lt;/h2&gt;
&lt;p&gt;讲随机森林之前，我们先了解一下什么叫集成学习。顾名思义，集成学习（ensemble learning）指的是将多个学习器进行有效地结合，组建一个“学习器委员会”，其中每个学习器担任委员会成员并行使投票表决权，使得委员会最后的决定更能够四方造福普度众生~…~，即其泛化性能要能优于其中任何一个学习器。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/随机森林1.jpeg?raw=true&quot; alt=&quot;随机森林&quot; title=&quot;Title&quot; height=&quot;300px&quot; width=&quot;650px&quot; /&gt;&lt;br /&gt;
在上图的集成模型中，若个体学习器都属于同一类别，例如都是决策树或都是神经网络，则称该集成为同质的（homogeneous）;若个体学习器包含多种类型的学习算法，例如既有决策树又有神经网络，则称该集成为异质的（heterogenous）。&lt;br /&gt;
&lt;strong&gt;同质集成&lt;/strong&gt;：个体学习器称为“基学习器”（base learner），对应的学习算法为“基学习算法”（base learning algorithm）。&lt;br /&gt;
&lt;strong&gt;异质集成&lt;/strong&gt;：个体学习器称为“组件学习器”（component learner）或直称为“个体学习器”。&lt;/p&gt;

&lt;p&gt;对个体学习器的要求： 要获得好的集成效果，个体学习器应该“好而不同”，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”，即学习器间要有差异。由于“准确性”和“多样性”本身就存在冲突，（根据交叉验证方式：“准确性”依托大量的样本，就会降低训练集的“多样性”，进而导致个体学习器“多样性”下降）所以如何产生并结合“好而不同”的个体学习器，恰恰是集成学习研究的核心内容。&lt;/p&gt;

&lt;p&gt;多个弱学习器集成就可以得到强学习器的推导：
以二分类问题为例，预测值y∈{-1,1}和真实函数f(x),假定基分类器的错误率为ϵ,则有：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/随机森林2.png?raw=true&quot; alt=&quot;随机森林&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
假设集成通过简单投票法结合TT个基分类器，若有超过半数的基分类器正确，则集成分类就正确：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/随机森林3.png?raw=true&quot; alt=&quot;随机森林&quot; title=&quot;Title&quot; height=&quot;100px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
假设基分类器的错误率相互独立，则可得：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/随机森林4.png?raw=true&quot; alt=&quot;随机森林&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
故，随着个体学习器数目TT的增大，集成的错误率将指数级下降，并最终趋于0。这就是为什么多个弱学习器能够组合出强学习器的原因所在。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;集成学习分类&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Boosting:个体学习器之间存在强依赖关系，必须串行生成的序列化方法。&lt;/li&gt;
  &lt;li&gt;Bagging：个体学习器之间存在弱依赖关系，可同时生成的并行化方法。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bagging&quot;&gt;Bagging&lt;/h2&gt;
&lt;p&gt;Bagging是一种并行式的集成学习方法，即基学习器的训练之间没有前后顺序可以同时进行，Bagging使用“有放回”采样的方式选取训练集，对于包含m个样本的训练集，进行m次有放回的随机采样操作，从而得到m个样本的采样集，这样训练集中有接近36.8%的样本没有被采到。按照相同的方式重复进行，我们就可以采集到T个包含m个样本的数据集，从而训练出T个基学习器，最终对这T个基学习器的输出进行结合。
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/随机森林5.png?raw=true&quot; alt=&quot;随机森林&quot; title=&quot;Title&quot; height=&quot;50px&quot; width=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Bagging算法的流程如下所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/随机森林6.jpeg?raw=true&quot; alt=&quot;随机森林&quot; title=&quot;Title&quot; height=&quot;300px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
可以看出Bagging主要通过样本的扰动来增加基学习器之间的多样性，因此Bagging的基学习器应为那些对训练集十分敏感的不稳定学习算法，例如：神经网络与决策树等。从偏差-方差分解来看，Bagging算法主要关注于降低方差，即通过多次重复训练提高稳定性。不同于AdaBoost的是，Bagging可以十分简单地移植到多分类、回归等问题。总的说起来则是：AdaBoost关注于降低偏差，而Bagging关注于降低方差。&lt;/p&gt;

&lt;h2 id=&quot;随机森林&quot;&gt;随机森林&lt;/h2&gt;
&lt;p&gt;随机森林（Random Forest）是Bagging的一个拓展体，它的基学习器固定为决策树，多棵树也就组成了森林，而“随机”则在于选择划分属性的随机，随机森林在训练基学习器时，也采用有放回采样的方式添加 &lt;strong&gt;样本扰动&lt;/strong&gt;，同时它还引入了一种 &lt;strong&gt;属性扰动&lt;/strong&gt;，即在基决策树的训练过程中，在选择划分属性时，RF先从候选属性集中随机挑选出一个包含K个属性的子集，再从这个子集中选择最优划分属性，一般推荐K=log2（d）&lt;/p&gt;

&lt;p&gt;这样随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，从而进一步提升了基学习器之间的差异度。相比决策树的Bagging集成，随机森林的起始性能较差（由于属性扰动，基决策树的准确度有所下降），但随着基学习器数目的增多，随机森林往往会收敛到更低的泛化误差。同时不同于Bagging中决策树从所有属性集中选择最优划分属性，随机森林只在属性集的一个子集中选择划分属性，因此训练效率更高。
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/随机森林7.jpeg?raw=true&quot; alt=&quot;随机森林&quot; title=&quot;Title&quot; height=&quot;300px&quot; width=&quot;600px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;结合策略&quot;&gt;结合策略&lt;/h2&gt;
&lt;p&gt;结合策略指的是在训练好基学习器后，如何将这些基学习器的输出结合起来产生集成模型的最终输出，下面将介绍一些常用的结合策略：&lt;/p&gt;
&lt;h3 id=&quot;平均法回归问题&quot;&gt;平均法（回归问题）&lt;/h3&gt;
&lt;p&gt;对数型输出hi(x) = R, 最常见的结合策略是使用平均法（averaging）.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;简单平均法（simple averaging）
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/随机森林8.png?raw=true&quot; alt=&quot;随机森林&quot; title=&quot;Title&quot; height=&quot;100px&quot; width=&quot;400px&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;加权平均法
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/随机森林9.png?raw=true&quot; alt=&quot;随机森林&quot; title=&quot;Title&quot; height=&quot;100px&quot; width=&quot;400px&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;易知简单平均法是加权平均法的一种特例，加权平均法可以认为是集成学习研究的基本出发点。由于各个基学习器的权值在训练中得出，一般而言，在个体学习器性能相差较大时宜使用加权平均法，在个体学习器性能相差较小时宜使用简单平均法。&lt;/p&gt;

&lt;h3 id=&quot;投票法分类问题&quot;&gt;投票法（分类问题）&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;绝对多数投票法
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/随机森林10.png?raw=true&quot; alt=&quot;随机森林&quot; title=&quot;Title&quot; height=&quot;100px&quot; width=&quot;400px&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;绝对多数投票法
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/随机森林11.png?raw=true&quot; alt=&quot;随机森林&quot; title=&quot;Title&quot; height=&quot;100px&quot; width=&quot;400px&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;加权投票法
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/随机森林12.png?raw=true&quot; alt=&quot;随机森林&quot; title=&quot;Title&quot; height=&quot;100px&quot; width=&quot;500px&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;绝对多数投票法（majority voting）提供了拒绝选项，这在可靠性要求很高的学习任务中是一个很好的机制。同时，对于分类任务，各个基学习器的输出值有两种类型，分别为类标记和类概率。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/随机森林13.png?raw=true&quot; alt=&quot;随机森林&quot; title=&quot;Title&quot; height=&quot;200px&quot; width=&quot;600px&quot; /&gt; &lt;br /&gt;
一些在产生类别标记的同时也生成置信度的学习器，置信度可转化为类概率使用，一般基于类概率进行结合往往比基于类标记进行结合的效果更好，需要注意的是对于异质集成，其类概率不能直接进行比较，此时需要将类概率转化为类标记输出，然后再投票。&lt;/p&gt;

&lt;h3 id=&quot;学习法&quot;&gt;学习法&lt;/h3&gt;
&lt;p&gt;学习法是一种更高级的结合策略，即学习出一种“投票”的学习器，Stacking是学习法的典型代表。Stacking的基本思想是：首先训练出T个基学习器，对于一个样本它们会产生T个输出，将这T个基学习器的输出与该样本的真实标记作为新的样本，m个样本就会产生一个m*T的样本集，来训练一个新的“投票”学习器。投票学习器的输入属性与学习算法对Stacking集成的泛化性能有很大的影响，书中已经提到：投票学习器采用类概率作为输入属性，选用多响应线性回归（MLR）一般会产生较好的效果。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/随机森林14.png?raw=true&quot; alt=&quot;随机森林&quot; title=&quot;Title&quot; height=&quot;400px&quot; width=&quot;600px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;周志华 《机器学习》&lt;/li&gt;
  &lt;li&gt;https://blog.csdn.net/u011826404/article/details/70172971&lt;/li&gt;
&lt;/ol&gt;
</description>
                <link>http://username.github.io/2020/02/14/%E6%A0%91%E5%9E%8B%E6%A8%A1%E5%9E%8B%E4%B9%8B%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97</link>
                <guid>http://username.github.io/2020/02/14/树型模型之随机森林</guid>
                <pubDate>Fri, 14 Feb 2020 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>树型模型</title>
                <description>
&lt;h2 id=&quot;树型模型&quot;&gt;树型模型&lt;/h2&gt;

&lt;p&gt;树型模型有助于探索数据集，并可视化预测的决策规则。当你听到关于树型模型的东西时，你可以将其想成是决策树或分支操作序列。树型模型高度精确、稳定且更易于解释。与线性模型相反，它们可以映射非线性关系以求解问题。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型.jpeg?raw=true&quot; alt=&quot;树型模型&quot; title=&quot;Title&quot; height=&quot;200px&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;决策树&quot;&gt;决策树&lt;/h2&gt;
&lt;p&gt;决策树(decision tress)是一类常见的机器学习方法。以二分类任务为例，我们希望从给定训练数据集学得一个模型用以对新示例进行分类，这个把样本分类的任务，可看做对“当前样本是正常分类吗？”这个问题的“决策”或“判定”过程。&lt;br /&gt;
顾名思义，决策树是基于树结构来进行决策的，这恰是人类面临决策问题时一种很自然的处理机制。例如下面判断西瓜是“好瓜”还是“坏瓜”的过程：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/西瓜决策树.jpg?raw=true&quot; alt=&quot;西瓜决策树&quot; title=&quot;Title&quot; height=&quot;300px&quot; width=&quot;300px&quot; /&gt;&lt;br /&gt;
在上图的决策树中，决策过程的每一次判定都是对某一属性的“测试”，决策最终结论则对应最终的判定结果。一般一颗决策树包含：一个根节点、若干个内部节点和若干个叶子节点，易知：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;每个非叶节点表示一个特征属性测试。&lt;/li&gt;
  &lt;li&gt;每个分支代表这个特征属性在某个值域上的输出。&lt;/li&gt;
  &lt;li&gt;每个叶子节点存放一个类别。&lt;/li&gt;
  &lt;li&gt;每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;决策树的构造过程&quot;&gt;决策树的构造过程&lt;/h3&gt;
&lt;p&gt;决策树的构造是一个递归的过程，有三种情形会导致递归返回：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(1) 当前结点包含的样本全属于同一类别，这时直接将该节点标记为叶节点，并设为相应的类别；&lt;/li&gt;
  &lt;li&gt;(2) 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分，这时将该节点标记为叶节点，并将其类别设为该节点所含样本最多的类别；&lt;/li&gt;
  &lt;li&gt;(3) 当前结点包含的样本集合为空，不能划分，这时也将该节点标记为叶节点，并将其类别设为父节点中所含样本最多的类别。算法的基本流程如下图所示：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型1.png?raw=true&quot; alt=&quot;1&quot; title=&quot;Title&quot; height=&quot;550px&quot; width=&quot;650px&quot; /&gt;&lt;br /&gt;
可以看出：决策树学习的关键在于如何选择划分属性，不同的划分属性得出不同的分支结构，从而影响整颗决策树的性能。属性划分的目标是让各个划分出来的子节点尽可能地“纯”，即属于同一类别。因此下面便是介绍量化纯度的具体方法，决策树最常用的算法有三种：ID3，C4.5和CART。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;id3算法&quot;&gt;ID3算法&lt;/h3&gt;
&lt;p&gt;ID3算法使用信息增益为准则来选择划分属性，“信息熵”(information entropy)是度量样本结合纯度的常用指标，假定当前样本集合D中第k类样本所占比例为pk，则样本集合D的信息熵定义为：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型2.png?raw=true&quot; alt=&quot;2&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
“信息熵“值越大表示越混乱，只有一个类别时，信息熵为0。
假定通过属性划分样本集D，产生了V个分支节点，v表示其中第v个分支节点，易知：分支节点包含的样本数越多，表示该分支节点的影响力越大。故可以计算出划分后相比原始数据集D获得的“信息增益”（information gain）。&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型3.png?raw=true&quot; alt=&quot;3&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
信息增益越大，表示使用该属性划分样本集D的效果越好，因此ID3算法在递归过程中，每次选择最大信息增益的属性作为当前的划分属性。&lt;/p&gt;

&lt;h3 id=&quot;c45算法&quot;&gt;C4.5算法&lt;/h3&gt;
&lt;p&gt;ID3算法存在一个问题，就是偏向于取值数目较多的属性，例如：如果存在一个唯一标识，这样样本集D将会被划分为|D|个分支，每个分支只有一个样本，这样划分后的信息熵为零，十分纯净，但是对分类毫无用处。因此C4.5算法使用了“增益率”（gain ratio）来选择划分属性，来避免这个问题带来的困扰。首先使用ID3算法计算出信息增益高于平均水平的候选属性，接着C4.5计算这些候选属性的增益率，增益率定义为：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型4.png?raw=true&quot; alt=&quot;4&quot; title=&quot;Title&quot; height=&quot;120px&quot; width=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;cart算法&quot;&gt;CART算法&lt;/h3&gt;
&lt;p&gt;CART决策树使用“基尼指数”（Gini index）来选择划分属性，基尼指数反映的是从样本集D中随机抽取两个样本，其类别标记不一致的概率，因此Gini(D)越小越好（表示集合越纯），基尼指数定义如下：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型5.png?raw=true&quot; alt=&quot;5&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
进而，使用属性α划分后的基尼指数为：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型6.png?raw=true&quot; alt=&quot;6&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;剪枝处理&quot;&gt;剪枝处理&lt;/h2&gt;
&lt;p&gt;从决策树的构造流程中我们可以直观地看出：不管怎么样的训练集，决策树总是能很好地将各个类别分离开来，这时就会遇到之前提到过的问题：过拟合（overfitting），即太依赖于训练样本。剪枝（pruning）则是决策树算法对付过拟合的主要手段，剪枝的策略有两种如下：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;预剪枝（prepruning）：在构造的过程中先评估，再考虑是否分支。&lt;/li&gt;
  &lt;li&gt;后剪枝（post-pruning）：在构造好一颗完整的决策树后，自底向上，评估分支的必要性。
评估指的是性能度量，即决策树的泛化性能。之前提到：可以使用测试集作为学习器泛化性能的近似，因此可以将数据集划分为训练集和测试集。预剪枝表示在构造数的过程中，对一个节点考虑是否分支时，首先计算决策树不分支时在测试集上的性能，再计算分支之后的性能，若分支对性能没有提升，则选择不分支（即剪枝）。后剪枝则表示在构造好一颗完整的决策树后，从最下面的节点开始，考虑该节点分支对模型的性能是否有提升，若无则剪枝，即将该节点标记为叶子节点，类别标记为其包含样本最多的类别。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;未剪枝决策树：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型7.jpeg?raw=true&quot; alt=&quot;7&quot; title=&quot;Title&quot; height=&quot;400px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
预剪枝决策树：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型8.png?raw=true&quot; alt=&quot;8&quot; title=&quot;Title&quot; height=&quot;350px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
后剪枝决策树：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型9.jpg?raw=true&quot; alt=&quot;9&quot; title=&quot;Title&quot; height=&quot;400px&quot; width=&quot;600px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;预剪枝与后剪枝优缺点比较
（1）时间开销　　　&lt;/p&gt;

&lt;p&gt;　　•   预剪枝：训练时间开销降低，测试时间开销降低&lt;/p&gt;

&lt;p&gt;　　•  后剪枝：训练时间开销增加，测试时间开销降低&lt;/p&gt;

&lt;p&gt;（2）过/欠拟合风险&lt;/p&gt;

&lt;p&gt;　　•  预剪枝：过拟合风险降低，欠拟合风险增加&lt;/p&gt;

&lt;p&gt;　　•  后剪枝：过拟合风险降低，欠拟合风险基本不变&lt;/p&gt;

&lt;p&gt;（3）泛化性能：后剪枝通常优于预剪枝&lt;/p&gt;

&lt;h2 id=&quot;连续值和缺失值处理&quot;&gt;连续值和缺失值处理&lt;/h2&gt;
&lt;p&gt;对于连续值的属性，若每个取值作为一个分支则显得不可行，因此需要进行离散化处理，常用的方法为二分法，基本思想为：给定样本集D与连续属性α，二分法试图找到一个划分点t将样本集D在属性α上分为≤t与＞t。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;首先将α的所有取值按升序排列，所有相邻的属性取值的均值作为候选划分点（n-1个，n为α所有的取值数目）。&lt;/li&gt;
  &lt;li&gt;计算每一个划分点划分集合D（即划分为两个分支）后的信息增益。&lt;/li&gt;
  &lt;li&gt;选择最大信息增益的划分点作为最优划分点。
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型10.png?raw=true&quot; alt=&quot;10&quot; title=&quot;Title&quot; height=&quot;100px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
现实中常会遇到不完整的样本，即某些属性值缺失。有时若简单采取剔除，则会造成大量的信息浪费，因此在属性值缺失的情况下需要解决两个问题：（1）如何选择划分属性。（2）给定划分属性，若某样本在该属性上缺失值，如何划分到具体的分支上。假定为样本集中的每一个样本都赋予一个权重，根节点中的权重初始化为1，则定义：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型11.png?raw=true&quot; alt=&quot;11&quot; title=&quot;Title&quot; height=&quot;200px&quot; width=&quot;500px&quot; /&gt;&lt;br /&gt;
对于（1）：通过在样本集D中选取在属性α上没有缺失值的样本子集，计算在该样本子集上的信息增益，最终的信息增益等于该样本子集划分后信息增益乘以样本子集占样本集的比重。即：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型12.png?raw=true&quot; alt=&quot;12&quot; title=&quot;Title&quot; height=&quot;100px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
对于（2）：若该样本子集在属性α上的值缺失，则将该样本以不同的权重（即每个分支所含样本比例）划入到所有分支节点中。该样本在分支节点中的权重变为：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/树型模型13.png?raw=true&quot; alt=&quot;13&quot; title=&quot;Title&quot; height=&quot;100px&quot; width=&quot;400px&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;
&lt;p&gt;1.周志华《机器学习》&lt;br /&gt;
2.https://blog.csdn.net/u011826404/article/details/53606485&lt;/p&gt;

</description>
                <link>http://username.github.io/2020/02/13/%E6%A0%91%E5%9E%8B%E6%A8%A1%E5%9E%8B</link>
                <guid>http://username.github.io/2020/02/13/树型模型</guid>
                <pubDate>Thu, 13 Feb 2020 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>线性模型之支持向量机</title>
                <description>
&lt;h2 id=&quot;间隔与支持向量&quot;&gt;间隔与支持向量&lt;/h2&gt;
&lt;p&gt;给定训练样本集D = {(x1,y1),(x2,y2),…,(xm,ym)}, yi∈{-1, +1}，分类学习最基本的想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开。但能将训练样本分开的超平面有很多，如图所示，我们应该努力去找到哪一个呢？  &lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm1.jpg?raw=true&quot; alt=&quot;1&quot; title=&quot;Title&quot; height=&quot;300px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
直观上看，应该去找位于两类训练样本”正中间“的划分超平面，即图6.1中红色的那个，因为该划分超平面对训练样本局部扰动的”容忍“性最好。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;函数间隔&lt;/strong&gt; （||&lt;strong&gt;wx&lt;/strong&gt;+b||） &lt;br /&gt;
在超平面&lt;strong&gt;wx&lt;/strong&gt; + b = 0确定的情况下，|&lt;strong&gt;wx&lt;/strong&gt;+b|能够代表点 &lt;strong&gt;x&lt;/strong&gt;距离超平面的远近，易知：当&lt;strong&gt;wx&lt;/strong&gt;+b&amp;gt;0时，表示x在超平面的一侧（正类，类标为1），而当&lt;strong&gt;wx&lt;/strong&gt;+b&amp;lt;0时，则表示x在超平面的另外一侧（负类，类别为-1），因此（&lt;strong&gt;wx&lt;/strong&gt;+b)y 的正负性恰能表示数据点x是否被分类正确。于是便引出了函数间隔的定义（functional margin）:&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm14.png?raw=true&quot; alt=&quot;14&quot; title=&quot;Title&quot; height=&quot;50px&quot; width=&quot;200px&quot; /&gt; &lt;br /&gt;
定义超平面(w,b)关于训练集T的函数间隔为超平面(w,b)关于T中所有样本点(xi,yi)的函数间隔之最小值，即&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm15.png?raw=true&quot; alt=&quot;15&quot; title=&quot;Title&quot; height=&quot;50px&quot; width=&quot;150px&quot; /&gt;   &lt;br /&gt;
函数间隔可以表示分类预测的正确性及确信度，但是选择分离超平面时，只有函数间隔还不够，因为只要成比例改变w和b，函数间隔也会成比例改变。这就引出了几何间隔。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;几何间隔&lt;/strong&gt; （点到超平面的距离）&lt;br /&gt;
在样本空间中，划分超平面可通过如下线性方程来描述：&lt;br /&gt;
                            wTx + b = 0&lt;br /&gt;
其中w = (w1;w2;…;wd)为法向量，决定了超平面的方向；b为位移项，决定了超平面与原点之间的距离。显然，划分超平面可被法向量 &lt;strong&gt;&lt;em&gt;w&lt;/em&gt;&lt;/strong&gt; 和位移b确定，下面我们将其记为（w,b)，样本空间中任意点x到超平面(w,b)的距离可写为&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm2.png?raw=true&quot; alt=&quot;2&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;150px&quot; /&gt;&lt;br /&gt;
假设超平面(w,b)能将训练样本正确分类，即对于(xi,yi)∈ D，若yi = +1，则有wTxi + b &amp;gt; 0；若yi = -1，则有wTxi + b &amp;lt; 0. 令  （缩放变换）&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm3.png?raw=true&quot; alt=&quot;3&quot; title=&quot;Title&quot; height=&quot;100px&quot; width=&quot;250px&quot; /&gt;&lt;br /&gt;
如图2所示，距离超平面最近的这几个训练样本点使上式的等号成立，它们被称为”&lt;strong&gt;支持向量（support vector）&lt;/strong&gt;“，两个异类支持向量到超平面的距离之和为&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm4.png?raw=true&quot; alt=&quot;4&quot; title=&quot;Title&quot; height=&quot;50px&quot; width=&quot;100px&quot; /&gt;&lt;br /&gt;
它被称为”间隔（margin）“&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm6.jpg?raw=true&quot; alt=&quot;6&quot; title=&quot;Title&quot; height=&quot;300px&quot; width=&quot;450px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;想要找到具有”最大间隔“的划分超平面，也就是要找到能满足上式中约束的参数w和b，使得γ最大，即&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm5.png?raw=true&quot; alt=&quot;5&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;250px&quot; /&gt; &lt;br /&gt;
显然，为了最大化间隔，仅需最大化1/||w||，这等价于最小化||w||的平方，于是，上式可重写为：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm7.png?raw=true&quot; alt=&quot;7&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;300px&quot; /&gt;&lt;br /&gt;
这就是 &lt;strong&gt;支持向量机(support vector machine)&lt;/strong&gt;的基本型。&lt;/p&gt;

&lt;h2 id=&quot;对偶问题&quot;&gt;对偶问题&lt;/h2&gt;
&lt;p&gt;我们希望求解式(6.6)来得到大间隔划分超平面所对应的模型&lt;br /&gt;
                        f(x)=&lt;strong&gt;&lt;em&gt;w&lt;/em&gt;&lt;/strong&gt;T &lt;strong&gt;&lt;em&gt;x&lt;/em&gt;&lt;/strong&gt; + b
其中 &lt;strong&gt;&lt;em&gt;w&lt;/em&gt;&lt;/strong&gt; 和 &lt;strong&gt;&lt;em&gt;b&lt;/em&gt;&lt;/strong&gt; 是模型参数，注意到式（6.6）本身是一个凸二次规划问题，能直接用现成的优化计算包(QP优化包)求解，但我们可以有更高效的方法-将原问题转换为它的对偶问题。原因如下：&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;1. 是因为使用对偶问题更容易求解&lt;/em&gt;&lt;/strong&gt;  &lt;br /&gt;
&lt;strong&gt;&lt;em&gt;2. 是因为通过对偶问题求解出现了向量内积的形式，从而能更加自然地引出核函数。&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;对式(6.6)使用 &lt;strong&gt;拉格朗日乘子法&lt;/strong&gt; 可得到其 ”对偶问题“。具体来说，对式(6.6)的每条约束添加拉格朗日乘子αi &amp;gt;=0，则该问题的拉格朗日函数可写为&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm8.png?raw=true&quot; alt=&quot;8&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;400px&quot; /&gt; &lt;br /&gt;
其中，α = (α1;α2;…;αm). 上式很容易验证：当其中有一个约束条件不满足时，L的最大值为 ∞（只需令其对应的α为 ∞即可）；当所有约束条件都满足时，L的最大值为1/2||w||^2（此时令所有的α为0），因此实际上原问题等价于：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm16.png?raw=true&quot; alt=&quot;16&quot; title=&quot;Title&quot; height=&quot;50px&quot; width=&quot;300px&quot; /&gt;&lt;br /&gt;
由于这个的求解问题不好做，因此一般我们将最小和最大的位置交换一下（需满足KKT条件） ，变成原问题的对偶问题：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm17.png?raw=true&quot; alt=&quot;17&quot; title=&quot;Title&quot; height=&quot;50px&quot; width=&quot;300px&quot; /&gt; &lt;br /&gt;
令L(w,b,a)对w和b的偏导为零可得&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm9.png?raw=true&quot; alt=&quot;9&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;400px&quot; /&gt; &lt;br /&gt;
将式(6.9)代入(6.8)，即可将L(w,b,α)中的w和b消去，再考虑式(6.10)的约束，就得到式(6.6)的对偶问题：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm11.png?raw=true&quot; alt=&quot;11&quot; title=&quot;Title&quot; height=&quot;150px&quot; width=&quot;400px&quot; /&gt; &lt;br /&gt;
解出α后，求出w与b即可得到模型:&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm12.png?raw=true&quot; alt=&quot;12&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
从对偶问题(6.11)解出的αi 是式(6.8)中拉格朗日乘子，它恰对应着训练样本(xi,yi)。
上述过程需要满足KKT(Karush-Kuhn-Tucker)条件，即要求&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm13.png?raw=true&quot; alt=&quot;13&quot; title=&quot;Title&quot; height=&quot;100px&quot; width=&quot;300px&quot; /&gt; &lt;br /&gt;
于是，对任意训练样本(xi,yi)，总有αi = 0 或 yif(xi) = 1. 若 αi = 0，则该样本将不会在式(6.12)的求和中出现，也就不会对f(x)有任何影响；若αi &amp;gt; 0，则必有yif(xi) = 1，所对应的样本位于最大间隔边界上，是一个支持向量。因此，支持向量机有一个重要性质：
&lt;strong&gt;训练完成后，大部分的训练样本都不需要保留，最终模型仅与支持向量有关。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如何求解式(6.11)呢？SMO(Sequential Minimal Optimization)是其中一个著名的算法。
SMO的基本思路是先固定αi之外的所有参数，然后求αi上的极值。由于存在约束&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm18.png?raw=true&quot; alt=&quot;18&quot; title=&quot;Title&quot; height=&quot;50px&quot; width=&quot;50px&quot; /&gt;，若固定αi之外的其他变量，则αi可由其他变量导出。于是，SMO每次选择两个变量αi和αj，并固定其他参数。这样，在参数初始化后，SMO不断执行如下两个步骤直至收敛：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;选取一对需要更新的变量αi和α;&lt;/li&gt;
  &lt;li&gt;固定αi和αj以外的参数，求解式(6.11)获得更新后的αi和αj.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;核函数&quot;&gt;核函数&lt;/h2&gt;
&lt;p&gt;由于上述的超平面只能解决线性可分的问题，对于线性不可分的问题，例如：异或问题&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm19.jpeg?raw=true&quot; alt=&quot;19&quot; title=&quot;Title&quot; height=&quot;250px&quot; width=&quot;500px&quot; /&gt;&lt;br /&gt;
我们需要使用核函数将其进行推广。一般地，解决线性不可分问题时，常常采用映射的方式，将低维原始空间映射到高维特征空间，使得数据集在高维空间中变得线性可分，从而再使用线性学习器分类。如果原始空间为有限维，即属性数有限，那么总是存在一个高维特征空间使得样本线性可分。若∅代表一个映射，则在特征空间中的划分函数变为：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm20.png?raw=true&quot; alt=&quot;20&quot; title=&quot;Title&quot; height=&quot;50px&quot; width=&quot;150px&quot; /&gt;&lt;br /&gt;
按照同样的方法，先写出新目标函数的拉格朗日函数，接着写出其对偶问题，求L关于w和b的极大，最后运用SOM求解α。可以得出：&lt;/p&gt;

&lt;p&gt;(1) 原对偶问题变为：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm22.png?raw=true&quot; alt=&quot;22&quot; title=&quot;Title&quot; height=&quot;150px&quot; width=&quot;400px&quot; /&gt; &lt;br /&gt;
(2) 分类函数：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm21.png?raw=true&quot; alt=&quot;21&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;400px&quot; /&gt; &lt;br /&gt;
因此，在线性不可分问题中，核函数的选择成了支持向量机的最大变数，若选择了不合适的核函数，则意味着将样本映射到了一个不合适的特征空间，则极可能导致性能不佳。同时，核函数需要满足以下这个必要条件：&lt;br /&gt;
定理6.1（核函数）令X为输入控件，k(.,.)是定义在X*X上的对称函数，则k是核函数当且仅当对于任意数据D = {x1,x2,…,xm}，”核矩阵“K总是半正定的：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm23.png?raw=true&quot; alt=&quot;23&quot; title=&quot;Title&quot; height=&quot;200px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
由于核函数的构造十分困难，通常我们都是从一些常用的核函数中选择，下面列出了几种常用的核函数：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm24.jpg?raw=true&quot; alt=&quot;24&quot; title=&quot;Title&quot; height=&quot;200px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
其中线性核为处理线性可分的情形，这使得它们在形式上统一起来。&lt;/p&gt;

&lt;h2 id=&quot;软间隔支持向量机&quot;&gt;软间隔支持向量机&lt;/h2&gt;
&lt;p&gt;前面的讨论中，我们主要解决了两个问题：&lt;strong&gt;当数据线性可分时，直接使用最大间隔的超平面划分；当数据线性不可分时，则通过核函数将数据映射到高维特征空间，使之线性可分&lt;/strong&gt;。然而在现实问题中，对于某些情形还是很难处理，例如数据中有噪声的情形，噪声数据（outlier）本身就偏离了正常位置，但是在前面的SVM模型中，我们要求所有的样本数据都必须满足约束（6.3），这称为”&lt;strong&gt;硬间隔&lt;/strong&gt;“。如果不要这些噪声数据还好，当加入这些outlier后导致划分超平面被挤歪了，如下图所示，对支持向量机的泛化性能造成很大的影响。&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm25.jpg?raw=true&quot; alt=&quot;25&quot; title=&quot;Title&quot; height=&quot;300px&quot; width=&quot;500px&quot; /&gt;&lt;br /&gt;
为了解决这一问题，我们需要允许某一些数据点不满足约束，即可以在一定程度上偏移超平面，同时使得不满足约束的数据点尽可能少，这便引出了“&lt;strong&gt;软间隔&lt;/strong&gt;”支持向量机的概念&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;允许某些数据点不满足约束y(w’x+b)≥1；&lt;/li&gt;
  &lt;li&gt;同时又使得不满足约束的样本尽可能少。
这样优化目标变为：
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm26.png?raw=true&quot; alt=&quot;26&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
其中C &amp;gt; 0是一个常数，l0/1是”0/1损失函数“&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm27.png?raw=true&quot; alt=&quot;27&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;200px&quot; /&gt;&lt;br /&gt;
如同阶跃函数，0/1损失函数虽然表示效果最好，但是数学性质不佳。因此常用其它函数作为“替代损失函数”。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;三种常用的替代损失函数如下：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm28.png?raw=true&quot; alt=&quot;28&quot; title=&quot;Title&quot; height=&quot;150px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
支持向量机中的损失函数为hinge损失，引入“&lt;strong&gt;松弛变量&lt;/strong&gt;”，目标函数与约束条件可以写为：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm29.png?raw=true&quot; alt=&quot;29&quot; title=&quot;Title&quot; height=&quot;100px&quot; width=&quot;450px&quot; /&gt;&lt;br /&gt;
其中C为一个参数，控制着目标函数与新引入正则项之间的权重，这样显然每个样本数据都有一个对应的松弛变量，用以表示该样本不满足约束的程度，将新的目标函数转化为拉格朗日函数得到：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm30.png?raw=true&quot; alt=&quot;30&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;600px&quot; /&gt;&lt;br /&gt;
按照与之前相同的方法，先让L求关于w，b以及松弛变量的极小，再使用SMO求出α，有：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm31.png?raw=true&quot; alt=&quot;31&quot; title=&quot;Title&quot; height=&quot;150px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
将w代入L化简，便得到其对偶问题：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/svm31.png?raw=true&quot; alt=&quot;32&quot; title=&quot;Title&quot; height=&quot;150px&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
将“软间隔”下产生的对偶问题与原对偶问题对比可以发现：新的对偶问题只是约束条件中的α多出了一个上限C，其它的完全相同，因此在引入核函数处理线性不可分问题时，便能使用与“硬间隔”支持向量机完全相同的方法。&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;周志华《机器学习》&lt;/li&gt;
  &lt;li&gt;https://blog.csdn.net/u011826404/article/details/54647611&lt;/li&gt;
  &lt;li&gt;李航 《统计学方法》&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;附录&quot;&gt;附录&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;朗格朗日乘子法&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;闭式解&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;对偶问题&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
                <link>http://username.github.io/2020/02/08/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA</link>
                <guid>http://username.github.io/2020/02/08/线性模型之支持向量机</guid>
                <pubDate>Sat, 08 Feb 2020 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>线性模型之逻辑回归</title>
                <description>
&lt;h2 id=&quot;定位&quot;&gt;定位&lt;/h2&gt;
&lt;p&gt;逻辑回归又名sigmoid回归、对数几率回归，是一种用于解决二分类问题的机器学习方法，用于估计某种事物的可能性。&lt;/p&gt;

&lt;h2 id=&quot;由来&quot;&gt;由来&lt;/h2&gt;
&lt;p&gt;如何使用线性模型做分类任务呢？答案：只需找一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来。
逻辑回归与线性回归都是一种广义线性模型。逻辑回归假设因变量y服从伯努利分布，而线性回归假设因变量y服从高斯分布。逻辑回归是以线性回归为理论支持的，去掉sigmoid映射函数的话，逻辑回归就是一个线性回归，但是逻辑回归通过引入sigmoid函数引入了非线性因素，因此可以轻松处理0/1分类问题。&lt;/p&gt;

&lt;h2 id=&quot;目标函数&quot;&gt;目标函数&lt;/h2&gt;
&lt;p&gt;考虑二分类任务，其输出标记y ∈ {0,1}，而线性回归模型产生的预测值z = wTx + b 是实值，于是，我们需将实值z转换为0/1值。最理想的是“单位阶跃函数(unit-step function)”&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归4.png?raw=true&quot; alt=&quot;4&quot; title=&quot;Title&quot; height=&quot;100px&quot; width=&quot;200px&quot; /&gt;&lt;br /&gt;
即若预测值z大于0就判断为正例，小于零就判断为反例，预测值为临界值零则可任意判别，如下图所示：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归5.jpg?raw=true&quot; alt=&quot;5&quot; title=&quot;Title&quot; height=&quot;300px&quot; width=&quot;550px&quot; /&gt;&lt;br /&gt;
但是阶跃函数不连续，于是我们希望找到能在一定程度上近似单位阶跃函数的“替代函数”，并希望它单调可微。对数几率函数(logistic function)正是这样一个常用的替代函数：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归6.png?raw=true&quot; alt=&quot;6&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;150px&quot; /&gt;&lt;br /&gt;
对数几率函数是一种“Sigmoid 函数（即形似S的函数）”，它将z值转化为接近0或1的y值，并且其输出值在z = 0附近变化很陡，将z值带入上式得：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归7.png?raw=true&quot; alt=&quot;7&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;200px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上式可变化为：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归8.png?raw=true&quot; alt=&quot;8&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;250px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;若将y视为样本x作为正例的可能性，则1-y是其反例可能性，两者的比值  &lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归9.png?raw=true&quot; alt=&quot;9&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;80px&quot; /&gt; &lt;br /&gt;
称为“几率”，反映了x作为正例的相对可能性。对几率取对数则得到“对数几率（logit）”&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归10.png?raw=true&quot; alt=&quot;10&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;80px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;所以对数几率函数实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率，因此，其对应的模型称为“对数几率回归（logistic regression）”&lt;/p&gt;

&lt;h2 id=&quot;损失函数&quot;&gt;损失函数&lt;/h2&gt;
&lt;p&gt;若将hθ(x)视为类后验概率估计p(y=1|x;θ)，则有：&lt;br /&gt;
p(y=1|x;θ) = hθ(x), y=1的概率&lt;br /&gt;
p(y=0|x;θ) = 1 - hθ(x), y=0的概率&lt;br /&gt;
所以有：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归12.png?raw=true&quot; alt=&quot;12&quot; title=&quot;Title&quot; height=&quot;40px&quot; width=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上面为样本的条件概率之积，即交叉熵，交叉熵是用来衡量两个概率分布之间的差异。交叉熵越大，两个分布之间的差异越大，越对实验结果感到意外，反之，交叉熵越小，两个分布越相似，越符合预期。&lt;/p&gt;

&lt;p&gt;于是，我们可通过“极大似然法(maximum likelihood method)”来估计参数θ，损失函数原始形式如下：  &lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归2.png?raw=true&quot; alt=&quot;2&quot; title=&quot;Title&quot; height=&quot;50px&quot; width=&quot;400px&quot; /&gt; &lt;br /&gt;
即令&lt;strong&gt;每个样本属于其真实标记的概率越大越好&lt;/strong&gt;。L表示所有训练样本的条件概率之积。&lt;/p&gt;

&lt;p&gt;转化为对数形式（可以将连乘变成累加），即为对数损失函数（似然函数）：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归3.png?raw=true&quot; alt=&quot;3&quot; title=&quot;Title&quot; height=&quot;50px&quot; width=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;令&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归13.png?raw=true&quot; alt=&quot;13&quot; title=&quot;Title&quot; height=&quot;50px&quot; width=&quot;60px&quot; /&gt; ，将最大值优化问题转换成最小值优化问题：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归14.png?raw=true&quot; alt=&quot;14&quot; title=&quot;Title&quot; height=&quot;60px&quot; width=&quot;150px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;因此逻辑回归的损失函数为：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归23.png?raw=true&quot; alt=&quot;23&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对数损失函数是关于θ的高阶可导连续凸函数，根据凸优化理论，经典的数值优化算法如梯度下降法（gradient descent method）、牛顿法（Newton method）等都可求得其最优解。&lt;/p&gt;

&lt;h2 id=&quot;梯度下降&quot;&gt;梯度下降&lt;/h2&gt;
&lt;p&gt;为了使cost function最小，也就是使似然函数最大，我们可以使用linear regression提到的Gradient Descent.&lt;br /&gt;
(1)&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归17.png?raw=true&quot; alt=&quot;17&quot; title=&quot;Title&quot; height=&quot;30px&quot; width=&quot;100px&quot; /&gt;&lt;br /&gt;
(2)&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归19.png?raw=true&quot; alt=&quot;19&quot; title=&quot;Title&quot; height=&quot;250px&quot; width=&quot;350px&quot; /&gt;&lt;br /&gt;
其中：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归20.png?raw=true&quot; alt=&quot;20&quot; title=&quot;Title&quot; height=&quot;80px&quot; width=&quot;250px&quot; /&gt; &lt;br /&gt;
上式推导如下：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归24.png?raw=true&quot; alt=&quot;24&quot; title=&quot;Title&quot; height=&quot;150px&quot; width=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;化简的梯度：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归21.png?raw=true&quot; alt=&quot;21&quot; title=&quot;Title&quot; height=&quot;60px&quot; width=&quot;350px&quot; /&gt;  &lt;br /&gt;
结合等式（1）（2）得到参数θ的更新策略如下：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归22.png?raw=true&quot; alt=&quot;22&quot; title=&quot;Title&quot; height=&quot;60px&quot; width=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考：&lt;/h2&gt;
&lt;p&gt;1.https://blog.csdn.net/Zfq740695564/article/details/81783420&lt;br /&gt;
2.周志华《机器学习》&lt;/p&gt;

</description>
                <link>http://username.github.io/2020/02/04/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92</link>
                <guid>http://username.github.io/2020/02/04/线性模型之逻辑回归</guid>
                <pubDate>Tue, 04 Feb 2020 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>线性模型之线性回归</title>
                <description>
&lt;p&gt;线性回归（linear regression）试图学得一个线性模型以尽可能准确地预测实值输出标记。&lt;/p&gt;

&lt;p&gt;我们使用从UCI Housing Data Set获得的波士顿房价数据集进行模型的训练和预测。下面的散点图展示了使用模型对部分房屋价格进行的预测。其中，每个点的横坐标表示同一类房屋真实价格的中位数，纵坐标表示线性回归模型根据特征预测的结果，当二者值完全相等的时候就会落在虚线上。所以模型预测得越准确，则点离虚线越近。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归0.png?raw=true&quot; alt=&quot;0&quot; title=&quot;Title&quot; height=&quot;200px&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;输入一维输出一维&quot;&gt;输入一维，输出一维&lt;/h2&gt;
&lt;p&gt;我们先考虑一种最简单的情形：输入属性的数目只有一个。&lt;br /&gt;
对于点对 &lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/lir1.png?raw=true&quot; alt=&quot;lir1&quot; title=&quot;Title&quot; /&gt; , 其中xi ∈ R。对离散属性，若属性值间存在”序“关系，可通过连续化将其转化为连续值，例如二值属性”身高“的取值”高“ 和 ”矮“ 可转化为{1.0, 0.0}&lt;/p&gt;

&lt;p&gt;有以下线性模型：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归1.png?raw=true&quot; alt=&quot;线性回归1&quot; title=&quot;Title&quot; /&gt;&lt;br /&gt;
使得&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归2.png?raw=true&quot; alt=&quot;线性回归2&quot; title=&quot;Title&quot; /&gt;&lt;br /&gt;
当输入是很多个xi ，每个xi都有一个标签yi ，那么该线性模型，就是找一条直线拟合这些点，也就是上式中的f(x) ，对于每一个输出的f(xi) 与 yi ，可以设置不同的损失函数，我们以MSE（均方误差）为例：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归3.png?raw=true&quot; alt=&quot;线性回归3&quot; title=&quot;Title&quot; /&gt;&lt;br /&gt;
均方误差有非常好的几何意义，它对应了常用的欧几里得距离或简称欧式距离。基于均方误差最小化来进行模型求解的方法称为”最小二乘法“（least square method）。在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。
那么使得 Loss 最小的 w 与 b ，就是最优模型参数。既：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归4.png?raw=true&quot; alt=&quot;线性回归4&quot; title=&quot;Title&quot; /&gt; &lt;br /&gt;
求解方法，我们可以先将 Loss 对 w 与 b 求导：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归5.png?raw=true&quot; alt=&quot;线性回归5&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;直接令上式为0，求解 w 与 b ，就是“最小二乘法”；在一维输入，一维输出的情形下，通常直接使用最小二乘求出最优解。当输入输出维度变多后，可以使用梯度下降法迭代求解。&lt;/p&gt;

&lt;h2 id=&quot;输入多维输出一维&quot;&gt;输入多维，输出一维&lt;/h2&gt;
&lt;p&gt;更一般的情形是样本有d个属性描述，对于点对&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归6.png?raw=true&quot; alt=&quot;6&quot; title=&quot;Title&quot; /&gt;，其中&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归7.png?raw=true&quot; alt=&quot;7&quot; title=&quot;Title&quot; /&gt;，有以下线性模型：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归8.png?raw=true&quot; alt=&quot;8&quot; title=&quot;Title&quot; /&gt;&lt;br /&gt;
使得&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归9.png?raw=true&quot; alt=&quot;9&quot; title=&quot;Title&quot; /&gt;&lt;br /&gt;
为了便于讨论，我们把w和b吸收入向量形式w=(w;b),即&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归10.png?raw=true&quot; alt=&quot;10&quot; title=&quot;Title&quot; /&gt;&lt;br /&gt;
相应的，把数据集D表示为一个m x (d+1)大小的矩阵X，其中每行对应于一个示例，该行前d个元素对应于示例的d个属性值，最后一个元素恒置为1，即&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归11.png?raw=true&quot; alt=&quot;11&quot; title=&quot;Title&quot; /&gt;&lt;br /&gt;
上式可改写为：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归12.png?raw=true&quot; alt=&quot;12&quot; title=&quot;Title&quot; /&gt;&lt;br /&gt;
由于 &lt;strong&gt;&lt;em&gt;X&lt;/em&gt;&lt;/strong&gt; 的维度是m x (n + 1) ， &lt;strong&gt;&lt;em&gt;W&lt;/em&gt;&lt;/strong&gt; 的维度是 (n + 1) x 1， 方程组的个数和求解未知数个数（&lt;strong&gt;&lt;em&gt;W&lt;/em&gt;&lt;/strong&gt;）不相等，上式无法使用传统求解方程组的算法来求解。这里同样使用最小二乘法，在等式两边同乘&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归16.png?raw=true&quot; alt=&quot;16&quot; title=&quot;Title&quot; /&gt; ：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归13.png?raw=true&quot; alt=&quot;13&quot; title=&quot;Title&quot; /&gt;&lt;br /&gt;
现在左边 &lt;strong&gt;&lt;em&gt;W&lt;/em&gt;&lt;/strong&gt; 的系数矩阵&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归17.png?raw=true&quot; alt=&quot;17&quot; title=&quot;Title&quot; /&gt;的维度是(n+1) x (n+1) ，右边的矩阵维度也为 (n+1) x (n+1) ，并且此方程 &lt;strong&gt;&lt;em&gt;W&lt;/em&gt;&lt;/strong&gt; 的解正是最小二乘解，相应证明可参照数值代数中的详解。即该模型的参数最优解为：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归14.png?raw=true&quot; alt=&quot;14&quot; title=&quot;Title&quot; /&gt;&lt;br /&gt;
该线性模型为：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归15.png?raw=true&quot; alt=&quot;15&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;输入多维输出多维&quot;&gt;输入多维，输出多维&lt;/h2&gt;
&lt;p&gt;对于点对&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归18.png?raw=true&quot; alt=&quot;18&quot; title=&quot;Title&quot; /&gt; ，其中 &lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归19.png?raw=true&quot; alt=&quot;19&quot; title=&quot;Title&quot; /&gt;，&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归20.png?raw=true&quot; alt=&quot;20&quot; title=&quot;Title&quot; /&gt;，有以下线性模型：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归21.png?raw=true&quot; alt=&quot;21&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这就是神经网络中的全连接层，用矩阵的形式可写为下式：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归22.png?raw=true&quot; alt=&quot;22&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可简写为下式：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归23.png?raw=true&quot; alt=&quot;23&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中，&lt;strong&gt;&lt;em&gt;X&lt;/em&gt;&lt;/strong&gt; 的维度为 m x (n + 1) ，&lt;strong&gt;&lt;em&gt;W&lt;/em&gt;&lt;/strong&gt; 的维度为 (n + 1) x k ，&lt;strong&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/strong&gt; 的维度为 m x k 。多维输入，多维输出的线性模型，也就是全连接层，一般不使用最小二乘法求解，在神经网络寻优中，通常使用梯度下降法，求解参数。&lt;/p&gt;

&lt;p&gt;类似于一维输入最小化均方误差公式，有：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归24.png?raw=true&quot; alt=&quot;24&quot; title=&quot;Title&quot; height=&quot;50px&quot; width=&quot;350px&quot; /&gt;&lt;br /&gt;
令&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归25.png?raw=true&quot; alt=&quot;25&quot; title=&quot;Title&quot; height=&quot;20px&quot; width=&quot;200px&quot; /&gt;，对w^求导得到：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归26.png?raw=true&quot; alt=&quot;26&quot; title=&quot;Title&quot; height=&quot;50px&quot; width=&quot;250px&quot; /&gt;&lt;br /&gt;
令上式为零可得w^最优解的闭式解，但由于涉及矩阵逆的计算，比单变量情形要复杂一些。&lt;br /&gt;
当&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归29.png?raw=true&quot; alt=&quot;29&quot; title=&quot;Title&quot; height=&quot;20px&quot; width=&quot;30px&quot; /&gt;为满秩矩阵或正定矩阵时，令上式为零可得：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归27.png?raw=true&quot; alt=&quot;27&quot; title=&quot;Title&quot; height=&quot;30px&quot; width=&quot;150px&quot; /&gt;&lt;br /&gt;
其中&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归28.png?raw=true&quot; alt=&quot;28&quot; title=&quot;Title&quot; height=&quot;20px&quot; width=&quot;30px&quot; /&gt;是矩阵&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归29.png?raw=true&quot; alt=&quot;29&quot; title=&quot;Title&quot; height=&quot;20px&quot; width=&quot;30px&quot; /&gt;的逆矩阵。令xi^ = (xi:1)，则最终学得的多元线性回归模型为：&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归30.png?raw=true&quot; alt=&quot;30&quot; title=&quot;Title&quot; height=&quot;30px&quot; width=&quot;200px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;然而，现实任务中&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归29.png?raw=true&quot; alt=&quot;29&quot; title=&quot;Title&quot; height=&quot;20px&quot; width=&quot;30px&quot; /&gt;往往不是满秩矩阵。例如在许多任务中我们会遇到大量的变量，其数目甚至超过样例数，导致&lt;strong&gt;&lt;em&gt;X&lt;/em&gt;&lt;/strong&gt;的列数多于行数，&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归29.png?raw=true&quot; alt=&quot;29&quot; title=&quot;Title&quot; height=&quot;20px&quot; width=&quot;30px&quot; /&gt;显然不满秩。此时可解出多个w^，它们都能使均方误差最小化。选择哪一个解作为输出，将由学习算法的归纳偏好决定，常见的做法是引入正则化项。&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考：&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/90917024&lt;/li&gt;
  &lt;li&gt;周志华《机器学习》&lt;/li&gt;
&lt;/ol&gt;
</description>
                <link>http://username.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2020/02/03/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92</link>
                <guid>http://username.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2020/02/03/线性模型之线性回归</guid>
                <pubDate>Mon, 03 Feb 2020 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>线性模型</title>
                <description>
&lt;h2 id=&quot;线性模型&quot;&gt;线性模型&lt;/h2&gt;
&lt;p&gt;线性模型（linear model）试图学得一个通过属性的线性组合来进行预测的函数，即  &lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性组合.png?raw=true&quot; alt=&quot;线性组合&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一般用向量形式写成
        &lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性组合(向量形式).png?raw=true&quot; alt=&quot;线性组合(向量形式)&quot; title=&quot;Title&quot; /&gt; ，其中&lt;strong&gt;&lt;em&gt;w&lt;/em&gt;&lt;/strong&gt;=(&lt;em&gt;w1&lt;/em&gt;;&lt;em&gt;w2&lt;/em&gt;;…;&lt;em&gt;wd&lt;/em&gt;)，解释一下这里向量中的分号：逗号分割的向量通常为行向量，分号分割通常为列向量。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;w&lt;/em&gt;和&lt;em&gt;b&lt;/em&gt;学得之后，模型就得以确定。&lt;em&gt;w&lt;/em&gt;和&lt;em&gt;b&lt;/em&gt;一般使用训练数据拟合、迭代或直接解出，这个求解过程被称作“学习”。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性模型.jpeg?raw=true&quot; alt=&quot;线性模型&quot; title=&quot;Title&quot; height=&quot;200px&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;线性回归&quot;&gt;线性回归&lt;/h2&gt;
&lt;p&gt;线性回归是一个回归问题，即用一条线去拟合训练数据。”线性回归“试图学得一个线性模型以尽可能准确地预测实值输出标记。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;目标函数&lt;/strong&gt;：&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归1.png?raw=true&quot; alt=&quot;线性回归1&quot; title=&quot;Title&quot; /&gt;，使得&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归2.png?raw=true&quot; alt=&quot;线性回归2&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;损失函数&lt;/strong&gt;：MSE-均方误差（注意与感知机的区别，此处误分类点与坐标轴垂直）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归3.png?raw=true&quot; alt=&quot;线性回归3&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;学习策略&lt;/strong&gt;：最小化损失函数，求解参数w和b&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/线性回归4.png?raw=true&quot; alt=&quot;线性回归4&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;求解方法&lt;/strong&gt;： 最小二乘法，梯度下降法&lt;/p&gt;

&lt;h2 id=&quot;逻辑回归&quot;&gt;逻辑回归&lt;/h2&gt;
&lt;p&gt;逻辑回归是一个二分类问题。将线性回归的输出作为sigmoid函数的输入，最终的输出便是分类的结果即是输入的条件概率。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;目标函数&lt;/strong&gt;：&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归1.png?raw=true&quot; alt=&quot;逻辑回归1&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;原始损失函数&lt;/strong&gt;：求所有训练样本的条件概率之积的最大值。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归2.png?raw=true&quot; alt=&quot;逻辑回归2&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;对数损失函数&lt;/strong&gt;：目标是求得损失函数的最大值，即：最大似然估计。通过转换为对数形式，将最大值优化问题转换成最小值优化问题&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/逻辑回归3.png?raw=true&quot; alt=&quot;逻辑回归3&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;学习策略&lt;/strong&gt;：最大似然估计&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;求解方法&lt;/strong&gt;：梯度下降、牛顿法&lt;/p&gt;

&lt;h2 id=&quot;感知机&quot;&gt;感知机&lt;/h2&gt;
&lt;p&gt;感知机是一个二分类问题。线性回归的输出作为阶跃函数的输入，最终的输出便是分类的结果。（感知器算法存在跳跃，在0点不可导，且在0附近模型容易受到干扰）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;目标函数&lt;/strong&gt;：&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/感知机1.png?raw=true&quot; alt=&quot;感知机1&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;损失函数&lt;/strong&gt;：误分类点到超平面（wx+b=0）的总距离（1/|w|不影响求最优值，故省掉）&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/感知机2.png?raw=true&quot; alt=&quot;感知机2&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;学习策略&lt;/strong&gt;：最小化损失函数。（注意与线性回归的区别，此处误分类点与超平面垂直）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;求解方法&lt;/strong&gt;：梯度下降法&lt;/p&gt;

&lt;h2 id=&quot;svm&quot;&gt;SVM&lt;/h2&gt;
&lt;p&gt;在感知器分类选分类超平面时，我们可以选择很多个平面作为超平面，而选择哪个超平面最好呢，我们可以选择距离正样本和负样本最远的超平面(间隔最大)作为分类超平面，基于这种想法人们提出了SVM算法。&lt;/p&gt;

&lt;p&gt;支持向量机（support vector machines，SVM）是一种二类分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机。&lt;/p&gt;

&lt;p&gt;支持向量机学习方法包含构建由简至繁的模型：
当训练数据线性可分时，通过硬间隔最大化（hard margin maximization），学习一个线性分类器，即
&lt;strong&gt;线性可分支持向量机（linear support vector machine in linearly separable case）&lt;/strong&gt;；
当训练数据近似线性可分时，通过软间隔最大化（soft margin maximization），也学习一个线性的分类器，即 &lt;strong&gt;线性支持向量机（linear support vector machine）&lt;/strong&gt;；
当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习 &lt;strong&gt;非线性支持向量机（no-linear support vector machine）&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;损失函数&lt;/strong&gt;: 为合页函数，当分类错误时，函数间隔越大，则损失函数值越大。当分类正确且样本点距离超平面一定距离以上，则损失函数值为0。误分类的点和与分类超平面距离较近的点会影响损失函数的值。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;学习策略&lt;/strong&gt;：间隔最大化，可形式化为一个求解凸二次规划（convex quadratic progrmming）的问题，也等价于正则化的合页损失函数的最小化问题。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;线性回归用于解决回归问题；其他三类解决分类问题，且都是对线性回归的输出做了一些处理，logistic和svm是由感知器发展改善而来的。&lt;/p&gt;

&lt;p&gt;区别在于三者的损失函数不同。后两者的损失函数的目的都是增加对分类影响较大的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。考虑局部最优化，如何让靠近中间线的点尽可能的远离中间线会占用更高的权值，远离中间线的值，权重为零；逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重，在所有样本上最优。&lt;/p&gt;
</description>
                <link>http://username.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2020/01/02/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B</link>
                <guid>http://username.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2020/01/02/线性模型</guid>
                <pubDate>Thu, 02 Jan 2020 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>MySQL explain命令详解</title>
                <description>
&lt;p&gt;MySQL的EXPLAIN命令用于SQL语句的查询执行计划(QEP)。这条命令的输出结果能够让我们了解MySQL 优化器是如何执行SQL 语句的。这条命令并没有提供任何调整建议，但它能够提供重要的信息帮助你做出调优决策。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;语法&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;MySQL 的EXPLAIN 语法可以运行在SELECT 语句或者特定表上。如果作用在表上，那么此命令等同于DESC 表命令。&lt;/p&gt;

&lt;p&gt;UPDATE和DELETE 命令也需要进行性能改进，当这些命令不是直接在表的主码上运行时，为了确保最优化的索引使用率，需要把它们改写成SELECT 语句(以便对它们执行EXPLAIN 命令)。请看下面的示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;UPDATE table1
SET col1 = X, col2 = Y
WHERE id1 = 9
AND dt &amp;gt;= '2010-01-01';
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个UPDATE语句可以被重写成为下面这样的SELECT语句：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT col1, col2
FROM table1
WHERE id1 = 9
AND dt &amp;gt;= '2010-01-01';  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在5.6.10版本里面,是可以直接对DML语句进行explain分析操作的.&lt;/p&gt;

&lt;p&gt;MySQL 优化器是基于开销来工作的，它并不提供任何的QEP的位置。这意味着QEP 是在每条SQL 语句执行的时候动态地计算出来的。在MySQL 存储过程中的SQL 语句也是在每次执行时计算QEP 的。存储过程缓存仅仅解析查询树。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;各列详解&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;MySQL EXPLAIN命令能够为SQL语句中的每个表生成以下信息：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/mysql1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这个QEP 显示没有使用任何索引(也就是全表扫描)并且处理了大量的行来满足查询。&lt;/p&gt;

&lt;p&gt;对同样一条SELECT 语句，一个优化过的QEP 如下所示:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/mysql2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在这个QEP 中，我们看到使用了一个索引，且估计只有一行数据将被获取。&lt;/p&gt;

&lt;p&gt;QEP 中每个行的所有列表如下所示：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;列&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;描述&lt;/th&gt;
      &lt;th&gt;备注&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;id&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;The SELECT identifier&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;select_type&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;The SELECT type&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;table&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;The table for the output row&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;partitions&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;The matching partitions&lt;/td&gt;
      &lt;td&gt;(这一列只有在EXPLAIN PARTITIONS 语法中才会出现)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;type&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;The join type&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;possible_keys&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;The possible indexes to choose&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;key&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;The index actually chosen&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;key_len&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;The length of the chosen key&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ref&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;The columns compared to the index&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;rows&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Estimate of rows to be examined&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;filtered&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Percentage of rows filtered by table condition&lt;/td&gt;
      &lt;td&gt;(这一列只有在EXPLAINED EXTENDED 语法中才会出现)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Extra&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Additional information&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;这些列展示了SELECT 语句对每一个表的QEP。一个表可能和一个物理模式表或者在SQL 执行时生成的内部临时表(例如从子查询或者合并操作会产生内部临时表)相关联。&lt;/p&gt;

&lt;p&gt;可以参考MySQL Reference Manual 获得更多信息：&lt;/p&gt;

&lt;p&gt;http://dev.mysql.com/doc/refman/5.5/en/explain-output.html。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;key&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;key 列指出优化器选择使用的索引。一般来说SQL 查询中的每个表都仅使用一个索引。也存在索引合并的少数例外情况，如给定表上用到了两个或者更多索引。&lt;/p&gt;

&lt;p&gt;下面是QEP 中key 列的示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;key: item_id
key: NULL
key: first, last
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SHOW CREATE TABLE &amp;lt;table&amp;gt;命令是最简单的查看表和索引列细节的方式。和key 列相关的列还包括possible_keys、rows 以及key_len。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ROWS&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;rows 列提供了试图分析所有存在于累计结果集中的行数目的MySQL 优化器估计值。QEP 很容易描述这个很困难的统计量。查询中总的读操作数量是基于合并之前行的每一行的rows 值的连续积累而得出的。这是一种嵌套行算法。&lt;/p&gt;

&lt;p&gt;以连接两个表的QEP 为例。通过id=1 这个条件找到的第一行的rows 值为1，这等于对第一个表做了一次读操作。第二行是通过id=2 找到的，rows 的值为5。这等于有5 次读操作符合当前1 的积累量。参考两个表，读操作的总数目是6。在另一个QEP 中，第一rows 的值是5，第二rows 的值是1。这等于第一个表有5 次读操作，对5个积累量中每个都有一个读操作。因此两个表总的读操作的次数是10(5+5)次。&lt;/p&gt;

&lt;p&gt;最好的估计值是1，一般来说这种情况发生在当寻找的行在表中可以通过主键或者唯一键找到的时候。
 在下面的QEP 中，外面的嵌套循环可以通过id=1 来找到，其估计的物理行数是1。第二个循环处理了10行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;********************* 1. row ***********************
id: 1
select_type: SIMPLE
table: p
type: const
possible_keys: PRIMARY
key: PRIMARY
key_len: 4
ref: const
rows: 1
Extra:
********************* 2. row ***********************
id: 1
select_type: SIMPLE
table: c
type: ref
possible_keys: parent_id
key: parent_id
key_len: 4
ref: const
rows: 10
Extra:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以使用SHOW STATUS 命令来查看实际的行操作。这个命令可以提供最佳的确认物理行操作的方式。请看下面的示例：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/mysql3.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在下一个QEP 中，通过id=1 找到的外层嵌套循环估计有160行。第二个循环估计有1 行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;********************* 1. row ***********************
id: 1
select_type: SIMPLE
table: p
type: ALL
possible_keys: NULL
key: NULL
key_len: NULL
ref: NULL
rows: 160
Extra:
********************* 2. row ***********************
id: 1
select type: SIMPLE
table: c
type: ref
possible_keys: PRIMARY,parent_id
key: parent_id
key_len: 4
ref: test.p.parent_id
rows: 1
Extra: Using where
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过SHOW STATUS 命令可以查看实际的行操作，该命令表明物理读操作数量大幅增加。请看下面的示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql&amp;gt; SHOW SESSION STATUS LIKE 'Handler_read%';
+--------------------------------------+---------+
| Variable_name | Value |
+--------------------------------------+---------+
| Handler_read_first | 1 |
| Handler_read_key | 164 |
| Handler_read_last | 0 |
| Handler_read_next | 107 |
| Handler_read_prev | 0 |
| Handler_read_rnd | 0 |
| Handler_read_rnd_next | 161 |
+--------------------------------------+---------+
相关的QEP 列还包括key列。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;possible_keys&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;possible_keys 列指出优化器为查询选定的索引。&lt;/p&gt;

&lt;p&gt;一个会列出大量可能的索引(例如多于3 个)的QEP 意味着备选索引数量太多了，同时也可能提示存在一个无效的单列索引。&lt;/p&gt;

&lt;p&gt;可以用SHOW INDEXES 命令来检查索引是否有效且是否具有合适的基数。&lt;/p&gt;

&lt;p&gt;为查询确定QEP 的速度也会影响到查询的性能。如果发现有大量的可能的索引，则意味着这些索引没有被使用到。
 相关的QEP 列还包括key 列。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;key_len&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;key_len 列定义了用于SQL 语句的连接条件的键的长度。此列值对于确认索引的有效性以及多列索引中用到的列的数目很重要。此列的一些示例值如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;key_len: 4 // INT NOT NULL
key_len: 5 // INT NULL
key_len: 30 // CHAR(30) NOT NULL
key_len: 32 // VARCHAR(30) NOT NULL
key_len: 92 // VARCHAR(30) NULL CHARSET=utf8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从这些示例中可以看出，是否可以为空、可变长度的列以及key_len 列的值只和用在连接和WHERE 条件中的索引的列有关。索引中的其他列会在ORDER BY 或者GROUP BY 语句中被用到。&lt;/p&gt;

&lt;p&gt;下面这个来自于著名的开源博客软件WordPress 的表展示了如何以最佳方式使用带有定义好的表索引的SQL 语句：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; CREATE TABLE `wp_posts` (
  `ID` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `post_date` datetime NOT NULL DEFAULT '0000-00-00 00:00:00',
  `post_status` varchar(20) NOT NULL DEFAULT 'publish' ,
  `post_type` varchar(20) NOT NULL DEFAULT 'post',
  PRIMARY KEY (`ID`),
  KEY `type_status_date`(`post_type`,`post_status`,`post_date`,`ID`)
 ) DEFAULT CHARSET=utf8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个表的索引包括post_type、post_status、post_date 以及ID列。&lt;/p&gt;

&lt;p&gt;下面是一个演示索引列用法的SQL 查询：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;EXPLAIN SELECT ID, post_title FROM wp_posts WHERE post_type='post' AND post_date &amp;gt; '2010-06-01';
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个查询的QEP 返回的key_len 是62。这说明只有post_type列上的索引用到了(因为(20×3)+2=62)。尽管查询在WHERE 语句中使用了post_type 和post_date 列，但只有post_type 部分被用到了。其他索引没有被使用的原因是MySQL 只能使用定义索引的最左边部分。为了更好地利用这个索引，可以修改这个查询来调整索引的列。请看下面的示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql&amp;gt; EXPLAIN SELECT ID, post_title
-&amp;gt; FROM wp_posts
-&amp;gt; WHERE post_type='post'
-&amp;gt; AND post_status='publish'
-&amp;gt; AND post_date &amp;gt; '2010-06-01';
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在SELECT查询的添加一个post_status 列的限制条件后，QEP显示key_len 的值为132，这意味着post_type、post_status、post_date 三列(62+62+8，(20×3)+2，(20×3)+2，8)都被用到了。&lt;/p&gt;

&lt;p&gt;此外，这个索引的主码列ID 的定义是使用MyISAM 存储索引的遗留痕迹。当使用InnoDB 存储引擎时，在非主码索引中包含主码列是多余的，这可以从key_len 的用法看出来。相关的QEP 列还包括带有Using index 值的Extra 列。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;注&lt;/code&gt;：key_len 计算&lt;/p&gt;

&lt;p&gt;(1) 索引字段的附加信息：可以分为变长和定长数据类型讨论，当索引字段为定长数据类型，比如char，int，datetime，需要有是否为空的标记，这个标记需要占用1个字节；对于变长数据类型，比如：varchar，除了是否为空的标记外，还需要有长度信息，需要占用2个字节；&lt;/p&gt;

&lt;p&gt;(备注：当字段定义为非空的时候，是否为空的标记将不占用字节)&lt;/p&gt;

&lt;p&gt;(2)同时还需要考虑表所使用的字符集，不同的字符集，gbk编码的为一个字符2个字节，utf8编码的一个字符3个字节;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;table&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;table 列是EXPLAIN 命令输出结果中的一个单独行的唯一标识符。这个值可能是表名、表的别名或者一个为查询产生临时表的标识符，如派生表、子查询或集合。下面是QEP 中table 列的一些示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table: item
table: &amp;lt;derivedN&amp;gt;
table: &amp;lt;unionN,M&amp;gt;  表中N 和M 的值参考了另一个符合id 列值的table 行。相关的QEP 列还有select_type
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;select_type&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;select_type 列提供了各种表示table 列引用的使用方式的类型。最常见的值包括SIMPLE、PRIMARY、DERIVED 和UNION。其他可能的值还有UNION RESULT、DEPENDENT SUBQUERY、DEPENDENT UNION、UNCACHEABLE UNION 以及UNCACHEABLE QUERY。&lt;/p&gt;

&lt;p&gt;1.SIMPLE&lt;/p&gt;

&lt;p&gt;对于不包含子查询和其他复杂语法的简单查询，这是一个常见的类型。&lt;/p&gt;

&lt;p&gt;2.PRIMARY&lt;/p&gt;

&lt;p&gt;这是为更复杂的查询而创建的首要表(也就是最外层的表)。这个类型通常可以在DERIVED 和UNION 类型混合使用时见到。&lt;/p&gt;

&lt;p&gt;3.DERIVED&lt;/p&gt;

&lt;p&gt;当一个表不是一个物理表时，那么就被叫做DERIVED。下面的SQL 语句给出了一个QEP 中DERIVED select-type 类型的示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql&amp;gt; EXPLAIN SELECT MAX(id)
-&amp;gt; FROM (SELECT id FROM users WHERE first = 'west') c;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4.DEPENDENT SUBQUERY&lt;/p&gt;

&lt;p&gt;这个select-type 值是为使用子查询而定义的。下面的SQL语句提供了这个值：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql&amp;gt; EXPLAIN SELECT p.*
-&amp;gt; FROM parent p
-&amp;gt; WHERE p.id NOT IN (SELECT c.parent_id FROM child c);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5.UNION&lt;/p&gt;

&lt;p&gt;这是UNION 语句其中的一个SQL 元素。&lt;/p&gt;

&lt;p&gt;6.UNION RESULT&lt;/p&gt;

&lt;p&gt;这是一系列定义在UNION 语句中的表的返回结果。当select_type 为这个值时，经常可以看到table 的值是&amp;lt;unionN,M&amp;gt;，这说明匹配的id 行是这个集合的一部分。下面的SQL产生了一个UNION和UNION RESULT select-type：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql&amp;gt; EXPLAIN SELECT p.* FROM parent p WHERE p.val LIKE 'a%'
-&amp;gt; UNION
-&amp;gt; SELECT p.* FROM parent p WHERE p.id &amp;gt; 5;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;partitions&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;partitions 列代表给定表所使用的分区。这一列只会在EXPLAIN PARTITIONS 语句中出现。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Extra&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Extra 列提供了有关不同种类的MySQL 优化器路径的一系列额外信息。Extra 列可以包含多个值，可以有很多不同的取值，并且这些值还在随着MySQL 新版本的发布而进一步增加。下面给出常用值的列表。你可以从下面的地址找到更全面的值的列表：&lt;/p&gt;

&lt;p&gt;http://dev.mysql.com/doc/refman/5.5/en/explain-output.html。&lt;/p&gt;

&lt;p&gt;1.Using where&lt;/p&gt;

&lt;p&gt;这个值表示查询使用了where 语句来处理结果——例如执行全表扫描。如果也用到了索引，那么行的限制条件是通过获取必要的数据之后处理读缓冲区来实现的。&lt;/p&gt;

&lt;p&gt;2.Using temporary&lt;/p&gt;

&lt;p&gt;这个值表示使用了内部临时(基于内存的)表。一个查询可能用到多个临时表。有很多原因都会导致MySQL 在执行查询期间创建临时表。两个常见的原因是在来自不同表的列上使用了DISTINCT，或者使用了不同的ORDER BY 和GROUP BY 列。&lt;/p&gt;

&lt;p&gt;想了解更多内容可以访问：&lt;/p&gt;

&lt;p&gt;http://forge.mysql.com/wiki/Overview_of_query_execution_and_use_of_temp_tables。&lt;/p&gt;

&lt;p&gt;可以强制指定一个临时表使用基于磁盘的MyISAM 存储引擎。这样做的原因主要有两个：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;内部临时表占用的空间超过min(tmp_table_size，max_heap_table_size)系统变量的限制
 
使用了TEXT/BLOB 列
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.Using filesort&lt;/p&gt;

&lt;p&gt;这是ORDER BY 语句的结果。这可能是一个CPU 密集型的过程。
 可以通过选择合适的索引来改进性能，用索引来为查询结果排序。详细过程请参考第4 章。&lt;/p&gt;

&lt;p&gt;4.Using index&lt;/p&gt;

&lt;p&gt;这个值重点强调了只需要使用索引就可以满足查询表的要求，不需要直接访问表数据。请参考第5 章的详细示例来理解这个值。&lt;/p&gt;

&lt;p&gt;5.Using join buffer&lt;/p&gt;

&lt;p&gt;这个值强调了在获取连接条件时没有使用索引，并且需要连接缓冲区来存储中间结果。
 如果出现了这个值，那应该注意，根据查询的具体情况可能需要添加索引来改进性能。&lt;/p&gt;

&lt;p&gt;6.Impossible where&lt;/p&gt;

&lt;p&gt;这个值强调了where 语句会导致没有符合条件的行。请看下面的示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql&amp;gt; EXPLAIN SELECT * FROM user WHERE 1=2;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;7.Select tables optimized away&lt;/p&gt;

&lt;p&gt;这个值意味着仅通过使用索引，优化器可能仅从聚合函数结果中返回一行。&lt;/p&gt;

&lt;p&gt;8.Distinct
 这个值意味着MySQL 在找到第一个匹配的行之后就会停止搜索其他行。&lt;/p&gt;

&lt;p&gt;9.Index merges&lt;/p&gt;

&lt;p&gt;当MySQL 决定要在一个给定的表上使用超过一个索引的时候，就会出现以下格式中的一个，详细说明使用的索引以及合并的类型。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Using sort_union(...)
Using union(...)
Using intersect(...)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;id&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;id 列是在QEP 中展示的表的连续引用。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ref&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ref 列可以被用来标识那些用来进行索引比较的列或者常量。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;filtered&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;filtered 列给出了一个百分比的值，这个百分比值和rows 列的值一起使用，可以估计出那些将要和QEP 中的前一个表进行连接的行的数目。前一个表就是指id 列的值比当前表的id 小的表。这一列只有在EXPLAIN EXTENDED 语句中才会出现。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;type&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;type 列代表QEP 中指定的表使用的连接方式。下面是最常用的几种连接方式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const 当这个表最多只有一行匹配的行时出现system 这是const 的特例，当表只有一个row 时会出现eq_ref 这个值表示有一行是为了每个之前确定的表而读取的
ref 这个值表示所有具有匹配的索引值的行都被用到
range 这个值表示所有符合一个给定范围值的索引行都被用到
ALL 这个值表示需要一次全表扫描其他类型的值还有fulltext 、ref_or_null 、index_merge 、unique_subquery、index_subquery 以及index。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;想了解更多信息可以访问http://dev.mysql.com/doc/refman/5.5/en/explain-output.html。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;解释EXPLAIN 输出结果&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;理解你的应用程序(包括技术和实现可能性)和优化SQL 语句同等重要。下面给出一个从父子关系中获取孤立的父辈记录的商业需求的例子。这个查询可以用三种不同的方式构造。尽管会产生相同的结果，但QEP 会显示三种不同的路径。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/mysql4.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/mysql5.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

</description>
                <link>http://username.github.io/2017/04/05/mysql-explain</link>
                <guid>http://username.github.io/2017/04/05/mysql-explain</guid>
                <pubDate>Wed, 05 Apr 2017 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>Cookie和Session的区别</title>
                <description>
&lt;p&gt;Http是一个无状态协议，http协议状态的保持通常有两种实现方式：cookie和session。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Cookies：通过客户端来保持状态信息&lt;/p&gt;

    &lt;p&gt;Cookie是服务器发给客户端的特殊信息&lt;/p&gt;

    &lt;p&gt;Cookie是以文本的方式保存在客户端，每次请求时都带上它&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Session：通过服务器端来保持状态信息&lt;/p&gt;

    &lt;p&gt;Session是服务器和客户端之间的一系列的交互动作&lt;/p&gt;

    &lt;p&gt;服务器为每个客户端开辟内存空间，从而保持状态信息&lt;/p&gt;

    &lt;p&gt;由于需要客户端也要持有一个标识(id)，因此，也要求服务器端和客户端传输该标识，&lt;/p&gt;

    &lt;p&gt;标识(id)可以借助Cookie机制或者其他的途径来保存&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;COOKIE机制&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1) Cookie的基本特点&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Cookie保存在客户端&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;只能保存字符串对象，不能保存对象类型&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;需要客户端浏览器的支持：客户端可以不支持，浏览器用户可能会禁用Cookie&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2) 采用Cookie需要解决的问题&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Cookie的创建&lt;/p&gt;

    &lt;p&gt;通常是在服务器端创建的(当然也可以通过javascript来创建)&lt;/p&gt;

    &lt;p&gt;服务器通过在http的响应头加上特殊的指示，那么浏览器在读取这个指示后就会生成相应的cookie了&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cookie存放的内容&lt;/p&gt;

    &lt;p&gt;业务信息(“key”,”value”)&lt;/p&gt;

    &lt;p&gt;过期时间&lt;/p&gt;

    &lt;p&gt;域和路径&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;浏览器是如何通过Cookie和服务器通信？&lt;/p&gt;

    &lt;p&gt;通过请求与响应，cookie在服务器和客户端之间传递&lt;/p&gt;

    &lt;p&gt;每次请求和响应都把cookie信息加载到响应头中；依靠cookie的key传递。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;COOKIE编程&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Cookie类&lt;/p&gt;

    &lt;p&gt;Servlet API封装了一个类：javax.servlet.http.Cookie，封装了对Cookie的操作，包括：&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;public Cookie(String name, String value) //构造方法，用来创建一个Cookie&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;HttpServletRequest.getCookies() //从Http请求中可以获取Cookies&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;HttpServletResponse.addCookie(Cookie) //往Http响应添加Cookie&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;public int getMaxAge() //获取Cookie的过期时间值&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;public void setMaxAge(int expiry) //设置Cookie的过期时间值&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cookie的创建&lt;/p&gt;

    &lt;p&gt;Cookie是一个名值对(key=value)，而且不管是key还是value都是字符串&lt;/p&gt;

    &lt;p&gt;Cookie visit = new Cookie(“visit”, “1”);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cookie的类型——过期时间&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;会话Cookie&lt;/p&gt;

        &lt;p&gt;Cookie.setMaxAge(-1);//负整数&lt;/p&gt;

        &lt;p&gt;保存在浏览器的内存中，也就是说关闭了浏览器，cookie就会丢失&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;普通cookie&lt;/p&gt;

        &lt;p&gt;Cookie.setMaxAge(60);//正整数，单位是秒&lt;/p&gt;

        &lt;p&gt;表示浏览器在1分钟内不继续访问服务器，Cookie就会被过时失效并销毁(通常保存在文件中)&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;注意&lt;/code&gt;：&lt;/p&gt;

&lt;p&gt;cookie.setMaxAge(0);//等价于不支持Cookie；&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SESSION机制&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;每次客户端发送请求，服务断都检查是否含有sessionId。&lt;/p&gt;

&lt;p&gt;如果有，则根据sessionId检索出session并处理；如果没有，则创建一个session，并绑定一个不重复的sessionId。&lt;/p&gt;

&lt;p&gt;1) 基本特点&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;状态信息保存在服务器端。这意味着安全性更高&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;通过类似与Hashtable的数据结构来保存&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;能支持任何类型的对象(session中可含有多个对象)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2) 保存会话id的技术&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Cookie&lt;/p&gt;

    &lt;p&gt;这是默认的方式，在客户端与服务器端传递JSeesionId&lt;/p&gt;

    &lt;p&gt;缺点：客户端可能禁用Cookie&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;表单隐藏字段&lt;/p&gt;

    &lt;p&gt;在被传递回客户端之前，在 form 里面加入一个hidden域，设置JSeesionId：&lt;/p&gt;

    &lt;p&gt;&amp;lt;input type=hidden name=jsessionid value=”3948E432F90932A549D34532EE2394” /&amp;gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;URL重写&lt;/p&gt;

    &lt;p&gt;直接在URL后附加上session id的信息&lt;/p&gt;

    &lt;p&gt;HttpServletResponse对象中，提供了如下的方法：&lt;/p&gt;

    &lt;p&gt;encodeURL(url); //url为相对路径&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;SESSION编程&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1) HttpSession接口&lt;/p&gt;

&lt;p&gt;Servlet API定义了接口：javax.servlet.http.HttpSession， Servlet容器必须实现它，用以跟踪状态。&lt;/p&gt;

&lt;p&gt;当浏览器与Servlet容器建立一个http会话时，容器就会通过此接口自动产生一个HttpSession对象&lt;/p&gt;

&lt;p&gt;2) 获取Session&lt;/p&gt;

&lt;p&gt;HttpServletRequest对象获取session，返回HttpSession：&lt;/p&gt;

&lt;p&gt;request.getSession(); //表示如果session对象不存在，就创建一个新的会话&lt;/p&gt;

&lt;p&gt;request.getSession(true); //等价于上面这句；如果session对象不存在，就创建一个新的会话&lt;/p&gt;

&lt;p&gt;request.getSession(false); //表示如果session对象不存在就返回 null，不会创建新的会话对象&lt;/p&gt;

&lt;p&gt;3) Session存取信息&lt;/p&gt;

&lt;p&gt;session.setAttribute(String name,Object o) //往session中保存信息&lt;/p&gt;

&lt;p&gt;Object session.getAttribute(String name) //从session对象中根据名字获取信息&lt;/p&gt;

&lt;p&gt;4) 设置Session的有效时间&lt;/p&gt;

&lt;p&gt;public void setMaxInactiveInterval(int interval)&lt;/p&gt;

&lt;p&gt;设置最大非活动时间间隔，单位秒；&lt;/p&gt;

&lt;p&gt;如果参数interval是负值，表示永不过时。零则是不支持session。&lt;/p&gt;

&lt;p&gt;通过配置web.xml来设置会话超时，单位是分钟&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   &amp;lt;seesion-config&amp;gt;

       &amp;lt;session-timeout&amp;gt;1&amp;lt;/session-timeout&amp;gt;

   &amp;lt;/session-config&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;允许两种方式并存，但前者优先级更高&lt;/p&gt;

&lt;p&gt;5) 其他常用的API&lt;/p&gt;

&lt;p&gt;HttpSession.invalidate() //手工销毁Session&lt;/p&gt;

&lt;p&gt;boolean HttpSession.isNew() //判断Session是否新建&lt;/p&gt;

&lt;p&gt;如果是true，表示服务器已经创建了该session，但客户端还没有加入(还没有建立会话的握手)&lt;/p&gt;

&lt;p&gt;HttpSession.getId() //获取session的id&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;两种状态跟踪机制的比较&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Cookie&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Session&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;保持在客户端&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;保存在服务器端&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;只能保持字符串对象&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;支持各种类型对象&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;通过过期时间值区分Cookie的类型&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;需要sessionid来维护与客户端的通信&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;类型1：会话Cookie——负数&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;保存会话技术1：Cookie(默认)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;类型1：普通Cookie——正数&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;保存会话技术2：表单隐藏字段&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;类型3：不支持Cookie——0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;保存会话技术3：url重写&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;应用领域&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;web交易需要保存状态的时候，都可以使用，比如在分布式场景下可以利用分布式session技术等。&lt;/p&gt;
</description>
                <link>http://username.github.io/2017/02/07/cookiesession</link>
                <guid>http://username.github.io/2017/02/07/cookiesession</guid>
                <pubDate>Tue, 07 Feb 2017 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>Spring Boot入门</title>
                <description>
&lt;h2 id=&quot;why-spring-boot&quot;&gt;Why Spring Boot?&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。&lt;/p&gt;

&lt;p&gt;多年以来，Spring IO平台饱受非议的一点就是大量的XML配置以及复杂的依赖管理。Spring Boot并不是要成为Spring IO平台里面众多“Foundation”层项目的替代者。Spring Boot的目标不在于为已解决的问题域提供新的解决方案，而是为平台带来另一种开发体验，从而简化对这些已有技术的使用。&lt;/p&gt;

&lt;p&gt;Spring Boot，甚至可以说整个Spring生态系统都使用到了Groovy编程语言。Boot所提供的众多便捷功能，都是借助于Groovy强大的MetaObject协议、可插拔的AST转换过程以及内置的依赖解决方案引擎所实现的。在其核心的编译模型之中，Boot使用Groovy来构建工程文件，所以它可以使用通用的导入和样板方法（如类的main方法）对类所生成的字节码进行装饰（decorate）。这样使用Boot编写的应用就能保持非常简洁，却依然可以提供众多的功能。&lt;/p&gt;

&lt;h2 id=&quot;whats-spring-boot&quot;&gt;What’s Spring Boot?&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;从最根本上来讲，Spring Boot就是一些库的集合，它能够被任意项目的构建系统所使用。简便起见，该框架也提供了命令行界面，它可以用来运行和测试Boot应用。框架的发布版本，包括集成的CLI（命令行界面），可以在Spring仓库中手动下载和安装。一种更为简便的方式是使用Groovy环境管理器（Groovy enVironment Manager，GVM），它会处理Boot版本的安装和管理。Boot及其CLI可以通过GVM的命令行gvm install springboot进行安装。&lt;/p&gt;

&lt;p&gt;要进行打包和分发的工程会依赖于像Maven或Gradle这样的构建系统。为了简化依赖图，Boot的功能是模块化的，通过导入Boot所谓的“starter”模块，可以将许多的依赖添加到工程之中。为了更容易地管理依赖版本和使用默认配置，框架提供了一个parent POM，工程可以继承它。Spring Boot工程的样例POM文件定义如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;
&amp;lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
     xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&amp;gt;
	&amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;

	&amp;lt;groupId&amp;gt;com.example&amp;lt;/groupId&amp;gt;
	&amp;lt;artifactId&amp;gt;myproject&amp;lt;/artifactId&amp;gt;
	&amp;lt;version&amp;gt;1.0.0-SNAPSHOT&amp;lt;/version&amp;gt;

	&amp;lt;!-- Inherit defaults from Spring Boot --&amp;gt;
	&amp;lt;parent&amp;gt;
    	&amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
    	&amp;lt;artifactId&amp;gt;spring-boot-starter-parent&amp;lt;/artifactId&amp;gt;
    	&amp;lt;version&amp;gt;1.0.0.RC1&amp;lt;/version&amp;gt;
	&amp;lt;/parent&amp;gt;

	&amp;lt;!-- Add typical dependencies for a web application --&amp;gt;
	&amp;lt;dependencies&amp;gt;
    	&amp;lt;dependency&amp;gt;
        	&amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
        	&amp;lt;artifactId&amp;gt;spring-boot-starter-web&amp;lt;/artifactId&amp;gt;
    	&amp;lt;/dependency&amp;gt;
    	&amp;lt;dependency&amp;gt;
        	&amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
        	&amp;lt;artifactId&amp;gt;spring-boot-starter-actuator&amp;lt;/artifactId&amp;gt;
    	&amp;lt;/dependency&amp;gt;
	&amp;lt;/dependencies&amp;gt;

	&amp;lt;repositories&amp;gt;
    	&amp;lt;repository&amp;gt;
        	&amp;lt;id&amp;gt;spring-snapshots&amp;lt;/id&amp;gt;
        	&amp;lt;url&amp;gt;http://repo.spring.io/libs-snapshot&amp;lt;/url&amp;gt;
    	&amp;lt;/repository&amp;gt;
	&amp;lt;/repositories&amp;gt;

	&amp;lt;pluginRepositories&amp;gt;
    	&amp;lt;pluginRepository&amp;gt;
        	&amp;lt;id&amp;gt;spring-snapshots&amp;lt;/id&amp;gt;
        	&amp;lt;url&amp;gt;http://repo.spring.io/libs-snapshot&amp;lt;/url&amp;gt;
    	&amp;lt;/pluginRepository&amp;gt;
	&amp;lt;/pluginRepositories&amp;gt;

	&amp;lt;build&amp;gt;
    	&amp;lt;plugins&amp;gt;
        	&amp;lt;plugin&amp;gt;
            	&amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
            	&amp;lt;artifactId&amp;gt;spring-boot-maven-plugin&amp;lt;/artifactId&amp;gt;
        	&amp;lt;/plugin&amp;gt;
    	&amp;lt;/plugins&amp;gt;
	&amp;lt;/build&amp;gt;
&amp;lt;/project&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use?&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;使用CLI构建&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Spring Boot在刚刚公开宣布之后就将一个样例发布到了Twitter上，它目前成为了最流行的一个应用样例。它的全部描述如程序清单1.2所示，一个非常简单的Groovy文件可以生成功能强大的以Spring为后端的web应用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@RestController
class App {
	@RequestMapping(&quot;/&quot;)
	String home() {
		&quot;hello&quot;
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个应用可以通过spring run App.groovy命令在Spring Boot CLI中运行。Boot会分析文件并根据各种“编译器自动配置（compiler auto-configuration）”标示符来确定其意图是生成Web应用。然后，它会在一个嵌入式的Tomcat中启动Spring应用上下文，并且使用默认的8080端口。&lt;/p&gt;

&lt;p&gt;Boot能够自动确定类所需的功能，这一点使其成为了强大的快速应用开发工具。当应用在Boot CLI中执行时，它们在使用内部的Groovy编译器进行构建，这个编译器可以在字节码生成的时候以编码的方式探查并修改类。通过这种方式，使用CLI的开发人员不仅可以省去定义默认配置，在一定程度上甚至可以不用定义特定的导入语句，它们可以在编译的过程中识别出来并自动进行添加。除此之外，当应用在CLI中运行时，Groovy内置的依赖管理器，“Grape”，将会解析编译期和运行时的类路径依赖，与Boot编译器的自动配置机制类似。这种方式不仅使得框架更加对用户友好，而且能够让不同版本的Spring Boot与特定版本的来自于Spring IO平台的库相匹配，这样一来开发人员就不用关心如何管理复杂的依赖图和版本结构了。另外，它还有助于快速原型的开发并生成概念原型的工程代码。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;非CLI构建&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;对于不是使用CLI构建的工程，Boot提供了许多的“starter”模块，它们定义了一组依赖，这些依赖能够添加到构建系统之中，从而解析框架及其父平台所需的特定类库。例如，spring-boot-starter-actuator依赖会引入一组基本的Spring项目，从而实现应用的快速配置和即时可用。关于这种依赖，值得强调的一点就是当开发Web应用，尤其是RESTful Web服务的时候，如果包含了spring-boot-starter-web依赖，它就会为你提供启动嵌入式Tomcat容器的自动化配置，并且提供对微服务应用有价值的端点信息，如服务器信息、应用指标（metrics）以及环境详情。除此之外，如果引入spring-boot-starter-security模块的话，actuator会自动配置Spring Security，从而为应用提供基本的认证以及其他高级的安全特性。它还会为应用结构引入一个内部的审计框架，这个框架可以用来生成报告或其他的用途，比如开发认证失败的锁定策略。&lt;/p&gt;

&lt;p&gt;为了阐述在Java Maven工程中，如何快速地使Spring Web工程准备就绪，如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package com.infoq.springboot;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.EnableAutoConfiguration;
import org.springframework.web.bind.annotation.*;

@RestController
@EnableAutoConfiguration
public class Application {

	@RequestMapping(&quot;/&quot;)
	public String home() {
		return &quot;Hello&quot;;
	}

	public static void main(String[] args) {
		SpringApplication.run(Application.class, args);
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在Application类上的@EnableAutoConfiguration注解会告知Boot要采用一种特定的方式来对应用进行配置。这种方法会将其他样板式的配置均假设为框架默认的约定，因此能够聚焦于如何尽快地使应用准备就绪以便运行起来。Application类是可运行的，因此，当我们以Java Application的方式运行这个类时，就能启动该应用及其嵌入式的容器，这样也能实现即时地开发。&lt;/p&gt;

&lt;p&gt;为了发布版本而构建工程时，Boot的Maven和Gradle插件可以嵌入（hook）到这些构建系统的打包过程中，以生成可执行的“胖jar包（fat jar）”，这种jar包含了工程的所有依赖并且能够以可运行jar的方式执行。使用Maven打包Boot应用只需运行mvn package命令，与之类似，使用Gradle时，执行gradle build命令将会在构建的目标地址下生成可运行的jar。&lt;/p&gt;

&lt;h2 id=&quot;use-in-microservice&quot;&gt;Use in MicroService&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;Boot对Spring应用的开发进行了简化，提供了模块化方式导入依赖的能力，强调了开发RESTful Web服务的功能并提供了生成可运行jar的能力，这一切都清晰地表明在开发可部署的微服务方面Boot框架是一个强大的工具。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://docs.spring.io/spring-boot/docs/2.0.0.BUILD-SNAPSHOT/reference/htmlsingle/&quot;&gt;Spring Boot Reference Guide&lt;/a&gt;&lt;/p&gt;

</description>
                <link>http://username.github.io/2017/01/13/spring-boot</link>
                <guid>http://username.github.io/2017/01/13/spring-boot</guid>
                <pubDate>Fri, 13 Jan 2017 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>WSGI理解</title>
                <description>
&lt;p&gt;&lt;strong&gt;What’s WSGI&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Web服务器网关接口(Python Web Server Gateway Interface，缩写为WSGI)是为Python语言定义的Web服务器和Web应用程序或框架之间的一种简单而通用的接口。自从WSGI被开发出来以后，许多其它语言中也出现了类似接口。WSGI是作为Web服务器与Web应用程序或应用框架之间的一种低级别的接口，以提升可移植Web应用开发的共同点。WSGI是基于现存的CGI标准而设计的。&lt;/p&gt;

&lt;p&gt;WSGI区分为两个部份：一为“服务器”或“网关”，另一为“应用程序”或“应用框架”。在处理一个WSGI请求时，服务器会为应用程序提供环境资讯及一个回呼函数(Callback Function)。当应用程序完成处理请求后，透过前述的回呼函数，将结果回传给服务器。所谓的 WSGI 中间件同时实现了API的两方，因此可以在WSGI服务和WSGI应用之间起调解作用：从WSGI服务器的角度来说，中间件扮演应用程序，而从应用程序的角度来说，中间件扮演服务器。“中间件”组件可以执行以下功能：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;重写环境变量后，根据目标URL，将请求消息路由到不同的应用对象。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;允许在一个进程中同时运行多个应用程序或应用框架。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;负载均衡和远程处理，通过在网络上转发请求和响应消息。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;进行内容后处理，例如应用XSLT样式表。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;WSGI没有官方的实现, 因为WSGI更像一个协议。只要遵照这些协议,WSGI应用(Application)都可以在任何服务器(Server)上运行, 反之亦然。WSGI就是Python的CGI包装，相对于Fastcgi是PHP的CGI包装。&lt;/p&gt;

&lt;p&gt;WSGI将 web 组件分为三类： web服务器，web中间件,web应用程序， wsgi基本处理模式为 ： WSGI Server -&amp;gt; (WSGI Middleware)* -&amp;gt; WSGI Application 。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/wsgi.jpg&quot; alt=&quot;wsgi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;WSGI Server/gateway&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;wsgi server可以理解为一个符合wsgi规范的web server，接收request请求，封装一系列环境变量，按照wsgi规范调用注册的wsgi app，最后将response返回给客户端。文字很难解释清楚wsgi server到底是什么东西，以及做些什么事情，最直观的方式还是看wsgi server的实现代码。以python自带的wsgiref为例，wsgiref是按照wsgi规范实现的一个简单wsgi server。&lt;/p&gt;

&lt;p&gt;Python 2.5和更高版本带有一个WSGI服务器: wsgiref。&lt;/p&gt;

&lt;p&gt;大致流程如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;服务器创建socket，监听端口，等待客户端连接。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;当有请求来时，服务器解析客户端信息放到环境变量environ中，并调用绑定的handler来处理请求。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;handler解析这个http请求，将请求信息例如method，path等放到environ中。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;wsgi handler再将一些服务器端信息也放到environ中，最后服务器信息，客户端信息，本次请求信息全部都保存到了环境变量environ中。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;wsgi handler 调用注册的wsgi app，并将environ和回调函数传给wsgi app&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;wsgi app 将reponse header/status/body 回传给wsgi handler&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;最终handler还是通过socket将response信息塞回给客户端。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Application Interface&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;WSGI应用程序接口被实现为callable对象：一个函数、一个方法、一个类或者一个实现了object.&lt;strong&gt;call&lt;/strong&gt;()方法的实例，这个callable对象必须能够：&lt;/p&gt;

&lt;p&gt;1.接受两个参数&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;一个包含类CGI变量的字典 environ&lt;/li&gt;
  &lt;li&gt;一个被应用用来发送HTTP状态码/信息和HTTP头部信息给服务器的回调函数 start_response&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2.将响应内容以字符串迭代器的形式发给服务器&lt;/p&gt;

&lt;p&gt;应用程序的框架如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# The application interface is a callable object

# It accepts two arguments:
# environ points to a dictionary containing CGI like environment
# variables which is populated by the server for each
# received request from the client；
# start_response is a callback function supplied by the server
# which takes the HTTP status and headers as arguments
def application ( environ, start_response ):
    # Build the response body possibly
    # using the supplied environ dictionary
    response_body = 'Request method: %s' % environ['REQUEST_METHOD']

    # HTTP response code and message
    status = '200 OK'

    # HTTP headers expected by the client
    # They must be wrapped as a list of tupled pairs:
    # [(Header name, Header value)].
    response_headers = [
        ('Content-Type', 'text/plain'),
        ('Content-Length', str(len(response_body)))
    ]
  
    # Send them to the server using the supplied function
    start_response(status, response_headers)

    # Return the response body. Notice it is wrapped
    # in a list although it could be any iterable.
    return [response_body]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Enviroment Dict&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;服务器将来自客户端请求中的类似CGI的变量填充到环境变量字典中，下面脚本将输出整个字典：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#! /usr/bin/env python

# Python's bundled WSGI server
from wsgiref.simple_server import make_server

def application (environ, start_response):

    # Sorting and stringifying the environment key, value pairs
    response_body = [
        '%s: %s' % (key, value) for key, value in sorted(environ.items())
    ]
    response_body = '\n'.join(response_body)

    status = '200 OK'
    response_headers = [
        ('Content-Type', 'text/plain'),
        ('Content-Length', str(len(response_body)))
    ]
    start_response(status, response_headers)

    return [response_body]

# Instantiate the server
httpd = make_server (
    'localhost', # The host name
    8051, # A port number where to wait for the request
    application # The application object name, in this case a function
)

# Wait for a single request, serve it and quit
httpd.handle_request()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将上面程序片段保存为environment.py，在终端用运行：python environment.py，服务器将监听8051端口，打开浏览器输入：http://localhost:8051&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Apple_PubSub_Socket_Render: /private/tmp/com.apple.launchd.omVjTfSPWb/Render
COLORFGBG: 7;0
CONTENT_LENGTH: 
CONTENT_TYPE: text/plain
GATEWAY_INTERFACE: CGI/1.1
HOME: /Users/yuzujin
HTTP_ACCEPT: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8
HTTP_ACCEPT_ENCODING: gzip, deflate, sdch, br
HTTP_ACCEPT_LANGUAGE: zh-CN,zh;q=0.8,en;q=0.6,pt;q=0.4,th;q=0.2,ar;q=0.2,ja;q=0.2
HTTP_CONNECTION: keep-alive
HTTP_HOST: localhost:8051
HTTP_UPGRADE_INSECURE_REQUESTS: 1
HTTP_USER_AGENT: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36
ITERM_PROFILE: Default
ITERM_SESSION_ID: w0t4p0:B9A76F07-EEF7-495A-8CC6-ECAA412ADBB0
LANG: zh_CN.UTF-8
LC_CTYPE: zh_CN.UTF-8
LESS: -R
LOGNAME: yuzujin
LSCOLORS: Gxfxcxdxbxegedabagaacd
OLDPWD: /Users/yuzujin/app/test/static/font
PAGER: less
PATH: /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin
PATH_INFO: /
PWD: /Users/yuzujin/app
QUERY_STRING: 
REMOTE_ADDR: 127.0.0.1
REMOTE_HOST: 1.0.0.127.in-addr.arpa
REQUEST_METHOD: GET
SCRIPT_NAME: 
SERVER_NAME: 1.0.0.127.in-addr.arpa
SERVER_PORT: 8051
SERVER_PROTOCOL: HTTP/1.1
SERVER_SOFTWARE: WSGIServer/0.1 Python/2.7.6
SHELL: /bin/zsh
SHLVL: 1
SSH_AUTH_SOCK: /private/tmp/com.apple.launchd.itL8Kal1Y1/Listeners
TERM: xterm-256color
TERM_PROGRAM: iTerm.app
TERM_PROGRAM_VERSION: 3.0.12
TERM_SESSION_ID: w0t4p0:B9A76F07-EEF7-495A-8CC6-ECAA412ADBB0
TMPDIR: /var/folders/2k/yqr8f64n66j1q_ydgf6phb9h0000gn/T/
USER: yuzujin
VERSIONER_PYTHON_PREFER_32_BIT: no
VERSIONER_PYTHON_VERSION: 2.7
XPC_FLAGS: 0x0
XPC_SERVICE_NAME: 0
ZSH: /Users/yuzujin/.oh-my-zsh
_: /usr/bin/python
__CF_USER_TEXT_ENCODING: 0x1F5:0x19:0x34
wsgi.errors: &amp;lt;open file '&amp;lt;stderr&amp;gt;', mode 'w' at 0x106c721e0&amp;gt;
wsgi.file_wrapper: wsgiref.util.FileWrapper
wsgi.input: &amp;lt;socket._fileobject object at 0x106d7a450&amp;gt;
wsgi.multiprocess: False
wsgi.multithread: True
wsgi.run_once: False
wsgi.url_scheme: http
wsgi.version: (1, 0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Response Iterable&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;WSGI 应用的返回是return [response_body]，如果换成return response_body，会发生什么呢？
服务器将生成一个字节迭代器，一次向客户端只发送一个字节，这样速度很慢，而return [response_body]是生成一个字符串迭代器，效率高很多。&lt;/p&gt;

&lt;p&gt;如果迭代器yield多个字符串，则响应主体的长度将是所有字符串长度之和，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#! /usr/bin/env python

from wsgiref.simple_server import make_server

def application(environ, start_response):

    response_body = [
        '%s: %s' % (key, value) for key, value in sorted(environ.items())
    ]
    response_body = '\n'.join(response_body)

    # Adding strings to the response body
    response_body = [
        'The Beggining\n',
        '*' * 30 + '\n',
        response_body,
        '\n' + '*' * 30 ,
        '\nThe End'
    ]

    # So the content-lenght is the sum of all string's lengths
    content_length = sum([len(s) for s in response_body])

    status = '200 OK'
    response_headers = [
        ('Content-Type', 'text/plain'),
        ('Content-Length', str(content_length))
    ]

    start_response(status, response_headers)
    return response_body

httpd = make_server('localhost', 8051, application)
httpd.handle_request()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;WSGI请求解析&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;1.GET方式&lt;/p&gt;

&lt;p&gt;GET方式下，query string就是?之后的所有内容。&lt;/p&gt;

&lt;p&gt;例如请求为：http://localhost:8051/?age=10&amp;amp;hobbies=software&amp;amp;hobbies=tunning&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python

from wsgiref.simple_server import make_server
from cgi import parse_qs, escape

html = &quot;age: %(age)s software: %(checked-software)s tunning: %(checked-software)s hobbies: %(hobbies)s&quot;
          
def application (environ, start_response):

    # Returns a dictionary in which the values are lists
    d = parse_qs(environ['QUERY_STRING'])

    # As there can be more than one value for a variable then
    # a list is provided as a default value.
    age = d.get('age', [''])[0] # Returns the first age value
    hobbies = d.get('hobbies', []) # Returns a list of hobbies

    # Always escape user input to avoid script injection
    age = escape(age)
    hobbies = [escape(hobby) for hobby in hobbies]

    response_body = html % { # Fill the above html template in
        'checked-software': ('', 'checked')['software' in hobbies],
        'checked-tunning': ('', 'checked')['tunning' in hobbies],
        'age': age or 'Empty',
        'hobbies': ', '.join(hobbies or ['No Hobbies?'])
    }

    status = '200 OK'

    # Now content type is text/html
    response_headers = [
        ('Content-Type', 'text/html'),
        ('Content-Length', str(len(response_body)))
    ]

    start_response(status, response_headers)
    return [response_body]

httpd = make_server('localhost', 8051, application)

# Now it is serve_forever() in instead of handle_request()
httpd.serve_forever()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.POST方式&lt;/p&gt;

&lt;p&gt;POST方式下，query string不是在url中传递的，而是在HTTP请求体中，WSGI server提供了wsgi.input文件获取环境变量。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python

from wsgiref.simple_server import make_server
from cgi import parse_qs, escape

html = &quot;age: %(age)s software: %(checked-software)s tunning: %(checked-software)s hobbies: %(hobbies)s&quot;
    
def application(environ, start_response):

    # the environment variable CONTENT_LENGTH may be empty or missing
    try:
        request_body_size = int(environ.get('CONTENT_LENGTH', 0))
    except (ValueError):
        request_body_size = 0

    # When the method is POST the variable will be sent
    # in the HTTP request body which is passed by the WSGI server
    # in the file like wsgi.input environment variable.
    request_body = environ['wsgi.input'].read(request_body_size)
    d = parse_qs(request_body)

    age = d.get('age', [''])[0] # Returns the first age value.
    hobbies = d.get('hobbies', []) # Returns a list of hobbies.

    # Always escape user input to avoid script injection
    age = escape(age)
    hobbies = [escape(hobby) for hobby in hobbies]

    response_body = html % { # Fill the above html template in
        'checked-software': ('', 'checked')['software' in hobbies],
        'checked-tunning': ('', 'checked')['tunning' in hobbies],
        'age': age or 'Empty',
        'hobbies': ', '.join(hobbies or ['No Hobbies?'])
    }

    status = '200 OK'

    response_headers = [
        ('Content-Type', 'text/html'),
        ('Content-Length', str(len(response_body)))
    ]

    start_response(status, response_headers)
    return [response_body]

httpd = make_server('localhost', 8051, application)
httpd.serve_forever()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;WSGI 应用框架&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;appier&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Appier is an object-oriented Python web framework built for super fast app development. It’s as lightweight as possible, but not too lightweight. It gives you the power of bigger frameworks, without their complexity.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;bobo&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bobo is a light-weight framework. Its goal is to be easy to use and remember.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bottle&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bottle is a fast and simple micro-framework for small web-applications. It offers request dispatching (Routes) with url parameter support, Templates, key/value Databases, a build-in HTTP Server and adapters for many third party WSGI/HTTP-server and template engines. All in a single file and with no dependencies other than the Python Standard Library.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CherryPy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CherryPy is a pythonic, object-oriented web development framework. Includes support for WSGI servers. CherryPy 3 includes better support for living alongside other WSGI frameworks, applications, and middleware.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Django&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Includes support for WSGI servers&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Falcon&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Falcon is a high-performance Python framework for building cloud APIs. It encourages the REST architectural style, and tries to do as little as possible while remaining highly effective.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Flask&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Flask is a microframework for Python based on Werkzeug, Jinja 2 and good intentions.&lt;/p&gt;

&lt;p&gt;It inherits its high WSGI usage and compliance from Werkzeug.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;notmm&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The notmm toolkit is a fork of Django that doesn’t get in your way. Features includes improved WSGI support (Paste), SQLAlchemy, and very few developers! ;-)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;PoorWSGI&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Poor WSGI for Python is light WGI connector with uri routing between WSGI server and your application. It have mod_python compatible request object, which is post to all uri or http state handler.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pycnic&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Pycnic is a mimimalist JSON API oriented framework for Python 2.7 and 3.x. It provides routing, cookies, and JSON error handling, while maintaining a small codebase.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pyramid&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Merger of the Pylons and repoze.bfg projects, Pyramid is a minimalist web framework aiming at composability and making developers paying only for what they use.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;QWeb&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Another WSGI framework (not sure what the distinguishing features are)
repoze.zope2
A module that implements an analogue of the Zope 2 ZPublisher, with some major simplifications and cleanups. Its core mission is to allow publishing existing Zope2 applications in a WSGI environment that externalizes some of the features of “classic” Zope2 into middleware.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TurboGears&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Database-driven app in minutes; inherits its WSGI support from CherryPy.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;web.py&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Makes web apps. A small RESTful library.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;web2py&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A full stack framework includes its own Database Abstraction Layer (with support for SQLite, MySQL, PostgreSQL, MSSQL, DB2, Informix, Oracle, FireBase, Ingres and Google App Engine), its own template laguage, and a web based IDE. web2py itself is a WSGI app. Not related to web.py.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;WebCore&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A nanoframework (only a few hundred lines of code) offering an entry_points-based dependency graphing extension system, MVC separation, reusable namespaces, and universal URL dispatch protocol with tight WebOb integration and natural Python semantics.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;weblayer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;weblayer is a lightweight, componentised package for writing WSGI applications.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Zope 3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The venerable Python web framework, recreated anew in Zope 3, and now a WSGI application. It seems to have some WSGI bits deep inside the publisher, but they aren’t really documented at this time.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;WSGI 服务器&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;ajp-wsgi&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A threaded/forking WSGI server implemented in C (it embeds a Python interpreter to run the actual application). It communicates with the web server via AJP, and is known to work with mod_jk and mod_proxy_ajp. Also available in an SCGI flavor.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Aspen&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A pure-Python web server (using the CherryPy module mentioned next) with three hooks to hang your WSGI on.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;cherrypy.wsgiserver&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CherryPy’s “high-speed, production ready, thread pooled, generic WSGI server.” Includes SSL support. Supports Transfer-Encoding: chunked. For details on running foreign (non-CherryPy) applications under the CherryPy WSGI server, see WSGI Support. See also the CherryPy wiki ModWSGI page.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;chiral.web.httpd&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A fast HTTP server supporting WSGI, with extensions for Coroutine-based pages with deeply-integrated COMET support.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;cogen.web.wsgi&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;WSGI server with extensions for coroutine oriented programming.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;FAPWS&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fapws is a WSGI binding between Python and libev.&lt;/p&gt;

&lt;p&gt;See also: author’s block, GoogleGroup.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;fcgiapp&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;fcgiapp is a Python wrapper for the C FastCGI SDK. It’s used by PEAK’s FastCGI servers to provide WSGI-over-FastCGI.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;flup&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Includes threaded and forking versions of servers that support FastCGI, SCGI, and AJP protocols.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;gevent-fastcgi&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;WSGI-over-FastCGI server implemented using gevent coroutine-based networking library. Supports FastCGI connection multiplexing. Includes adapters for Django and other frameworks that use PasteDeploy.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Gunicorn&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;WSGI HTTP Server for UNIX, fast clients and nothing else. This is a port of Unicorn to Python and WSGI.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ISAPI-WSGI&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An implementation of WSGI for running as a ISAPI extension under IIS.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;James&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;James provides a very simple multi-threaded WSGI server implementation based on the HTTPServer from Python’s standard library. (unmaintained)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Julep&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A WSGI Server inspired by Unicorn, written in pure Python.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;m2twisted&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;WSGI server built with M2Crypto and twisted.web2 with some SSL related tricks. Used with client side smart cards and it is also possible to run the HTTPS server with a key in a HSM (like a crypto token)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;modjy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Modjy is a java servlets to WSGI gateway that enables the running of jython WSGI applications inside java servlet containers.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;mod_wsgi&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Python WSGI adapter module for Apache&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;NWSGI&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;NWSGI is a .NET implementation of the Python WSGI specification for IronPython and IIS. This makes it easy to run Python web applications on Windows Server. This is a potential alternative to ISAPI + ISAPI_WSGI modules.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;netius&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Netius is a Python network library that can be used for the rapid creation of asynchronous non-blocking servers and clients. It has no dependencies, it’s cross-platform, and brings some sample netius-powered servers out of the box, namely a production-ready WSGI server.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;paste.httpserver&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Minimalistic threaded WSGI server built on BaseHTTPServer. Doesn’t support Transfer-Encoding: chunked.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;phusion passenger&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;“proof of concept” WSGI since 2008 (1.x), support upgraded to “beta” in version 3 (with limitations e.g. requires Ruby even when unused) and first-class in Passenger 4.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;python-fastcgi&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;python-fastcgi is a lightweight wrapper around the Open Market FastCGI C Library/SDK. It includes threaded and forking WSGI server implementations.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Spawning&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;twisted.web&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A WSGI server based on Twisted Web’s HTTP server (requires Twisted 8.2 or later).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;uWSGI&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fast, self-healing, developer-friendly WSGI server, meant for professional deployment and development of Python Web applications.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;werkzeug.serving&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Werkzeug’s multithreaded and multiprocessed development server. Wraps wsgiref to add a reloader, multiprocessing, static files handling and SSL.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;wsgid&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Wsgid is a generic WSGI handler for mongrel2 webserver. Wsgid offers a complete daemon environment (start/stop/restart) to your app workers, including automatically re-spawning of processes.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;WSGIserver&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;WSGIserver is a high-speed, production ready, thread pooled, generic WSGI server with SSL support for both Python 2 (2.6 and above) and Python 3 (3.1 and above). WSGIserver is a one file project with no dependency.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;WSGIUtils&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Includes a threaded HTTP server.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;wsgiref (Python 3)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Included as part of thef standard library since Python 2.5; it includes a threaded HTTP server, a CGI server (for running any WSGI application as a CGI script), and a framework for building other servers.&lt;/p&gt;

&lt;p&gt;For versions prior to Python 2.5, see wsgiref’s original home.&lt;/p&gt;

</description>
                <link>http://username.github.io/2017/01/10/wsgi</link>
                <guid>http://username.github.io/2017/01/10/wsgi</guid>
                <pubDate>Tue, 10 Jan 2017 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>CGI的前世今生</title>
                <description>
&lt;p&gt;&lt;strong&gt;CGI&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;CGI (Common Gateway Interface) 是外部应用程序（CGI程序）与 WEB 服务器之间的接口标准（协议），是在CGI程序和Web服务器之间传递信息的过程。CGI规范允许Web服务器执行外部程序，并将它们的输出发送给Web浏览器，CGI 将 Web 的一组简单的静态超媒体文档变成一个完整的新的交互式媒体。&lt;/p&gt;

&lt;p&gt;最初，CGI 是在 1993 年由美国国家超级电脑应用中心（NCSA）为 NCSA HTTPd Web 服务器开发的。&lt;/p&gt;

&lt;p&gt;这个 Web 服务器使用了 UNIX shell 环境变量来保存从 Web 服务器传递出去的参数，然后生成一个运行 CGI 的独立进程。CGI的第一个实现是 Perl 写的。&lt;/p&gt;

&lt;p&gt;对一个 CGI 程序，做的工作其实只有：从环境变量(environment variables)和标准输入(standard input)中读取数据、处理数据、向标准输出(standard output)输出数据。&lt;/p&gt;

&lt;p&gt;环境变量中存储的叫 Request Meta-Variables，也就是诸如 QUERY_STRING、PATH_INFO 之类的东西，这些是由 Web Server 通过环境变量传递给 CGI 程序的，CGI 程序也是从环境变量中读取的。标准输入中存放的往往是用户通过 PUTS 或者 POST 提交的数据，这些数据也是由 Web Server 传过来的。&lt;/p&gt;

&lt;p&gt;环境变量列表：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;变量&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;解释&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;SERVER_NAME&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;运行CGI序为机器名或IP地址。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;SERVER_INTERFACE&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;WWW服务器的类型，如：CERN型或NCSA型。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;SERVER_PROTOCOL&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;通信协议，应当是HTTP/1.0。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;SERVER_PORT&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TCP端口，一般说来web端口是80。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;HTTP_ACCEPT&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;HTTP定义的浏览器能够接受的数据类型。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;HTTP_REFERER&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;发送表单的文件URL。（并非所有的浏览器都传送这一变量）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;HTTP_USER-AGENT&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;发送表单的浏览的有关信息。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;GETWAY_INTERFACE&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CGI程序的版本，在UNIX下为 CGI/1.1。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;PATH_TRANSLATED&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;PATH_INFO中包含的实际路径名。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;PATH_INFO&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;浏览器用GET方式发送数据时的附加路径。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;SCRIPT_NAME&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CGI程序的路径名。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;QUERY_STRING&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;表单输入的数据，URL中问号后的内容。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;REMOTE_HOST&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;发送程序的主机名，不能确定该值。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;REMOTE_ADDR&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;发送程序的机器的IP地址。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;REMOTE_USER&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;发送程序的人名。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CONTENT_TYPE&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;POST发送，一般为application/xwww-form-urlencoded。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CONTENT_LENGTH&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;POST方法输入的数据的字节数。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;CGI 的缺点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;效率低下：每一个连接 fork 一个进程处理。&lt;/li&gt;
  &lt;li&gt;功能十分有限：CGI只能收到一个请求，输出一个响应。很难在 CGI 体系去对Web请求的控制，例如：用户认证等。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;FastCGI&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;FastCGI (Fast Common Gateway Interface) 是一种让交互程序与 Web Server 通信的协议。FastCGI 是 CGI 的增强版本。 FastCGI 致力于减少 WebServer 与 CGI 程序之间互动的开销，从而使 Web Server 可以同时处理更多的请求。&lt;/p&gt;

&lt;p&gt;不同于 CGI 为每一个请求创建一个进程，FastCGI 维护一个进程池用来处理持续不断的请求，这些进程是由FastCGI 管理，而不是Web Server。Fastcgi 先fork一个master，解析配置文件，初始化执行环境，然后再fork多个worker。当请求过来时，master会传递给一个worker，然后立即可以接受下一个请求。这样就避免了重复的劳动，效率自然是高。而且当worker不够用时，master可以根据配置预先启动几个worker等着；当然空闲worker太多时，也会停掉一些，这样就提高了性能，也节约了资源。这就是Fastcgi的对进程的管理。&lt;/p&gt;

&lt;p&gt;Web Server 是通过 Unix domain socket、命名管道(Name pipe)或者 TCP 链接来向一个FastCGI 进程发送环境变量信息和请求本身的。请求的响应内容以同样的方式返回给 Web Server，Web Server 随后将响应发给客户端。这个链接在响应完成后可能会被关闭，但是web server 和 FastCGI 进程都会被保留。&lt;/p&gt;

&lt;p&gt;使用FastCGI 的 Web Server&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Apache HTTP Server (部分)&lt;/p&gt;

    &lt;p&gt;通过mod_fcgid模块实现，这个模块曾属于第三方，后来成为Apache的一个子项目，这个模块仅支持Unix 套接字，不支持TCP套接字。&lt;/p&gt;

    &lt;p&gt;一个较早的第三方模块mod_fcgi也还在用。
从Apache 2.4开始，mod_proxy_fcgi被加进来，为了支持TCP FastCGI servers。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Cherokee HTTP Server&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hiawatha Webserver（英语：Hiawatha_(web_server)）&lt;/p&gt;

    &lt;p&gt;支持FastCGI的負載平衡&lt;/p&gt;

    &lt;p&gt;支持chrooted FastCGI 伺服器&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Lighttpd&lt;/li&gt;
  &lt;li&gt;Nginx&lt;/li&gt;
  &lt;li&gt;LiteSpeed Web Server&lt;/li&gt;
  &lt;li&gt;Microsoft IIS&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;PHP-CGI&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;php-cgi 是一种CGI 协议的实现。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;php-cgi 其实就是PHP 解析器。&lt;/li&gt;
  &lt;li&gt;在CGI 模式时，当Web Server 收到 xx/index.php 请求时，会启动php-cgi，php-cgi 会解析php.ini 文件，初始化环境，然后根据请求参数进行处理，再返回处理后的结果。(都是以CGI 协议规范来进行)&lt;/li&gt;
  &lt;li&gt;php-cgi 在每个请求时都会启动一个进程，然后读取php.ini 进行解析，可想而知效率相对比较低。&lt;/li&gt;
  &lt;li&gt;php-cgi 无法实现平滑重启。修改php.ini 配置后，后面启动的php-cgi 程序还是不会感知。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;PHP-FPM&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;FastCGI Process Management，是一种FastCGI 协议的实现。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;当请求到来时，php-fpm 启动并读取php.ini 文件完成初始化环境，然后启动一个master，再启动多个worker。当请求过来时，master 会传递给一个worker，然后等待下一个请求。php-fpm 会动态配置worker 的数量。&lt;/li&gt;
  &lt;li&gt;一个php-fpm 进程可以处理多个请求，会启动多个php-cgi 程序。&lt;/li&gt;
  &lt;li&gt;php-fpm 可以实现平衡重启。修改php.ini 后，当启用新的worker 会使用新的配置。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;spawn-fcgi&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Spawn-FCGI是一个通用的FastCGI管理服务器，它是lighttpd中的一部份，很多人都用Lighttpd的Spawn-FCGI进行 FastCGI模式下的管理工作，不过有不少缺点。而PHP-FPM的出现多少缓解了一些问题，但PHP-FPM有个缺点就是要重新编译，这对于一些已经运行的环境可能有不小的风险(refer)，在php5.3.3中可以直接使用PHP-FPM了。&lt;/p&gt;

&lt;p&gt;Spawn-FCGI目前已经独成为一个项目，更加稳定一些，也给很多Web站点的配置带来便利。已经有不少站点将它与nginx搭配来解决动态网页。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;spawn-fcgi -a 127.0.0.1 -p9000 -C5 -u www-data -g www-data -f /usr/bin/php-CGI
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数含义如下:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;-f 指定调用FastCGI的进程的执行程序位置，根据系统上所装的PHP的情况具体设置&lt;/li&gt;
  &lt;li&gt;-a 绑定到地址addr&lt;/li&gt;
  &lt;li&gt;-p 绑定到端口port&lt;/li&gt;
  &lt;li&gt;-s 绑定到unix socket的路径path&lt;/li&gt;
  &lt;li&gt;-C 指定产生的FastCGI的进程数，默认为5(仅用于PHP)&lt;/li&gt;
  &lt;li&gt;-P 指定产生的进程的PID文件路径&lt;/li&gt;
  &lt;li&gt;-u和-g FastCGI使用什么身份(-u 用户 -g用户组)运行，Ubuntu下可以使用www-data，其他的根据情况配置，如nobody、apache等&lt;/li&gt;
&lt;/ul&gt;

</description>
                <link>http://username.github.io/2017/01/08/cgi</link>
                <guid>http://username.github.io/2017/01/08/cgi</guid>
                <pubDate>Sun, 08 Jan 2017 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>MongoDB集群分片</title>
                <description>
&lt;p&gt;MongoDB副本集有两个问题没有解决：从节点每个上面的数据都是对数据库全量拷贝，从节点压力会不会过大；数据压力大到机器支撑不了的时候不能做到自动扩展。&lt;/p&gt;

&lt;p&gt;处理海量数据时，传统数据库是在数据库层之上增加一个数据访问层组件，它主要的作用是SQL解析、路由处理。根据应用的请求的功能解析当前访问的sql判断是在哪个业务数据库、哪个表访问查询并返回数据结果。数据库的增加、删除、备份需要程序去控制。&lt;/p&gt;

&lt;p&gt;MongoDB所有的这一切通过他自己的内部机制就可以搞定：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yuzujin/yuzujin.github.com/blob/master/images/mongo15.jpg?raw=true&quot; alt=&quot;mongo15&quot; title=&quot;Title&amp;quot; &amp;quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从图中可以看到有四个组件：mongos、config server、shard、replica set。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;mongos&lt;/code&gt;，数据库集群请求的入口，所有的请求都通过mongos进行协调，不需要在应用程序添加一个路由选择器，mongos自己就是一个请求分发中心，它负责把对应的数据请求请求转发到对应的shard服务器上。在生产环境通常有多mongos作为请求的入口，防止其中一个挂掉所有的mongodb请求都没有办法操作。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;config server&lt;/code&gt;，顾名思义为配置服务器，存储所有数据库元信息（路由、分片）的配置。mongos本身没有物理存储分片服务器和数据路由信息，只是缓存在内存里，配置服务器则实际存储这些数据。mongos第一次启动或者关掉重启就会从 config server 加载配置信息，以后如果配置服务器信息变化会通知到所有的 mongos 更新自己的状态，这样 mongos 就能继续准确路由。在生产环境通常有多个 config server 配置服务器，因为它存储了分片路由的元数据，这个可不能丢失！就算挂掉其中一台，只要还有存货， mongodb集群就不会挂掉。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;shard&lt;/code&gt;，这就是传说中的分片了。上面提到一个机器就算能力再大也有天花板，一台普通的机器做不了的多台机器来做，一台机器的一个数据表 Collection1 存储了 1T 数据，压力太大了！在分给4个机器后，每个机器都是256G，则分摊了集中在一台机器的压力。也许有人问一台机器硬盘加大一点不就可以了，为什么要分给四台机器呢？不要光想到存储空间，实际运行的数据库还有硬盘的读写、网络的IO、CPU和内存的瓶颈。在mongodb集群只要设置好了分片规则，通过mongos操作数据库就能自动把对应的数据操作请求转发到对应的分片机器上。在生产环境中分片的片键可要好好设置，这个影响到了怎么把数据均匀分到多个分片机器上，不要出现其中一台机器分了1T，其他机器没有分到的情况，如下图：&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yuzujin/yuzujin.github.com/blob/master/images/mongo16.jpg?raw=true&quot; alt=&quot;mongo16&quot; title=&quot;Title&amp;quot; &amp;quot;Title&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;replica set&lt;/code&gt;，其实上图4个分片如果没有 replica set 是个不完整架构，假设其中的一个分片挂掉那四分之一的数据就丢失了，所以在高可用性的分片架构还需要对于每一个分片构建 replica set 副本集保证分片的可靠性。生产环境通常是 2个副本 + 1个仲裁。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;如何搭建高可用的mongodb集群&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;首先确定各个组件的数量，mongos 3个， config server 3个，数据分3片 shard server 3个，每个shard 有一个副本一个仲裁也就是 3 * 2 = 6 个，总共需要部署15个实例。这些实例可以部署在独立机器也可以部署在一台机器，我们这里测试资源有限，只准备了3台机器，在同一台机器只要端口不同就可以，看一下物理部署图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yuzujin/yuzujin.github.com/blob/master/images/mongo17.jpg?raw=true&quot; alt=&quot;mongo17&quot; title=&quot;Title&amp;quot; &amp;quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;1.准备机器，IP分别设置为： 192.168.0.1、192.168.0.2、192.168.0.3。&lt;/p&gt;

&lt;p&gt;2.分别在每台机器建立mongos 、config 、 shard1 、shard2、shard3 五个目录。 因为mongos不存储数据，只需要建立日志文件目录即可。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 建立mongos目录
$ mkdir -p ~/mongodbtest/mongos/log

# 建立config server数据文件存放目录
$ mkdir -p ~/mongodbtest/config/data

# 建立config server日志文件存放目录
$ mkdir -p ~/mongodbtest/config/log

# 建立shard1数据文件存放目录
$ mkdir -p ~/mongodbtest/shard1/data

# 建立shard1日志文件存放目录
$ mkdir -p ~/mongodbtest/shard1/log

# 建立shard2数据文件存放目录
$ mkdir -p ~/mongodbtest/shard2/data

# 建立shard2日志文件存放目录
$ mkdir -p ~/mongodbtest/shard2/log

# 建立shard3数据文件存放目录
$ mkdir -p ~/mongodbtest/shard3/data

# 建立shard3日志文件存放目录
$ mkdir -p ~/mongodbtest/shard3/log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.规划5个组件对应的端口号，由于一个机器需要同时部署 mongos、config server、shard1、shard2、shard3，所以需要用端口进行区分。这个端口可以自由定义，在本文 mongos为 20000，config server 为 21000， shard1为 22001，shard2为22002，shard3为22003.&lt;/p&gt;

&lt;p&gt;4.在每一台服务器分别启动配置服务器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./mongod --configsvr --dbpath ~/mongodbtest/config/data --port 21000 --logpath 
~/mongodbtest/config/log/config.log --fork
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5.在每一台服务器分别启动mongos服务器&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./mongos --configdb 192.168.0.1:21000, 192.168.0.2:21000, 192.168.0.3:21000 
--port 20000 --logpath ~/mongodbtest/mongos/log/mongos.log --fork
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6.配置各个分片的副本集&lt;/p&gt;

&lt;p&gt;为了快速启动并节约测试环境存储空间，这里加上 nojournal 是为了关闭日志信息，在我们的测试环境不需要初始化这么大的redo日志。同样设置 oplogsize是为了降低 local 文件的大小，oplog是一个固定长度的 capped collection,它存在于”local”数据库中,用于记录Replica Sets操作日志。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 在每个机器里分别设置分片1服务及副本集shard1
$ ./mongod --shardsvr --replSet shard1 --port 22001 --dbpath ~/mongdbtest/shard1/data 
--logpath ~/mongdbtest/shard1/log/shard1.log --fork --nojournal --oplogSize 10

# 在每个机器里分别设置分片2服务及副本集shard2
$ ./mongod --shardsvr --replSet shard2 --port 22002 --dbpath ~/mongdbtest/shard2/data 
--logpath ~/mongdbtest/shard2/log/shard2.log --fork --nojournal --oplogSize 10

 # 在每个机器里分别设置分片3服务及副本集shard3
$ ./mongod --shardsvr --replSet shard3 --port 22003 --dbpath ~/mongdbtest/shard3/data 
--logpath ~/mongdbtest/shard3/log/shard3.log --fork --nojournal --oplogSize 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分别对每个分片配置副本集，任意登陆一个机器，比如登陆192.168.0.1，连接MongoDB&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 设置第一个分片副本集
$ ./mongo 192.168.0.1:22001
&amp;gt; use admin;
&amp;gt; config={_id:&quot;shard1&quot;, members:[{_id:0, host:&quot;192.168.0.1:22001&quot;},
{_id:1, host:&quot;192.168.0.2:22001&quot;},
{_id:2, host:&quot;192.168.0.3:22001&quot;,arbiterOnly:true}]}
&amp;gt; rs.initiate(config);

# 设置第二个分片副本集
$ ./mongo 192.168.0.1:22002
&amp;gt; use admin;
&amp;gt; config={_id:&quot;shard2&quot;, members:[{_id:0, host:&quot;192.168.0.1:22002&quot;},
{_id:1, host:&quot;192.168.0.2:22002&quot;},
{_id:2, host:&quot;192.168.0.3:22002&quot;,arbiterOnly:true}]}
&amp;gt; rs.initiate(config);

# 设置第三个分片副本集
$ ./mongo 192.168.0.1:22003
&amp;gt; use admin;
&amp;gt; config={_id:&quot;shard3&quot;, members:[{_id:0, host:&quot;192.168.0.1:22003&quot;},
{_id:1, host:&quot;192.168.0.2:22003&quot;},
{_id:2, host:&quot;192.168.0.3:22003&quot;,arbiterOnly:true}]}
&amp;gt; rs.initiate(config);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;7.目前搭建了mongodb配置服务器、路由服务器，各个分片服务器，不过应用程序连接到 mongos 路由服务器并不能使用分片机制，还需要在程序里设置分片配置，让分片生效。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./mongo 192.168.0.1:20000
&amp;gt; use admin;
# 串联路由服务器和分片副本集1
&amp;gt; db.runCommand({addshard:&quot;shard1/192.168.0.1:22001,192.168.0.2:22001,
192.168.0.3:22001&quot;});
# 串联路由服务器和分片副本集2
&amp;gt; db.runCommand({addshard:&quot;shard2/192.168.0.1:22002,192.168.0.2:22002,
192.168.0.3:22002&quot;});
# 串联路由服务器和分片副本集3
&amp;gt; db.runCommand({addshard:&quot;shard3/192.168.0.1:22003,192.168.0.2:22003,
192.168.0.3:22003&quot;}); 

# 查看分片服务器的配置
&amp;gt; db.runCommand({listshards: 1});
   
{
    &quot;shards&quot; : [
        {
            &quot;_id&quot;: &quot;shard1&quot;,
            &quot;host&quot;: &quot;shard1/192.168.0.1:22001,192.168.0.2:22001,192.168.0.3:22001&quot;
        },
        {
            &quot;_id&quot;: &quot;shard2&quot;,
            &quot;host&quot;: &quot;shard2/192.168.0.1:22002,192.168.0.2:22002,192.168.0.3:22002&quot;
        },
        {
            &quot;_id&quot;: &quot;shard3&quot;,
            &quot;host&quot;: &quot;shard3/192.168.0.1:22003,192.168.0.2:22003,192.168.0.3:22003&quot;
        }
    ],
    &quot;ok&quot;:1
}
因为192.168.0.3是每个分片副本集的仲裁节点，所以在上面结果没有列出来。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;8.连接在mongos上，准备让指定的数据库、指定的集合分片生效。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 指定testdb分片生效

&amp;gt; db.runCommand({enablesharding:&quot;testdb&quot;})

# 指定数据里需要分片的集合和片键

&amp;gt; db.runCommand({shardcollection:&quot;testdb.table1&quot;, key:{id:1}})
#设置testdb的 table1 表需要分片，根据 id 自动分片到 shard1，shard2，shard3 上面去。
要这样设置是因为不是所有mongodb 的数据库和表都需要分片
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;9.测试分片结果&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 连接mongos服务器
$ ./mongos 192.168.0.1:20000
&amp;gt; use testdb;
&amp;gt; for (var i =1; i&amp;lt;10000; i++) db.table1.save({id:i,&quot;test1&quot;:&quot;testval1&quot;})
# 查看分片情况
&amp;gt; db.table1.stats()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;10.优化&lt;/p&gt;

&lt;p&gt;比如我们把所有的仲裁节点放在一台机器，其余两台机器承担了全部读写操作，但是作为仲裁的192.168.0.3相当空闲。让机器192.168.0.3多分担点责任吧！架构可以这样调整，把机器的负载分的更加均衡一点，每个机器既可以作为主节点、副本节点、仲裁节点，这样压力就会均衡很多了，如图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yuzujin/yuzujin.github.com/blob/master/images/mongo18.jpg?raw=true&quot; alt=&quot;mongo18&quot; title=&quot;Title&amp;quot; &amp;quot;Title&quot; /&gt;&lt;/p&gt;

</description>
                <link>http://username.github.io/2017/01/06/mongodb</link>
                <guid>http://username.github.io/2017/01/06/mongodb</guid>
                <pubDate>Fri, 06 Jan 2017 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>MongoDB副本集</title>
                <description>
&lt;p&gt;mongoDB官方已经不建议使用主从模式了，替代方案是采用副本集的模式。&lt;/p&gt;

&lt;p&gt;What’s Replica Sets(副本集)？&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;打魔兽世界总说打副本，其实这两个概念差不多一个意思。游戏里的副本是指玩家集中在高峰时间去一个场景打怪，会出现玩家暴多怪物少的情况，游戏开发商为了保证玩家的体验度，就为每一批玩家单独开放一个同样的空间同样的数量的怪物，这一个复制的场景就是一个副本，不管有多少个玩家各自在各自的副本里玩不会互相影响。&lt;/p&gt;

&lt;p&gt;mongoDB的副本也是这个，主从模式其实就是一个单副本的应用，没有很好的扩展性和容错性。而副本集具有多个副本保证了容错性，就算一个副本挂掉了还有很多副本存在，并且解决了主从模式的“主节点挂掉了，整个集群内会自动切换”问题。&lt;/p&gt;

&lt;p&gt;MongoDB副本集架构图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yuzujin/yuzujin.github.com/blob/master/images/mongo5.jpg?raw=true&quot; alt=&quot;mongo5&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;由图可以看到客户端连接到整个副本集，不关心具体哪一台机器是否挂掉。主服务器负责整个副本集的读写，副本集定期同步数据备份，一但主节点挂掉，副本节点就会选举一个新的主服务器，这一切对于应用服务器不需要关心。我们看一下主服务器挂掉后的架构：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yuzujin/yuzujin.github.com/blob/master/images/mongo6.jpg?raw=true&quot; alt=&quot;mongo6&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;How to config Replica Sets?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;1.准备一台机器（有条件的话准备3台机器）&lt;/p&gt;

&lt;p&gt;2.创建三个副本集的文件夹&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir -p ~/mongodbtest/replset1
$ mkdir -p ~/mongodbtest/replset1/data
    
$ mkdir -p ~/mongodbtest/replset2
$ mkdir -p ~/mongodbtest/replset2/data
    
$ mkdir -p ~/mongodbtest/replset3
$ mkdir -p ~/mongodbtest/replset3/data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3.开3个终端分别启动mongodb&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./mongod -dbpath ~/mongodbtest/replset1/data --replSet repset -port 27017
	
$ ./mongod -dbpath ~/mongodbtest/replset2/data --replSet repset -port 27018
	
$ ./mongod -dbpath ~/mongodbtest/replset3/data --replSet repset -port 27019
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yuzujin/yuzujin.github.com/blob/master/images/mongo7.png?raw=true&quot; alt=&quot;mongo6&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;4.初始化副本集&lt;/p&gt;

&lt;p&gt;登录任意一个mongodb，这里我们选择27017&lt;/p&gt;

&lt;p&gt;$ ./mongo 127.0.0.1:27017&lt;/p&gt;

&lt;p&gt;定义副本集配置变量，这里的 _id:”repset” 和上面命令参数“ –replSet repset” 要保持一样。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yuzujin/yuzujin.github.com/blob/master/images/mongo8.png?raw=true&quot; alt=&quot;mongo8&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;查看日志，副本集启动成功后&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yuzujin/yuzujin.github.com/blob/master/images/mongo9.png?raw=true&quot; alt=&quot;mongo9&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;查看副本集状态&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yuzujin/yuzujin.github.com/blob/master/images/mongo10.png?raw=true&quot; alt=&quot;mongo10&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;5.测试副本集数据复制功能&lt;/p&gt;

&lt;p&gt;mongodb默认是从主节点读取数据的，副本节点上不允许读，需要设置副本节点可以读(setSlaveOk)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yuzujin/yuzujin.github.com/blob/master/images/mongo11.png?raw=true&quot; alt=&quot;mongo11&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;6.测试副本集故障转移功能&lt;/p&gt;

&lt;p&gt;杀掉27017主节点，查看27018、27019的日志可以看到经过一系列的投票选择操作，27019当选主节点，27018从27019同步数据过来。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yuzujin/yuzujin.github.com/blob/master/images/mongo12.png?raw=true&quot; alt=&quot;mongo12&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;查看整个集群的状态，可以看到27017为状态不可达。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yuzujin/yuzujin.github.com/blob/master/images/mongo13.png?raw=true&quot; alt=&quot;mongo13&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;再启动原来的主节点27017，发现27017变为 SECONDARY，还是27019为主节点 PRIMARY。&lt;/p&gt;

&lt;p&gt;7.副本集读写分离&lt;/p&gt;

&lt;p&gt;主节点的读写压力过大如何解决？常见的解决方案是读写分离，mongodb副本集的读写分离如何做呢？&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/yuzujin/yuzujin.github.com/blob/master/images/mongo14.jpg?raw=true&quot; alt=&quot;mongo14&quot; title=&quot;Title&quot; /&gt;&lt;/p&gt;

&lt;p&gt;常规写操作来说并没有读操作多，所以一台主节点负责写，两台副本节点负责读。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;设置读写分离需要先在副本节点SECONDARY 设置 setSlaveOk。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在程序中设置副本节点负责读操作，如下代码（java）：&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;public class TestMongoDBReplSetReadSplit {
    public static void main (String[] args) {
        try {
            List&amp;lt;ServerAddress&amp;gt; addresses = new ArrayList&amp;lt;ServerAddress&amp;gt;();
            ServerAddress address1 = new ServerAddress(&quot;127.0.0.1&quot;,27017);
            ServerAddress address2 = new ServerAddress(&quot;127.0.0.1&quot;,27018);
            ServerAddress address3 = new ServerAddress(&quot;127.0.0.1&quot;,27019);
            addresses.add(address1);
            addresses.add(address2);
            addresses.add(address3);
            MongoClient client = new MongoClient(addresses);
            DB db = client.getDB(&quot;test&quot;);
            DBCollection coll = db.getCollection(&quot;testdb&quot;);
                 
            BasicDBObject object = new BasicDBObject();
            object.append(&quot;test2&quot;, &quot;testval2&quot;);
                 
            // 读操作从副本节点读取
            ReadPreference preference = ReadPreference.secondary();
            DBObject dbObject = coll.findOne(object, null, preference);
                 
            System.out.println(dbObject);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;读参数除了secondary一共还有五个参数：primary、primaryPreferred、secondary、secondaryPreferred、nearest。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;primary:默认参数，只从主节点上进行读取操作；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;primaryPreferred:大部分从主节点上读取数据,只有主节点不可用时从secondary节点读取数据。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;secondary:只从secondary节点上进行读取操作，存在的问题是secondary节点的数据会比primary节点数据“旧”。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;secondaryPreferred:优先从secondary节点进行读取操作，secondary节点不可用时从主节点读取数据；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;nearest:不管是主节点、secondary节点，从网络延迟最低的节点上读取数据。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
                <link>http://username.github.io/2017/01/05/mongodb</link>
                <guid>http://username.github.io/2017/01/05/mongodb</guid>
                <pubDate>Thu, 05 Jan 2017 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>MongoDB介绍及配置</title>
                <description>
&lt;h2 id=&quot;whats-nosql&quot;&gt;What’s NoSQL?&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;NoSQL，指的是非关系型的数据库。NoSQL有时也称作Not Only SQL的缩写，是对不同于传统的关系型数据库的数据库管理系统的统称。&lt;/p&gt;

&lt;p&gt;NoSQL用于超大规模数据的存储。（例如谷歌或Facebook每天为他们的用户收集万亿比特的数据）。这些类型的数据存储不需要固定的模式，无需多余操作就可以横向扩展。&lt;/p&gt;

&lt;h2 id=&quot;why-nosql-&quot;&gt;Why NoSQL ?&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;今天我们可以通过第三方平台（如：Google,Facebook等）可以很容易的访问和抓取数据。用户的个人信息，社交网络，地理位置，用户生成的数据和用户操作日志已经成倍的增加。我们如果要对这些用户数据进行挖掘，那SQL数据库已经不适合这些应用了, NoSQL数据库的发展也却能很好的处理这些大的数据。&lt;/p&gt;

&lt;h2 id=&quot;rdbms-vs-nosql&quot;&gt;RDBMS vs NoSQL&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;RDBMS&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;高度组织化结构化数据&lt;/li&gt;
  &lt;li&gt;结构化查询语言（SQL） (SQL)&lt;/li&gt;
  &lt;li&gt;数据和关系都存储在单独的表中。&lt;/li&gt;
  &lt;li&gt;数据操纵语言，数据定义语言&lt;/li&gt;
  &lt;li&gt;严格的一致性&lt;/li&gt;
  &lt;li&gt;基础事务&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;NoSQL&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;代表着不仅仅是SQL&lt;/li&gt;
  &lt;li&gt;没有声明性查询语言&lt;/li&gt;
  &lt;li&gt;没有预定义的模式&lt;/li&gt;
  &lt;li&gt;键 - 值对存储，列存储，文档存储，图形数据库&lt;/li&gt;
  &lt;li&gt;最终一致性，而非ACID属性&lt;/li&gt;
  &lt;li&gt;非结构化和不可预知的数据&lt;/li&gt;
  &lt;li&gt;CAP定理&lt;/li&gt;
  &lt;li&gt;高性能，高可用性和可伸缩性&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;nosql-数据库分类&quot;&gt;NoSQL 数据库分类&lt;/h2&gt;

&lt;hr /&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;类型&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;部分代表&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;特点&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;列存储&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Hbase&lt;br /&gt;Cassandra&lt;br /&gt;Hypertable&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;是按列存储数据的。&lt;br /&gt;最大的特点是方便存储结构化和半结构化数据，方便做数据压缩，&lt;br /&gt;对针对某一列或者某几列的查询有非常大的IO优势。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;文档存储&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MongoDB&lt;br /&gt;CouchDB&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;文档存储一般用类似json的格式存储，存储的内容是文档型的。&lt;br /&gt;这样也就有有机会对某些字段建立索引，实现关系数据库的某些功能。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;key-value存储&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Tokyo Cabinet / Tyrant&lt;br /&gt;Berkeley DB&lt;br /&gt;MemcacheDB&lt;br /&gt;Redis&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;可以通过key快速查询到其value。&lt;br /&gt;一般来说，存储不管value的格式，照单全收。（Redis包含了其他功能）&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;图存储&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Neo4J&lt;br /&gt;FlockDB&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;图形关系的最佳存储。使用传统关系数据库来解决的话性能低下，而且设计使用不方便。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;对象存储&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;db4o&lt;br /&gt;Versant&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;通过类似面向对象语言的语法操作数据库，通过对象的方式存取数据。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;xml数据库&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Berkeley DB XML&lt;br /&gt;BaseX&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;高效的存储XML数据，并支持XML的内部查询语法，比如XQuery,Xpath。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;whats-mongodb&quot;&gt;What’s MongoDB&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;MongoDB 是一个基于分布式文件存储的数据库。由 C++ 语言编写。旨在为 WEB 应用提供可扩展的高性能数据存储解决方案。&lt;/p&gt;

&lt;p&gt;MongoDB 是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。支持类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。可以用sql操作MongoDB，从关系型数据库迁移过来，开发人员学习成本会大大减少。如果再对底层的sql API做一层封装，开发基本可以感觉不到mongodb和关系型数据库的区别。&lt;/p&gt;

&lt;h2 id=&quot;how-use-mongodb&quot;&gt;How use MongoDB&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;MongoDb的简单使用有以下两种方式：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;单实例配置&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这种配置只适合简易开发时使用，生产使用不行，因为单节点挂掉整个数据业务全挂，结构如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/mongo1.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下载&amp;amp;启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir -p mongodbtest/single 
$ mkdir -p mongodbtest/single/data
$ cd mongodbtest/single
$ wget http://fastdl.mongodb.org/linux/mongodb-linux-x86_64-ubuntu1204-3.4.1.tgz
$ tar xzvf mongodb-linux-x86_64-ubuntu1204-3.4.1.tgz
$ cd mongodb-linux-x86_64-ubuntu1204-3.4.1/bin
$ ./mongod --dpath ~/mongodbtest/single/data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/mongo2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;使用ip+port访问：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/mongo3.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;主从模式&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;单实例模式的缺点就是单节点挂掉后，整个数据业务全挂，主从模式采用双机备份后主节点挂掉了后从节点可以接替主机继续服务。所以这种模式比单节点的高可用性要好很多。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/mongo4.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面看一下怎么一步步搭建一个mongodb的主从复制节点：&lt;/p&gt;

&lt;p&gt;1.准备一台机器，既当作主节点，又作为从节点。&lt;/p&gt;

&lt;p&gt;2.建立文件夹 ~/mongodbtest/master 和 ~/mongodbtest/slave。&lt;/p&gt;

&lt;p&gt;3.启动mongodb主节点程序。注意后面的这个 “ –master ”参数，标示主节点。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mongod –dbpath ~/mongodbtest/master –master

# 输出日志如下，成功！
[initandlisten] MongoDB starting: pid=18285 port=27017 dbpath=~/mongodbtest/master master=1
#日志显示主节点参数
[initandlisten] options: { dbpath: “~/mongodbtest/master”, master: true }
[initandlisten] waiting for connections on port 27017
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4.启动mongodb从节点程序。指定主节点ip地址和端口 –source 127.0.0.1:27017 和 标示从节点 –source 参数、端口号（区别于主节点）-port 27018。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mongod –dbpath ~/mongodbtest/slave –slave –source 192.168.0.1:27017 -port 27018

# 输出日志如下，成功！
[initandlisten] MongoDB starting : pid=17888 port=27018 dbpath=~/mongodbtest/slave slave=1
#日志显示从节点参数
[initandlisten] options: { dbpath: “~/mongodbtest/slave”, slave: true, source: “127.0.0.1:27018&quot; }
[initandlisten] waiting for connections on port 27018
#日志显示从节点 从主节点同步复制数据
[replslave] syncing from host:127.0.0.1:27018
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;5.该模式的缺点是：从服务器不可写，当master挂掉后，从服务器不会自动替代主服务器的功能，需要手工启动从服务器为master。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mongod –dbpath ~/mongodbtest/slave –master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6.测试主从复制&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;主节点：
# 连接
$ ./mongo 127.0.0.1:27017
# 建立test数据库
&amp;gt; use test;
# 往testdb表插入数据
&amp;gt; db.testdb.insert({&quot;test1&quot;:&quot;testval1&quot;});
# 查询
&amp;gt; db.testdb.find()
{&quot;_id&quot;: ObjectId(&quot;5284e6268ed115d6238bdb39&quot;), &quot;test1&quot; : &quot;testval1&quot;}
	
从节点：
# 连接
$ ./mongo 127.0.0.1:27018
# 查询数据库（会报错）
&amp;gt; show dbs;
{ 
	&quot;ok&quot;: 0,
	&quot;errmsg&quot;: &quot;not master and slaveOk=false&quot;
}
# 需要先设置slaveOk
&amp;gt; db.getMongo().setSlaveOk();
&amp;gt; db.testdb.find()
{&quot;_id&quot;: ObjectId(&quot;5284e6268ed115d6238bdb39&quot;), &quot;test1&quot; : &quot;testval1&quot;}
&lt;/code&gt;&lt;/pre&gt;
</description>
                <link>http://username.github.io/2016/12/30/mongodb</link>
                <guid>http://username.github.io/2016/12/30/mongodb</guid>
                <pubDate>Fri, 30 Dec 2016 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>MySQL事务入门</title>
                <description>
&lt;h2 id=&quot;概述&quot;&gt;概述&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;InnoDB 和 MyISAM 的最大不同有两点：一是支持事务（TRANSACTION）；二是采用了行级锁。&lt;/p&gt;

&lt;p&gt;事务是由一组SQL语句组成逻辑处理单元，事务具有以下4个属性，通常简称为事务的ACID属性。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;事务的ACID属性&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;原子性(Atomicity)：事务是一个原子操作单元，其对数据的修改，要么全都执行，要么全都不执行。&lt;/li&gt;
  &lt;li&gt;一致性(Consistent)：在事务开始和完成时，数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改，以保持数据的完整性；事务结束时，所有的内部数据结构(如B树索引或双向链表)也都必须是正确的。&lt;/li&gt;
  &lt;li&gt;隔离性(Isolation)：数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的“独立”环境执行。这意味着事务处理过程中的中间状态对外部是不可见的，反之亦然。&lt;/li&gt;
  &lt;li&gt;持久性(Durable)：事务完成之后，它对于数据的修改是永久性的，即使出现系统故障也能够保持。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;并发事务带来的问题&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;相对于串行处理来说，并发处理能大大增加数据库资源的利用率，提高数据库系统的事务吞吐量，从而可以支持更多的用户。但并发事务处理也会带来一些问题，主要包括以下几种情况：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;更新丢失(Lost Update)：当两个或多个事务选择同一行，然后基于最初选定的值更新该行时，由于每个事务都不知道其他事务的存在，就会发生丢失更新问题—最后的更新覆盖了由其他事务所做的更新。&lt;/li&gt;
  &lt;li&gt;脏读(Dirty Reads)：一个事务正在对一条记录做修改，在这个事务完成并提交前，这条记录的数据就处于不一致状态；这时，另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些“脏”数据，并据此做进一步的处理，就会产生未提交的数据依赖关系，这种现象被形象地称为“脏读”。&lt;/li&gt;
  &lt;li&gt;不可重复读(Non-Repeatable Reads)：一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现其读出的数据已经发生改变或某些记录已经被删除，这种现象就叫做“不可重复读”。&lt;/li&gt;
  &lt;li&gt;幻读(Phanton Reads)：一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为“幻读”。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;事务隔离级别&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“脏读”、“不可重复读”、“幻读”，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决。数据库实现事务隔离的方式，可以分为两种：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;一种是在读取数据前，对其加锁，阻止其他事务对数据进行修改。&lt;/li&gt;
  &lt;li&gt;另一种是不用加任何锁，通过一定机制生成一个数据请求时间点的一致性数据快照(snapshot)，并用这个快照来提供一定级别(语句级或事务级)的一致性读取。从用户的角度来看，好像是数据库可以提供同一数据的多个版本，因此，这种技术叫做&lt;code&gt;数据多版本并发控制(MultiVersion Concurrency Control)&lt;/code&gt;，也经常称为“多版本数据库”。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;数据库的事务隔离越严格，并发副作用越小，但付出的代价越大，因为事务隔离实质上就是使事务在一定程度上“串行化”进行，这显然与“并发”是矛盾的。&lt;/p&gt;

&lt;p&gt;为了解决“隔离”与“并发”的矛盾，ISO/ANSI SQL92定义了4个事务隔离级别，每个级别的隔离程度不同，允许出现的副作用也不同，应用可以根据自己的业务逻辑要求，通过选择不同的隔离级别来平衡“隔离”和“并发”的矛盾。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;隔离级别&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;读数据一致性&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;脏读&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;不可重复读&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;幻读&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;未提交读&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;最低级别，只能保证不读取物理上损坏的数据&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;是&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;是&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;是&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;已提交读&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;语句级&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;否&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;是&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;是&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;可重复读&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;事务级&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;否&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;否&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;是&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;可序列化&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;最高级别，事务级&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;否&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;否&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;否&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;各具体数据库并不一定完全实现了上述4个隔离级别，MySQL支持全部4个隔离级别，但是具体实现时，有一些特点。&lt;/p&gt;

&lt;h2 id=&quot;事务控制&quot;&gt;事务控制&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;MySQL通过SET AUTOCOMMIT、START TRANSACTION、COMMIT和ROLLBACK等语句支持本地事务，语法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;START TRANSACTION|BEGIN[WORK]
COMMIT[WORK][AND [NO] CHAIN][NO][RELEASE]
ROLLBACK[WORK][AND [NO] CHAIN][NO][RELEASE]
SET AUTOCOMMIT = {0|1}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;默认情况下，MySQL是自动提交的，如果需要通过明确的commit和rollback来提交和回滚事务，那么就需要通过明确的事务控制命令来开启事务，这是和Oracle的事务管理明显不同的地方。如果应用是从Oracle数据库迁移到MySQL数据库，则需要确保应用中是否对事务进行了明确的管理。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;START TRANSACTION或BEGIN语句可以开始一项新的事务。&lt;/li&gt;
  &lt;li&gt;COMMIT 和 ROLLBACK用来提交或者回滚事务。&lt;/li&gt;
  &lt;li&gt;CHAIN和RELEASE子句分别用来定义在事务提交或者回滚之后的操作，CHAIN会立即启动一个新事务，并且和刚才的事务具有相同的隔离级别，RELEASE则会断开和客户端的链接。&lt;/li&gt;
  &lt;li&gt;SET AUTOCOMMIT可以修改当前连接的提交方式，如果设置为0，则设置之后所有事务都需要通过明确的命令进行提交或者回滚。&lt;/li&gt;
&lt;/ul&gt;
</description>
                <link>http://username.github.io/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/2016/12/14/mysql</link>
                <guid>http://username.github.io/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/2016/12/14/mysql</guid>
                <pubDate>Wed, 14 Dec 2016 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>MySQL索引</title>
                <description>
&lt;h2 id=&quot;概述&quot;&gt;概述&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;索引对查询的速度有着至关重要的影响，理解索引也是进行数据库性能调优的起点。考虑如下情况，假设数据库中一个表有10^6条记录，DBMS的页面大小为4K，并存储100条记录。如果没有索引，查询将对整个表进行扫描，最坏的情况下，如果所有数据页都不在内存，需要读取10^4个页面，如果这10^4个页面在磁盘上随机分布，需要进行10^4次I/O，假设磁盘每次I/O时间为10ms(忽略数据传输时间)，则总共需要100s(但实际上要好很多很多)。如果对之建立B-Tree索引，则只需要进行log100(10^6)=3次页面读取，最坏情况下耗时30ms。这就是索引带来的效果，很多时候，当你的应用程序进行SQL查询速度很慢时，应该想想是否可以建索引。&lt;/p&gt;

&lt;h2 id=&quot;索引的类型&quot;&gt;索引的类型&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;索引是在存储引擎中实现的，而不是在服务器层中实现的。所以，每种存储引擎的索引都不一定完全相同，并不是所有的存储引擎都支持所有的索引类型。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;B-Tree索引&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;假设有如下一个表：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE TABLE People (
	last_name varchar(50)    not null,
	first_name varchar(50)    not null,
	dob        date           not null,
	gender     enum('m', 'f') not null,
	key(last_name, first_name, dob)
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其索引包含表中每一行的last_name、first_name和dob列。其结构大致如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/mysql2.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;索引存储的值按索引列中的顺序排列。可以利用B-Tree索引进行全关键字、关键字范围和关键字前缀查询，当然，如果想使用索引，你必须保证按索引的最左边前缀(leftmost prefix of the index)来进行查询。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;匹配全值(Match the full value)：对索引中的所有列都指定具体的值。例如，上图中索引可以帮助你查找出生于1960-01-01的Cuba Allen。&lt;/li&gt;
  &lt;li&gt;匹配最左前缀(Match a leftmost prefix)：你可以利用索引查找last name为Allen的人，仅仅使用索引中的第1列。&lt;/li&gt;
  &lt;li&gt;匹配列前缀(Match a column prefix)：例如，你可以利用索引查找last name以J开始的人，这仅仅使用索引中的第1列。&lt;/li&gt;
  &lt;li&gt;匹配值的范围查询(Match a range of values)：可以利用索引查找last name在Allen和Barrymore之间的人，仅仅使用索引中第1列。&lt;/li&gt;
  &lt;li&gt;匹配部分精确而其它部分进行范围匹配(Match one part exactly and match a range on another part)：可以利用索引查找last name为Allen，而first name以字母K开始的人。&lt;/li&gt;
  &lt;li&gt;仅对索引进行查询(Index-only queries)：如果查询的列都位于索引中，则不需要读取元组的值。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Hash索引&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;MySQL中，只有Memory存储引擎显示支持hash索引，是Memory表的默认索引类型，尽管Memory表也可以使用B-Tree索引。Memory存储引擎支持非唯一hash索引，这在数据库领域是罕见的，如果多个值有相同的hash code，索引把它们的行指针用链表保存到同一个hash表项中。&lt;/p&gt;

&lt;p&gt;假设创建如下一个表：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE TABLE testhash (
	fname VARCHAR(50) NOT NULL,
	lname VARCHAR(50) NOT NULL,
	KEY USING HASH(fname)
) ENGINE=MEMORY;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;假设索引使用hash函数f( )，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;f('Arjen') = 2323

f('Baron') = 7437

f('Peter') = 8784

f('Vadim') = 2458
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时，索引的结构大概如下：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Slot&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2323&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Pointer to row 1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2458&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Pointer to row 4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7437&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Pointer to row 2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8784&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Pointer to row 3&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Slots是有序的，但是记录不是有序的。当你执行
mysql&amp;gt; SELECT lname FROM testhash WHERE fname=’Peter’;
MySQL会计算’Peter’的hash值，然后通过它来查询索引的行指针。因为f(‘Peter’) = 8784，MySQL会在索引中查找8784，得到指向记录3的指针。
因为索引自己仅仅存储很短的值，所以，索引非常紧凑。Hash值不取决于列的数据类型，一个TINYINT列的索引与一个长字符串列的索引一样大。&lt;/p&gt;

&lt;p&gt;Hash索引有以下一些限制：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;由于索引仅包含hash code和记录指针，所以，MySQL不能通过使用索引避免读取记录。但是访问内存中的记录是非常迅速的，不会对性造成太大的影响。&lt;/li&gt;
  &lt;li&gt;不能使用hash索引排序。&lt;/li&gt;
  &lt;li&gt;Hash索引不支持键的部分匹配，因为是通过整个索引值来计算hash值的。&lt;/li&gt;
  &lt;li&gt;Hash索引只支持等值比较，例如使用=，IN( )和&amp;lt;=&amp;gt;。对于WHERE price&amp;gt;100并不能加速查询。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;常用索引&quot;&gt;常用索引&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;普通索引&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这是最基本的索引，它没有任何限制，比如上文中为title字段创建的索引就是一个普通索引，MyIASM中默认的BTREE类型的索引，也是我们大多数情况下用到的索引。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;–直接创建索引
CREATE INDEX index_name ON table(column(length)	
–修改表结构的方式添加索引
ALTER TABLE table_name ADD INDEX index_name ON (column(length))

–创建表的时候同时创建索引
CREATE TABLE `table` (
	`id` int(11) NOT NULL AUTO_INCREMENT ,
	`title` char(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL ,
	`content` text CHARACTER SET utf8 COLLATE utf8_general_ci NULL ,
	`time` int(10) NULL DEFAULT NULL ,
	PRIMARY KEY (`id`),
	INDEX index_name (title(length))
)

–删除索引
DROP INDEX index_name ON table
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;唯一索引&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;与普通索引类似，不同的就是：索引列的值必须唯一，但允许有空值（注意和主键不同）。如果是组合索引，则列值的组合必须唯一，创建方法和普通索引类似。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;–创建唯一索引
CREATE UNIQUE INDEX indexName ON table(column(length))

–修改表结构
ALTER TABLE table_name ADD UNIQUE indexName ON (column(length))

–创建表的时候直接指定

CREATE TABLE `table` (
	`id` int(11) NOT NULL AUTO_INCREMENT ,
	`title` char(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL ,
	`content` text CHARACTER SET utf8 COLLATE utf8_general_ci NULL ,
	`time` int(10) NULL DEFAULT NULL ,
	PRIMARY KEY (`id`),
	UNIQUE indexName (title(length))
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;全文索引（FULLTEXT）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;MySQL从3.23.23版开始支持全文索引和全文检索，FULLTEXT索引仅可用于 MyISAM 表；他们可以从CHAR、VARCHAR或TEXT列中作为CREATE TABLE语句的一部分被创建，或是随后使用ALTER TABLE 或CREATE INDEX被添加。&lt;/p&gt;

&lt;p&gt;对于较大的数据集，将你的资料输入一个没有FULLTEXT索引的表中，然后创建索引，其速度比把资料输入现有FULLTEXT索引的速度更为快。不过切记对于大容量的数据表，生成全文索引是一个非常消耗时间非常消耗硬盘空间的做法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;–创建表的适合添加全文索引
CREATE TABLE `table` (
	`id` int(11) NOT NULL AUTO_INCREMENT ,
	`title` char(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL ,
	`content` text CHARACTER SET utf8 COLLATE utf8_general_ci NULL ,
	`time` int(10) NULL DEFAULT NULL ,
	PRIMARY KEY (`id`),
	FULLTEXT (content)
);

–修改表结构添加全文索引
ALTER TABLE article ADD FULLTEXT index_content(content)

–直接创建索引
CREATE FULLTEXT INDEX index_content ON article(content)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;单列索引、多列索引&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;多个单列索引与单个多列索引的查询效果不同，因为执行查询时，MySQL只能使用一个索引，会从多个索引中选择一个限制最为严格的索引。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;组合索引（最左前缀）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;平时用的SQL查询语句一般都有比较多的限制条件，所以为了进一步榨取MySQL的效率，就要考虑建立组合索引。例如上表中针对title和time建立一个组合索引：ALTER TABLE article ADD INDEX index_titme_time (title(50),time(10))。建立这样的组合索引，其实是相当于分别建立了下面两组组合索引：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;– title,time
– title
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为什么没有time这样的组合索引呢？这是因为MySQL组合索引“最左前缀”的结果。简单的理解就是只从最左面的开始组合。并不是只要包含这两列的查询都会用到该组合索引，如下面的几个SQL所示：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;–使用到上面的索引

SELECT * FROM article WHREE title='测试' AND time=1234567890;
SELECT * FROM article WHREE utitle='测试';

–不使用上面的索引

SELECT * FROM article WHREE time=1234567890;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;设计索引的原则&quot;&gt;设计索引的原则&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;索引的设计可以遵循一些已有的原则，创建索引的时候尽量考虑符合这些原则。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;搜索的索引列，不一定是所要选择的列。最适合索引的列是出现再where子句中的列，或连接子句中指定的列，而不是出现在select关键字后的选择列表中的列。&lt;/li&gt;
  &lt;li&gt;使用唯一索引。考虑考虑某列中值的分布。索引的列的基数越大，索引的效果越好。例如，存放出生日期的列具有不同值，但很容易区分各行。而用来记录性别的列，只含有“M”和“F”，则对此列进行索引没有多大用处，因为不管搜索哪个值，都会得出大约一半的行。&lt;/li&gt;
  &lt;li&gt;使用短索引。如果对字符串列进行索引，应该指定一个前缀长度，只要有可能就应该这样做。例如，有一个CHAR(200)列，如果在前10个或20个字符进行索引能够节省大量索引空间，也可能会使查询更快。较小的索引涉及的磁盘IO较少，较短的值比较起来更快。对于较短的键值，索引高速缓存中的块能容纳更多的键值。&lt;/li&gt;
  &lt;li&gt;利用最左前缀。在创建一个n列的索引时，实际是创建了MySQL可利用的n个索引，多列索引可起几个索引的作用，因为可利用索引中最左边的列集来匹配行。这样的列集称为最左前缀。&lt;/li&gt;
  &lt;li&gt;不要过度索引。不要以为索引“越多越好”，每个额外的索引都要占用额外的磁盘空间，并降低写操作的性能。在修改表的内容时，索引必须进行更新，有时可能需要重构，因此，索引越多，所花的时间越长。如果有一个索引很少利用或从来不用，那么会不必要的减缓表的修改速度。&lt;/li&gt;
  &lt;li&gt;对于InnoDB存储引擎的表，记录默认会按照一定的顺序保存，如果有明确定义的主键，则按照主键顺序保存。如果没有主键，但是有唯一索引，那么就是按照唯一索引的顺序保存。如果既没有主键又没有唯一索引，那么表中会自动生成一个内部列，按照这个列的顺序保存。按照主键或者内部列进行的访问是最快的，所以innodb表尽量自己指定主键，当表中同时有几个列都是唯一的，都可以作为主键的时候，要选择最常作为访问条件的列作为主键，提高查询效率。另外，innodb表的普通索引都会保存主键的键值，所以主键要尽可能选择较短的数据类型，可以有效地减少索引的磁盘占用，提高索引的缓存效果。&lt;/li&gt;
&lt;/ul&gt;
</description>
                <link>http://username.github.io/2016/12/13/mysql</link>
                <guid>http://username.github.io/2016/12/13/mysql</guid>
                <pubDate>Tue, 13 Dec 2016 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>MySQL存储引擎介绍</title>
                <description>
&lt;h2 id=&quot;概述&quot;&gt;概述&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;插件式存储引擎是MySQL数据库最重要的特性之一，用户可以根据应用的需要选择如何存储和索引数据、是否使用事务等。MySQL默认支持多种存储引擎，以适用于不同领域的数据应用需要、用户可以选择不同的存储引擎提高应用的效率、提供灵活的存储，用户甚至可以按照自己的需要定制和使用自己的存储引擎，以实现最大程度的可定制型。&lt;/p&gt;

&lt;p&gt;MySQL 5.0支持的存储引擎包括MyISAM、InnoDB、BDB、MEMORY、MERGE、EXAMPLE、NDB Cluster、ARCHIVE、CSV、BLACKHOLE、FEDERATED等，其中InnoDB和BDB提供事务安全表，其他存储引擎都是非事务安全表。&lt;/p&gt;

&lt;p&gt;创建新表时如果不指定存储引擎，那么系统就会使用默认存储引擎，MySQL 5.5 之前默认存储引擎室MyISAM，5.5之后改为了InnoDB。如果要修改默认存储引擎，可以在参数文件中设置default-table-type。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. 查看当前默认存储引擎：

mysql&amp;gt; show variables like 'table_type';

+---------------+--------+
| Variable_name | Value  |
+---------------+--------+
| table_type    | MyISAM |
+---------------+--------+

2. 查看当前数据版本支持（Support）的存储引擎和默认(Default)存储引擎

mysql&amp;gt; show engines \G;

*************************** 1. row ***************************
      Engine: ndbcluster
     Support: NO
     Comment: Clustered, fault-tolerant tables
Transactions: NULL
      	  XA: NULL
  Savepoints: NULL
*************************** 2. row ***************************
      Engine: MRG_MYISAM
     Support: YES
     Comment: Collection of identical MyISAM tables
Transactions: NO
          XA: NO
  Savepoints: NO
*************************** 3. row ***************************
	  Engine: BLACKHOLE
 	 Support: YES
     Comment: /dev/null storage engine (anything you write to it disappears)
Transactions: NO
  		  XA: NO
  Savepoints: NO
*************************** 4. row ***************************
      Engine: CSV
     Support: YES
     Comment: CSV storage engine
Transactions: NO
          XA: NO
  Savepoints: NO
*************************** 5. row ***************************
      Engine: MEMORY
     Support: YES
 	 Comment: Hash based, stored in memory, useful for temporary tables
Transactions: NO
  	      XA: NO
  Savepoints: NO
*************************** 6. row ***************************
      Engine: FEDERATED
     Support: NO
     Comment: Federated MySQL storage engine
Transactions: NULL
          XA: NULL
  Savepoints: NULL
*************************** 7. row ***************************
      Engine: ARCHIVE
     Support: YES
     Comment: Archive storage engine
Transactions: NO
          XA: NO
  Savepoints: NO
*************************** 8. row ***************************
  	  Engine: InnoDB
 	 Support: YES
 	 Comment: Supports transactions, row-level locking, and foreign keys
Transactions: YES
	      XA: YES
  Savepoints: YES
*************************** 9. row ***************************
      Engine: MyISAM
     Support: DEFAULT
     Comment: Default engine as of MySQL 3.23 with great performance
Transactions: NO
    	  XA: NO
  Savepoints: NO

3. 查看支持的存储引擎

mysql&amp;gt; show variables like 'have%';
+-------------------------+----------+
| Variable_name           | Value    |
+-------------------------+----------+
| have_community_features | YES      |
| have_compress           | YES      |
| have_crypt              | YES      |
| have_csv                | YES      |	
| have_dynamic_loading    | YES      |
| have_geometry           | YES      |
| have_innodb             | YES      |
| have_ndbcluster         | DISABLED |
| have_openssl            | NO       |
| have_partitioning       | YES      |
| have_query_cache        | YES      |
| have_rtree_keys         | YES      |
| have_ssl                | NO       |
| have_symlink            | DISABLED |
+-------------------------+----------+

其中value为DISABLED的记录表示支持该存储引擎，但是数据库启动时被禁用。
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;常用存储引擎的特点&quot;&gt;常用存储引擎的特点&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;MyISAM&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;MyISAM 不支持事务、也不支持外键，其优点是访问速度快，对事务完整性没有要求或者以select、insert为主的应用基本上都可以使用MyISAM来创建表。&lt;/p&gt;

&lt;p&gt;每个MyISAM在磁盘上存储成3个文件，其文件名都和表名相同，但扩展名分别是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;.frm (存储表定义)&lt;/li&gt;
  &lt;li&gt;.MYD (MYData，存储数据)&lt;/li&gt;
  &lt;li&gt;.MYI (MYIndex，存储索引)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;数据文件和索引文件可以放置在不同的目录，平均分配IO，获得更快的速度。在创建表的时候可以通过DATA DIRECTORY 和 INDEX DIRECTORY 语句指定索引文件和数据文件的路径，也就是说不同MyISAM 表的索引文件和数据文件可以放置到不同的路径下，文件路径需要是绝对路径，并且具有访问权限。&lt;/p&gt;

&lt;p&gt;MyISAM 类型的表可能会损坏，损坏后的表可能不能被访问，会提示需要修复或者访问后返回错误的结果。MyISAM 类型的表提供修复工具，可以用check table语句来检查MyISAM表的健康，语法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CHECK TABLE tbl_name [, tbl_name] ... [option] ...

option = {
	FOR UPGRADE
  		| QUICK
  		| FAST
  		| MEDIUM
  		| EXTENDED
  		| CHANGED
}

mysql&amp;gt; check table questions;
+--------------------------+-------+----------+----------+
| Table                    | Op    | Msg_type | Msg_text |
+--------------------------+-------+----------+----------+
| feedback.questions 		| check | status   | OK       |
+--------------------------+-------+----------+----------+	
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;损坏的MyISAM表可以使用REPAIR TABLE 语句修复，语法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;REPAIR [NO_WRITE_TO_BINLOG | LOCAL] TABLE
tbl_name [, tbl_name] ...
[QUICK] [EXTENDED] [USE_FRM]

mysql&amp;gt; repair table questions;
+--------------------------+--------+----------+----------+
| Table                    | Op     | Msg_type | Msg_text |
+--------------------------+--------+----------+----------+
| feedback.questions 		| repair | status   | OK       |
+--------------------------+--------+----------+----------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;MyISAM 表支持3种不同的存储格式，分别是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;静态表 默认 记录固定长度 优点：存储快 缺点：占用空间大&lt;/li&gt;
  &lt;li&gt;动态表 变长字段  优点：占用空间少  缺点：频繁更新和删除记录容易产生碎片&lt;/li&gt;
  &lt;li&gt;压缩表 每个记录被单独压缩，占据非常小的磁盘空间&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;InnoDB&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;InnoDB 具有提交、回滚和崩溃恢复能力的事务安全。但是对比MyISAM存储引擎，InnoDB写的处理效率差一些，并且会占用更多的磁盘空间以保留数据和索引。&lt;/p&gt;

&lt;p&gt;1、自动增长列&lt;/p&gt;

&lt;p&gt;InnoDB 表的自动增长列可以手工插入，但是插入的值如果是空或者0，则实际插入的将是自动增长后的值。可以通过”ALTER TABLE *** AUTO_INCREMENT=n;” 语句强制设置自动增长列的初始值，默认从1开始，但是该强制的默认值是保留在内存中的，如果该值在使用之前数据库重新启动，那么这个强制的默认值就会丢失，就需要在数据库启动以后重新设置。&lt;/p&gt;

&lt;p&gt;可以使用LAST_INSERT_ID()查询当前线程最后插入记录使用的值。如果一次插入了多条记录，那么返回的是第一条记录使用的自动增长值。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql&amp;gt; select last_insert_id();
+------------------+
| last_insert_id() |
+------------------+
|               40 |
+------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于InnoDB表，自动增长列必须是索引。如果是组合索引，也必须是组合索引的第一列，但是对于MyISAM表，自动增长列可以是组合索引的其他列，这样插入记录后，自动增长列是按照组合索引的前面几列进行排序后递增的。&lt;/p&gt;

&lt;p&gt;2、外键约束&lt;/p&gt;

&lt;p&gt;MySQL 支持外键的存储引擎只有InnoDB，在创建外键的时候，要求父表必须有对应的索引，子表在创建外键的时候也会自动创建对应的索引。&lt;/p&gt;

&lt;p&gt;在创建索引时，可以指定在删除、更新父表时，对子表进行的相应操作，包括RESTRICT、NO ACTION、CASCADE、SET NULL。其中RESTRICT和NO ACTION相同，是指限制在子表有关联记录的情况下父表不能更新；CASCADE 表示父表在更新或删除时，更新或删除子表对应记录；SET NULL 则表示父表在更新或删除时，子表对应字段被SET NULL。&lt;/p&gt;

&lt;p&gt;当某个表被其他表创建了外键参照，那么该表的对应索引或主键禁止被删除。&lt;/p&gt;

&lt;p&gt;在导入多个表的数据时，如果需要忽略表之前的导入顺序，可以暂时关闭外键的检查；同样，在执行LOAD DATA和ALTER TABLE操作时，可以通过暂时关闭外键约束来加快处理的速度，关闭和打开外键约束的命令是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SET FOREIGN_KEY_CHECKS = 0;
SET FOREIGN_KEY_CHECKS = 1;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3、存储方式&lt;/p&gt;

&lt;p&gt;InnoDB的表空间结构图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/yuzujin/yuzujin.github.com/master/images/mysql1.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;InnoDB存储表和索引有以下两种方式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用共享表空间存储，这种方式创建的表的表结构保存在.frm文件中，数据和索引保存在innodb_data_home_dir 和 innodb_data_file_path 定义的表空间中，可以是多个文件。&lt;/li&gt;
  &lt;li&gt;使用多表空间（又称单独表空间）存储，这种方式创建的表的表结构仍然保存在.frm文件中，但是每个表的数据和索引单独保存在.ibd中。如果是个分区表，则每个分区对应单独的.ibd文件，文件名是“表名+分区名”，可以在创建分区时指定每个分区的数据文件位置，以此来将表的IO均匀分布在多个磁盘上。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在InnoDB存储引擎中，默认表空间文件是ibdata1，初始化为10M，且可以扩展:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql&amp;gt; show variables like 'innodb_data%';
+-----------------------+------------------------+
| Variable_name         | Value                  |
+-----------------------+------------------------+
| innodb_data_file_path | ibdata1:10M:autoextend |
| innodb_data_home_dir  |                        |
+-----------------------+------------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用共享表空间存储方式时，Innodb的所有数据保存在一个单独的表空间里面，而这个表空间可以由很多个文件组成，一个表可以跨多个文件存在，所以其大小限制不再是文件大小的限制，而是其自身的限制。从Innodb的官方文档中可以看到，其表空间的最大限制为64TB，也就是说，Innodb的单表限制基本上也在64TB左右了，当然这个大小是包括这个表的所有索引等其他相关数据。&lt;/p&gt;

&lt;p&gt;而在使用多表空间存储方式时，每个表的数据以一个单独的文件来存放，这个时候的单表限制，又变成文件系统的大小限制了。不同平台下，多表空间文件最大限度：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Operating System  File-size Limit
Win32 w/ FAT/FAT32  2GB/4GB
Win32 w/ NTFS          2TB (possibly larger)
Linux 2.4+          (using ext3 file system) 4TB
Solaris 9/10          16TB
MacOS X w/ HFS+         2TB
NetWare w/NSS file system  8TB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;共享表空间与多表空间可以通过在my.cnf中设置参数innodb_file_per_table来转换，若为1，则开启多表空间，否则，开启共享表存储。并且重新启动服务后才可以生效，对于新建的表按照多表空间的方式创建，已有的表仍然使用共享表空间存储。如果将已有的多表空间方式修改回共享表空间方式，则新建表会在共享表空间中创建，但已有的多表空间的表仍然保存原来的访问方式。所以多表空间的参数生效后，只对新建的表生效。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql&amp;gt; show variables like 'innodb_file%';
+------------------------+-------+
| Variable_name          | Value |	
+------------------------+-------+	
| innodb_file_io_threads | 4     |
| innodb_file_per_table  | OFF   |
+------------------------+-------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于使用多表空间特性的表，可以比较方便地进行单表备份和恢复操作，但是直接复制.ibd文件是不行的，因为没有共享表空间的数据字典信息，直接复制的.ibd文件和.frm文件恢复时是不能被正确识别的，但是可以通过下面的命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ALTER TABLE tbl_name DISCARD TABLESPACE;
ALTER TABLE tble_name IMPORT TABLESPACE;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将备份恢复到数据库中，但是这样的单表备份，只能恢复到表原来所在的数据库中，而不能恢复到其他的数据库中。如果要将单表恢复到目标数据库，则需要通过mysqldump和mysqlimport来实现。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;即便在多表空间的存储方式下，共享表空间仍然是必须的，Innodb把内部数据词典和在线重做日志放在这个文件中。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;MEMORY&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;MEMORY 存储引擎将所有数据保存在RAM中，在需要快速定位记录和其他类似数据的环境下，可提供极快的访问。MEMORY的缺陷是对表的大小有限制，太大的表无法缓存在内存中，其次是要确保表的数据可以恢复，数据异常终止后表的数据是可以恢复的。MEMORY表通常用于更新不太频繁的小表，用于快速得到访问结果。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MERGE&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;用于将一系列等同的MyISAM表以逻辑方式组合在一起，并作为一个对象引用它们。MERGE表的优点在于可以突破对单个MyISAM表大小的限制，并且通过将不同的表分布在多个磁盘上，可以有效地改善MERGE表的访问效率。&lt;/p&gt;

&lt;h2 id=&quot;常用存储引擎的对比&quot;&gt;常用存储引擎的对比&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;特点&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;MyISAM&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;InnoDB&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;MEMORY&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;MERGE&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;存储限制&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;有&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;64TB&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;有&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;没有&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;事务安全&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;支持&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;锁机制&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;表锁&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;行锁&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;表锁&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;表锁&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;B树索引&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;支持&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;支持&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;支持&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;支持&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;哈希索引&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;支持&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;全文索引&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;支持&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;集群索引&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;支持&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;数据缓存&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;支持&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;支持&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;索引缓存&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;支持&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;支持&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;支持&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;支持&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;数据可压缩&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;支持&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;空间使用&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;低&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;高&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;N/A&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;低&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;内存使用&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;低&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;高&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;中等&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;低&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;批量插入速度&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;高&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;低&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;高&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;高&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;支持外键&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;支持&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
                <link>http://username.github.io/2016/12/12/mysql</link>
                <guid>http://username.github.io/2016/12/12/mysql</guid>
                <pubDate>Mon, 12 Dec 2016 00:00:00 +0800</pubDate>
        </item>

        <item>
                <title>MySql外键介绍</title>
                <description>
&lt;h2 id=&quot;基本概念&quot;&gt;基本概念&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;MySQL中“键”和“索引”的定义相同，所以外键和主键一样也是索引的一种。不同的是MySQL会自动为所有表的主键进行索引，但是外键字段必须由用户进行明确的索引。用于外键关系的字段必须在所有的参照表中进行明确地索引，InnoDB不能自动地创建索引。&lt;/p&gt;

&lt;p&gt;外键可以是一对一的，一个表的记录只能与另一个表的一条记录连接，或者是一对多的，一个表的记录与另一个表的多条记录连接。&lt;/p&gt;

&lt;p&gt;如果需要更好的性能，并且不需要完整性检查，可以选择使用MyISAM表类型，如果想要在MySQL中根据参照完整性来建立表并且希望在此基础上保持良好的性能，最好选择表结构为innoDB类型。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;参照完整性&lt;/code&gt;：当外键与另一个表的字段有关系，而且这种关系是惟一时，这个系统就称为处于参照完整性的状态。也就是说，如果一个字段在所有的表中只出现一次，而且每个表的这个字段的变化都会影响其他表，这就是存在参照完整性。&lt;/p&gt;

&lt;p&gt;外键的好处：可以使得两张表关联，保证数据的一致性和实现一些级联操作。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;级联操作&lt;/code&gt;：外键可以保证新插入的记录的完整性。但是，如果在REFERENCES从句中从已命名的表删除记录会怎样？在使用同样的值作为外键的辅助表中会发生什么？很明显，那些记录也应该被删除，否则在数据库中就会有很多无意义的孤立记录。&lt;/p&gt;

&lt;h2 id=&quot;外键的使用条件&quot;&gt;外键的使用条件&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;两个表必须是InnoDB表，MyISAM表暂时不支持外键&lt;/li&gt;
  &lt;li&gt;外键列必须建立了索引，MySQL 4.1.2以后的版本在建立外键时会自动创建索引，但如果在较早的版本则需要显式建立；&lt;/li&gt;
  &lt;li&gt;外键关系的两个表的列必须是数据类型相似，也就是可以相互转换类型的列，比如int和tinyint可以，而int和char则不可以；&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;使用方法&quot;&gt;使用方法&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;外键的定义语法&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[CONSTRAINT symbol] FOREIGN KEY [id] (index_col_name, ...)
	REFERENCES tbl_name (index_col_name, ...)
	[ON DELETE {RESTRICT | CASCADE | SET NULL | NO ACTION | SET DEFAULT}]
	[ON UPDATE {RESTRICT | CASCADE | SET NULL | NO ACTION | SET DEFAULT}]
	
该语法可以在 CREATE TABLE 和 ALTER TABLE 时使用，如果不指定CONSTRAINT symbol，MYSQL会自动生成一个名字。

ON DELETE、ON UPDATE表示事件触发限制，可设参数：
① RESTRICT（限制外表中的外键改动，默认值）
② CASCADE（跟随外键改动）
③ SET NULL（设空值）
④ SET DEFAULT（设默认值）
⑤ NO ACTION（无动作，默认的）
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;示例&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;创建表1
	create table repo_table(
		repo_id char(13) not null primary key,
		repo_name char(14) not null
	) type=innodb;

创建表2
	create table busi_table(
		busi_id char(13) not null primary key,
		busi_name char(13) not null,
		repo_id char(13) not null,
		foreign key(repo_id) references repo_table(repo_id)
	) type=innodb;

插入数据
	insert into repo_table values(&quot;12&quot;,&quot;sz&quot;); //success
	insert into repo_table values(&quot;13&quot;,&quot;cd&quot;); //success
	insert into busi_table values(&quot;1003&quot;,&quot;cd&quot;, &quot;13&quot;); //success
	insert into busi_table values(&quot;1002&quot;,&quot;sz&quot;, &quot;12&quot;); //success
	insert into busi_table values(&quot;1001&quot;,&quot;gx&quot;, &quot;11&quot;); //failed,
	错误提示：ERROR 1452 (23000): Cannot add or update a child row: 
	a foreign key constraint fails (`smb_man`.`busi_table`, 
	CONSTRAINT `busi_table_ibfk_1` FOREIGN KEY (`repo_id`) REFERENCES `repo_table` (`repo_id`))
	
增加级联操作
	alter table busi_table add constraint id_check foreign key(repo_id)
	references repo_table(repo_id) on delete cascade on update cascade;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;相关操作&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;外键约束（表2）对父表（表1）的含义:&lt;/p&gt;

&lt;p&gt;在父表上进行update/delete以更新或删除在子表中有一条或多条对应匹配行的候选键时，父表的行为取决于：在定义子表的外键时指定的on update/on delete子句。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;关键字&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;含义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;CASCADE&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;删除包含与已删除键值有参照关系的所有记录&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;SET NULL&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;修改包含与已删除键值有参照关系的所有记录，使用NULL值替换(只能用于已标记为NOT NULL的字段)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;RESTRICT&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;拒绝删除要求，直到使用删除键值的辅助表被手工删除，并且没有参照时(这是默认设置，也是最安全的设置)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;NO ACTION&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;啥也不做&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;其他&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在外键上建立索引:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;index repo_id (repo_id),
foreign key(repo_id) references repo_table(repo_id))
&lt;/code&gt;&lt;/pre&gt;
</description>
                <link>http://username.github.io/2016/12/09/mysql</link>
                <guid>http://username.github.io/2016/12/09/mysql</guid>
                <pubDate>Fri, 09 Dec 2016 00:00:00 +0800</pubDate>
        </item>


</channel>
</rss>
